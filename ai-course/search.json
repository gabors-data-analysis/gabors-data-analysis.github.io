[
  {
    "objectID": "week03/ideas-good-report.html",
    "href": "week03/ideas-good-report.html",
    "title": "Some ideas on good report",
    "section": "",
    "text": "What is a good data analysis presentation"
  },
  {
    "objectID": "week03/ideas-good-report.html#three-types-of-reserach-questions",
    "href": "week03/ideas-good-report.html#three-types-of-reserach-questions",
    "title": "Some ideas on good report",
    "section": "Three types of reserach questions",
    "text": "Three types of reserach questions\n\nPatterns of association – focus on pattern discovery\nCausal question – identify the effect of an intervention\n\nExperiment\nObservational data\n\nBuild a predictive model – focus on selection, understand how model works"
  },
  {
    "objectID": "week03/ideas-good-report.html#let-us-focus-on-association",
    "href": "week03/ideas-good-report.html#let-us-focus-on-association",
    "title": "Some ideas on good report",
    "section": "Let us focus on association",
    "text": "Let us focus on association\nThink questions like\n\nAre people working more hours, make more money/hour?\nHow much less does a car worth when having 1000km more in odometer?\nWhat is the relationship between hotel prices and location / amenities?\n\nCausal would be\n\nIn an intervention, we make some people work more, will they have a high w/h?\n\nPrediction would be\n\nHere is wage data, let us build a model for hourly wage"
  },
  {
    "objectID": "week03/ideas-good-report.html#what-to-include-in-a-report",
    "href": "week03/ideas-good-report.html#what-to-include-in-a-report",
    "title": "Some ideas on good report",
    "section": "What to include in a report",
    "text": "What to include in a report\nConsider a few-page report on an association with some possibility for causal analysis (later)\nKey parts of the report\n\nIntroduction (why interesting)\nResearch question\nData description\n\nsource\nvariables\n\nMethods\n\nwhat method you use (e.g. cross section OLS)\nregression you estimate precisely\n\nResults\n\nInclude exhibits (think about graphs, tables)\nDecide on main result and focus on explaining it (e.g. \\(x\\) coefficient)\n\nHint: think for the reader – what would help them understand?\n\n\nConclusion\n\nShort: 1-2 para\nDiscussion to summarize result (in plain English)\nPlace result in context\n\nis it causal\nexternal validity\n\n(no need for future research)"
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html",
    "href": "week03/assets/trust_income_report_long.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html#motivation",
    "href": "week03/assets/trust_income_report_long.html#motivation",
    "title": "",
    "section": "1. Motivation",
    "text": "1. Motivation\nTrust lubricates social and economic interaction. Classical growth models treat trust as an informal institution that lowers transaction costs; empirical work often finds that high‑trust societies grow faster and innovate more. Here we ask a simpler descriptive question: are richer countries also more trusting?"
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html#data",
    "href": "week03/assets/trust_income_report_long.html#data",
    "title": "",
    "section": "2. Data",
    "text": "2. Data\n\nCoverage: 64 country‑surveys from WVS Wave 7 (2017‑2022).\nIncome: GDP per capita, PPP‑adjusted USD (World Bank).\nTrust:\n• Baseline – share answering “Most people can be trusted” (Q57).\n• Alternative – average of Q59–Q63 (trust in neighbours, acquaintances, first‑time met, other religion, other nationality), reverse‑coded."
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html#exploratory-pattern",
    "href": "week03/assets/trust_income_report_long.html#exploratory-pattern",
    "title": "",
    "section": "3. Exploratory pattern",
    "text": "3. Exploratory pattern\n\n\n\nTrust‑income scatter\n\n\nBoth the LOESS smoother and the dashed OLS line suggest a positive association. Annotated points mark extreme income positions (rich: MAC, SGP, NLD; poor: ETH, ZWE, TJK) and trust outliers."
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html#regression-analysis",
    "href": "week03/assets/trust_income_report_long.html#regression-analysis",
    "title": "",
    "section": "4. Regression analysis",
    "text": "4. Regression analysis\n\nBaseline trust\nβ₁ = 0.104 (SE 0.018, p = 0.000, R² = 0.35)\n\n\nAlternative trust index\nβ₁ = 0.137 (SE 0.030, p = 0.000, R² = 0.25)"
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html#interpretation",
    "href": "week03/assets/trust_income_report_long.html#interpretation",
    "title": "",
    "section": "5. Interpretation",
    "text": "5. Interpretation\nA one‑unit rise in log‑income (≈ 2.7× GDP per capita) is associated with a 0.104-point increase in the share trusting (scale 0–1). While not large, this accounts for roughly 35% of cross‑country variation. Channels could include better governance, education and formal institutions in high‑income settings, which foster generalised trust. Reverse causality—trust as a driver of growth—remains plausible."
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html#limitations-robustness",
    "href": "week03/assets/trust_income_report_long.html#limitations-robustness",
    "title": "",
    "section": "6. Limitations & robustness",
    "text": "6. Limitations & robustness\n\nCross‑section only; cannot infer causality.\nSmall N; high‑trust outlier (CHN) influences slope.\nAggregate averages hide within‑country disparities."
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html#short-assignment-prompt",
    "href": "week03/assets/trust_income_report_long.html#short-assignment-prompt",
    "title": "",
    "section": "7. Short assignment prompt",
    "text": "7. Short assignment prompt\nFor students:\n\nReplicate the scatter plot; label one additional country of your choice.\nRe‑estimate the regression adding a regional fixed effect (e.g. Europe vs. rest). How does β₁ change?\nPropose two mechanisms linking income and trust and outline an empirical strategy to test one of them.\nCritique the trust measures used here.\n\n\nPrepared 21 Apr 2025"
  },
  {
    "objectID": "assignments/assignment_02.html",
    "href": "assignments/assignment_02.html",
    "title": "Assignment 2: Data exploration and documentation",
    "section": "",
    "text": "Task\nIn this assignment, you will continue exploring the data, and create a readme document. This is an individual assignment.\n\nReview good practices for readme and data documentation\nFirst, consider the WVS Survey we used in class: Codebook, sample data. See the World Values Survey case study for more info. Link it to your AI. (Can move to google drive.)\nSecond, choose a research question using the VWS data. It can be broad or specific. Ask AI for ideas.\nWhat variables would you use to investigate the research question? Pick the most relevant ones (min 8, max 25)\nReview survey timing and discuss consequences\nConsidering these variables only create a single README.md (in markdown).\n\nThe readme\nIn the readme, include:\n\nkey info on the dataset itself (you need to figure out what this key info is based on the lecture and examples)\nkey variables description\ntable with descriptive statistics for the 5 most important selected variables\n\n\n\nTo submit\n\nREADME.md file. (lastname_firstname_dawai_week02_readme.md) (16p)\nWhat advice would you give to a fellow data analysis student on using AI to create README documents? (lastname_firstname_dawai_week02_advice.txt) (4p)\n\nList 2–3 pieces of advice based on your own experience using AI for this specific task.\n\n\n\n\nAI use restrictions\n\nDo not use AI to help you generate the advice. We want to hear your personal examples and reflections, not AI-generated suggestions."
  },
  {
    "objectID": "assignments/assignment_01.html",
    "href": "assignments/assignment_01.html",
    "title": "Reproduce an FT graph",
    "section": "",
    "text": "Look at this graph from Financial Times. Your task is to recreate this figure using real-world data.\nUse AI as your assistant to find real-world data that matches or closely resembles the one used in the FT graph, and recreate the same chart using that data.\nSuccess Criteria:\n\nData accuracy: Real Treasury yield and tariff announcement data\nDesign fidelity: Similar visual style, annotations, and layout\nCode quality: Reproducible workflow with clear documentation\n\nwhat to submit and grading\n\nGraph: A PDF version of your recreated graph, name it: “lastname_firstname_dawai_week01_ftfigure.pdf” (10p)\nData: A CSV file with the data used to create the graph, with the name (“lastname_firstname_dawai_week01_ftdata.csv”) (4p)\nCode (any language) (“lastname_firstname_dawai_week01_ftdata.py or .R”), scripts or notebooks also OK. (2p)\nOne para text on experience with AI: what model you used, was easy, where was human interaction needed. (“lastname_firstname_dawai_week01_ftdata.txt or .md”) (4p)"
  },
  {
    "objectID": "assignments/assignment_03.html",
    "href": "assignments/assignment_03.html",
    "title": "Assignment 03 – Creating a data analysis report",
    "section": "",
    "text": "This is a group assignment. Continue with the research question you had chosen in class.\nUse AI iteratively to help you create a report.\n\nFirst run\n\nThink in terms of two variables (\\(x\\) and \\(y\\)).\n\nPick a single variable or a combined index of variables\nPick a GDP variable\nThink about a causal link.\n\nCreate one carefully designed graph to illustrate the relationship (Graph 1)\nAt each step, explain your choices/decisions (ie why you chose a certain variable)\nCreate a Conclusion paragraph where you summarize your work and results in 80-100 words.\ncreate another graph to show heterogeneity of country size groups using population (Graph 2)\nRun a regression and interpret the coefficient. Make sure they are precise.\nUse AI to get a well cratfted folder structure\n\n\n\nExtended context window run 1\n\ncreate a system prompt will preferences, expected output, style, etc\nrecreate a the report\n\n\n\nExtended context window run 2\n\nOnce you have a draft, Upload slideshow for chapters 10 data analysis slides. and ask to follow it and revise report.\nrecreate a the report\n\n\n\nFinal repor\n\nYou have three versions. Use them to have a single one you like the most. Edit further if needed.\n\n\n\nTo submit\n\nreport of maximum 2 pages in .pdf format (including exhibits) (lastname_firstname_dawai_week03_report.pdf) (12p)\nyour code (or provide a link). (2p)\nText: What advice would you give to a fellow data analysis student on using AI to create a report? (How do various runs compare? What changes are decisive? How many iterations and useful?) (lastname_firstname_dawai_week03_advice.txt) (6p).\n\n\nUpload your report to the student folder called Reports at moodle or similar service if applicable. This is for the next class — each group will read another groups report and discuss the good and bad aspects of the report.\n\n\n\nAI use restrictions\n\nFor the report, use AI as your assistant, use it as input not as output. Don’t submit an AI-generated report without review and edit.\nDo not use AI to help you generate the advice. We want to hear your personal examples and reflections, not AI-generated suggestions."
  },
  {
    "objectID": "assignments/assignment_04.html",
    "href": "assignments/assignment_04.html",
    "title": "Assignment 4: Extend the Austrian Hotels Dataset",
    "section": "",
    "text": "In this assignment, you will use Claude Code to:\n\nGenerate a new data table that can be joined to the Austrian Hotels dataset\nJoin your new table with existing data\nPerform an analysis that answers an interesting question\nDocument your process and reflect on using agentic AI"
  },
  {
    "objectID": "assignments/assignment_04.html#overview",
    "href": "assignments/assignment_04.html#overview",
    "title": "Assignment 4: Extend the Austrian Hotels Dataset",
    "section": "",
    "text": "In this assignment, you will use Claude Code to:\n\nGenerate a new data table that can be joined to the Austrian Hotels dataset\nJoin your new table with existing data\nPerform an analysis that answers an interesting question\nDocument your process and reflect on using agentic AI"
  },
  {
    "objectID": "assignments/assignment_04.html#deliverables",
    "href": "assignments/assignment_04.html#deliverables",
    "title": "Assignment 4: Extend the Austrian Hotels Dataset",
    "section": "Deliverables",
    "text": "Deliverables\nSubmit a single PDF containing:\n\nYour data generation prompt and the resulting table description\nYour analysis with results (tables, charts, or statistics)\nThe key code Claude Code generated (not all of it - just the important parts)\nA brief reflection (see Part 4)"
  },
  {
    "objectID": "assignments/assignment_04.html#part-1-design-your-data-table-25",
    "href": "assignments/assignment_04.html#part-1-design-your-data-table-25",
    "title": "Assignment 4: Extend the Austrian Hotels Dataset",
    "section": "Part 1: Design Your Data Table (25%)",
    "text": "Part 1: Design Your Data Table (25%)\nCreate a new CSV file that joins to the Austrian Hotels dataset. Your table should:\n\nHave a clear join key (hotel_id, city, city+month+year, etc.)\nContain realistic patterns (not random numbers)\nEnable an interesting analysis question\n\n\nIdeas (pick one or create your own):\n\n\n\n\n\n\n\n\n\nTable\nColumns\nJoin Key\nAnalysis Question\n\n\n\n\nWeather\ncity, month, year, avg_temp, precipitation, snow_days\ncity + month + year\nDoes weather affect occupancy?\n\n\nStaff\nhotel_id, num_employees, avg_wage, languages_spoken\nhotel_id\nDo staffing levels correlate with reviews?\n\n\nEvents\nevent_id, city, event_name, month, year, expected_visitors\ncity + month + year\nHow much do events boost hotel prices?\n\n\nSustainability\nhotel_id, energy_rating, has_solar, waste_recycling_pct\nhotel_id\nDo eco-friendly hotels charge more?\n\n\nTransportation\ncity, airport_distance_km, train_station_distance, public_transport_score\ncity\nDoes accessibility affect tourism?\n\n\n\n\n\nWhat to submit:\n\nThe prompt you gave Claude Code to generate the data\nA summary of your generated table (columns, row count, sample rows)\nExplanation of the realistic patterns you built in"
  },
  {
    "objectID": "assignments/assignment_04.html#part-2-join-and-analyze-40",
    "href": "assignments/assignment_04.html#part-2-join-and-analyze-40",
    "title": "Assignment 4: Extend the Austrian Hotels Dataset",
    "section": "Part 2: Join and Analyze (40%)",
    "text": "Part 2: Join and Analyze (40%)\nUse Claude Code to join your new table with the existing Austrian Hotels data and answer your analysis question.\n\nRequirements:\n\nPerform at least one join operation\nInclude descriptive statistics (means, counts, distributions)\nCreate at least one visualization (chart or graph)\nAnswer your analysis question with evidence\n\n\n\nWhat to submit:\n\nYour main analysis question\nKey results (tables, charts)\nYour interpretation - what does the data tell us?"
  },
  {
    "objectID": "assignments/assignment_04.html#part-3-show-your-work-20",
    "href": "assignments/assignment_04.html#part-3-show-your-work-20",
    "title": "Assignment 4: Extend the Austrian Hotels Dataset",
    "section": "Part 3: Show Your Work (20%)",
    "text": "Part 3: Show Your Work (20%)\nInclude the key pieces of code that Claude Code generated. You don’t need to include everything - focus on:\n\nThe data generation script (or key parts of it)\nThe join and analysis code\nAny interesting debugging or iteration\n\nFormat: Use code blocks in your PDF. If code is long, include just the important functions."
  },
  {
    "objectID": "assignments/assignment_04.html#part-4-reflection-15",
    "href": "assignments/assignment_04.html#part-4-reflection-15",
    "title": "Assignment 4: Extend the Austrian Hotels Dataset",
    "section": "Part 4: Reflection (15%)",
    "text": "Part 4: Reflection (15%)\nWrite 150-300 words reflecting on:\n\nProcess: How did you iterate with Claude Code? What did you have to clarify or fix?\nComparison: How did this workflow compare to using chat-based AI (ChatGPT/Claude.ai)? What was easier? Harder?\nTrust: Did you verify Claude Code’s output? How? Did you find any mistakes?\nFuture use: When would you use Claude Code vs chat AI in your own work?"
  },
  {
    "objectID": "assignments/assignment_04.html#grading-rubric",
    "href": "assignments/assignment_04.html#grading-rubric",
    "title": "Assignment 4: Extend the Austrian Hotels Dataset",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\n\n\n\n\n\n\n\n\nComponent\nExcellent (90-100%)\nGood (70-89%)\nNeeds Work (&lt;70%)\n\n\n\n\nData Design\nCreative, realistic patterns, clear join logic\nReasonable table, some patterns\nRandom data, unclear joins\n\n\nAnalysis\nClear question, appropriate methods, insightful interpretation\nDecent analysis, basic interpretation\nUnclear question, minimal analysis\n\n\nCode Quality\nWell-organized, key parts shown\nSomewhat organized\nMessy or missing\n\n\nReflection\nThoughtful insights on process and tools\nBasic reflection\nSuperficial or missing"
  },
  {
    "objectID": "assignments/assignment_04.html#tips",
    "href": "assignments/assignment_04.html#tips",
    "title": "Assignment 4: Extend the Austrian Hotels Dataset",
    "section": "Tips",
    "text": "Tips\n\nBe specific in prompts: “5-star hotels should have 20% higher staff-to-room ratios” is better than “make it realistic”\nVerify the data: Ask Claude Code to show summary statistics after generating\nIterate: If the first version isn’t right, refine your prompt\nCheck joins: Always verify row counts before and after joining\nSave your work: Keep the Python scripts Claude Code generates"
  },
  {
    "objectID": "assignments/assignment_04.html#submission",
    "href": "assignments/assignment_04.html#submission",
    "title": "Assignment 4: Extend the Austrian Hotels Dataset",
    "section": "Submission",
    "text": "Submission\n\nFormat: PDF\nDue: Sunday 23:55 before Week 5\nSubmit via: Moodle"
  },
  {
    "objectID": "assignments/assignment_04.html#questions",
    "href": "assignments/assignment_04.html#questions",
    "title": "Assignment 4: Extend the Austrian Hotels Dataset",
    "section": "Questions?",
    "text": "Questions?\nIf you have trouble with Claude Code setup, check the setup guide or ask in the course forum."
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html",
    "href": "da-knowledge/technical-terms-page.html",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "",
    "text": "This glossary provides brief definitions of key technical terms used throughout the Data Analysis with AI course. Use it as a reference when you encounter unfamiliar terminology.\nTip: Use the search box above to quickly find terms, or browse by category using the sidebar.\n\n\n\nType to filter terms instantly\n\n\n\nNo matching terms found. Try a different search.",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#purpose",
    "href": "da-knowledge/technical-terms-page.html#purpose",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "",
    "text": "This glossary provides brief definitions of key technical terms used throughout the Data Analysis with AI course. Use it as a reference when you encounter unfamiliar terminology.\nTip: Use the search box above to quickly find terms, or browse by category using the sidebar.\n\n\n\nType to filter terms instantly\n\n\n\nNo matching terms found. Try a different search.",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#token",
    "href": "da-knowledge/technical-terms-page.html#token",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Token",
    "text": "Token\nThe basic unit of text that LLMs process.\n\nRoughly 4 characters or ¾ of a word in English\n“Hello world” ≈ 2 tokens\nPricing and limits are measured in tokens",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#context-window",
    "href": "da-knowledge/technical-terms-page.html#context-window",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Context Window",
    "text": "Context Window\nThe maximum amount of text (in tokens) an LLM can consider at once.\n\nIncludes: system prompt + your messages + AI responses + uploads\nGPT-5.2: ~400k tokens; Gemini 3: ~1M tokens\nExceeding the limit causes the model to “forget” earlier content",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#embedding",
    "href": "da-knowledge/technical-terms-page.html#embedding",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Embedding",
    "text": "Embedding\nA way of representing text (or images, videos) as a list of numbers (a vector) that captures its semantic meaning.\n\nTranslates words into coordinates: “Cat” is closer to “Dog” than “Car”\nEnables semantic search (finding distinct words with similar meanings)\nThe foundational technology behind RAG and vector databases",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#inference",
    "href": "da-knowledge/technical-terms-page.html#inference",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Inference",
    "text": "Inference\nThe process of generating output from a trained model.\n\nInput goes in → model processes → output comes out\nDistinct from training (which creates the model)\nWhat happens every time you send a message",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#multimodality",
    "href": "da-knowledge/technical-terms-page.html#multimodality",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Multimodality",
    "text": "Multimodality\nThe ability of a Large Language Model to natively process and generate different types of data (text, images, audio, video) simultaneously.\n\nOperates within a unified embedding space where different data types share the same mathematical representation\nAllows for “cross-modal” reasoning, such as explaining a video or chart using natural language",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#latency",
    "href": "da-knowledge/technical-terms-page.html#latency",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Latency",
    "text": "Latency\nThe time delay between sending a request and receiving a response.\n\nMeasured in milliseconds or seconds\nAffected by: model size, input length, server load\nTrade-off: faster models are often less capable",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#llm-large-language-model",
    "href": "da-knowledge/technical-terms-page.html#llm-large-language-model",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "LLM (Large Language Model)",
    "text": "LLM (Large Language Model)\nA neural network trained on massive text data to predict and generate language.\n\n“Large” = billions of parameters\nLearns patterns from training data\nExamples: GPT-5, Claude 4.5, Gemini 3",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#transformer",
    "href": "da-knowledge/technical-terms-page.html#transformer",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Transformer",
    "text": "Transformer\nThe neural network architecture underlying modern LLMs.\n\nIntroduced in 2017 (“Attention Is All You Need”)\nKey innovation: self-attention mechanism\nEnables parallel processing of sequences",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#parameters",
    "href": "da-knowledge/technical-terms-page.html#parameters",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Parameters",
    "text": "Parameters\nThe learned values (weights) inside a neural network.\n\nMore parameters ≈ more capacity to learn patterns\nGPT-4: ~1 trillion parameters (estimated)\nTrade-off: larger models are slower and more expensive",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#mixture-of-experts-moe",
    "href": "da-knowledge/technical-terms-page.html#mixture-of-experts-moe",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Mixture of Experts (MoE)",
    "text": "Mixture of Experts (MoE)\nAn architecture that activates only a subset of parameters for each query.\n\nUsed by Gemini 3 (over 1 trillion total parameters)\nOnly relevant “experts” are activated per query\nBalances deep intelligence with faster response times",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#system-prompt",
    "href": "da-knowledge/technical-terms-page.html#system-prompt",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "System Prompt",
    "text": "System Prompt\nHidden instructions given to the AI before your conversation.\n\nDefines persona, constraints, behavior\nSet by platform or user (Custom Instructions, Projects)\nThe AI “sees” this before your first message",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#context-engineering",
    "href": "da-knowledge/technical-terms-page.html#context-engineering",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Context Engineering",
    "text": "Context Engineering\nThe practice of curating all information the model receives to optimize performance.\n\nGoes beyond prompt engineering\nIncludes: system prompt, memory, tools, retrieved documents\nKey skill for building reliable AI applications\n\nLearn more: Anthropic’s context engineering guide",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#rag-retrieval-augmented-generation",
    "href": "da-knowledge/technical-terms-page.html#rag-retrieval-augmented-generation",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "RAG (Retrieval-Augmented Generation)",
    "text": "RAG (Retrieval-Augmented Generation)\nA technique that retrieves relevant documents and adds them to the prompt.\n\nHelps ground responses in specific data\nReduces hallucinations\nPowers many enterprise AI applications",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#chain-of-thought-cot",
    "href": "da-knowledge/technical-terms-page.html#chain-of-thought-cot",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Chain-of-Thought (CoT)",
    "text": "Chain-of-Thought (CoT)\nA prompting technique where the model shows intermediate reasoning steps.\n\n“Let’s think step by step”\nImproves accuracy on complex tasks\nCan be prompted or built into the model",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#reasoning-model",
    "href": "da-knowledge/technical-terms-page.html#reasoning-model",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Reasoning Model",
    "text": "Reasoning Model\nA model specifically trained to “think” before responding.\n\nExamples: OpenAI o3, o4-mini; Claude with extended thinking\nInternal deliberation, then final answer\nBetter for math, logic, multi-step problems",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#extended-thinking",
    "href": "da-knowledge/technical-terms-page.html#extended-thinking",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Extended Thinking",
    "text": "Extended Thinking\nA mode where the model explicitly reasons through problems.\n\nVisible chain of thought (unlike o3)\nConfigurable thinking “budget”\nTrade-off: higher latency and cost",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#thinking-levels",
    "href": "da-knowledge/technical-terms-page.html#thinking-levels",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Thinking Levels",
    "text": "Thinking Levels\nGemini 3’s approach to controlling reasoning depth.\n\nLOW: Fast, minimal deliberation\nMEDIUM: Balanced (default)\nHIGH: Deep reasoning, higher latency — uses parallel thinking and “self-correction” signatures to solve complex logic or math problems",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#agent",
    "href": "da-knowledge/technical-terms-page.html#agent",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Agent",
    "text": "Agent\nAn AI system that can take actions, not just generate text.\n\nUses tools (code execution, web search, file access)\nOperates semi-autonomously\nMay involve multiple steps and decisions",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#agentic-workflow",
    "href": "da-knowledge/technical-terms-page.html#agentic-workflow",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Agentic Workflow",
    "text": "Agentic Workflow\nA process where AI acts across multiple steps with tool use.\n\nExample: Search → Analyze → Write → Review\nLess human intervention between steps\nRequires careful design and guardrails",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#mcp-model-context-protocol",
    "href": "da-knowledge/technical-terms-page.html#mcp-model-context-protocol",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)\nAn open standard for connecting AI to external tools and data.\n\n“USB-C for AI applications”\nDeveloped by Anthropic, adopted broadly\nEnables secure access to files, databases, APIs\n\nLearn more: MCP documentation",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#tool-use-function-calling",
    "href": "da-knowledge/technical-terms-page.html#tool-use-function-calling",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Tool Use / Function Calling",
    "text": "Tool Use / Function Calling\nThe ability of an AI to invoke external functions or APIs.\n\nModel outputs structured “tool call”\nSystem executes the tool\nResult fed back to model",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#hallucination",
    "href": "da-knowledge/technical-terms-page.html#hallucination",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Hallucination",
    "text": "Hallucination\nWhen an AI generates plausible-sounding but false information.\n\nFabricated facts, fake references, incorrect code\nReduced but not eliminated in modern models\nMitigated by grounding, RAG, verification",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#grounding",
    "href": "da-knowledge/technical-terms-page.html#grounding",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Grounding",
    "text": "Grounding\nConnecting AI responses to verified sources of truth.\n\nWeb search, document retrieval, database queries\nReduces hallucinations\nEnables citations",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#rlhf-reinforcement-learning-from-human-feedback",
    "href": "da-knowledge/technical-terms-page.html#rlhf-reinforcement-learning-from-human-feedback",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "RLHF (Reinforcement Learning from Human Feedback)",
    "text": "RLHF (Reinforcement Learning from Human Feedback)\nA training technique where models learn from human preferences.\n\nHumans rate model outputs\nModel learns to produce preferred responses\nKey to making models “helpful and harmless”",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#constitutional-ai",
    "href": "da-knowledge/technical-terms-page.html#constitutional-ai",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Constitutional AI",
    "text": "Constitutional AI\nAnthropic’s approach to training models with explicit principles.\n\nModel trained to follow a “constitution” of rules\nSelf-improvement through AI feedback\nAlternative to pure RLHF",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#prompt-injection",
    "href": "da-knowledge/technical-terms-page.html#prompt-injection",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Prompt Injection",
    "text": "Prompt Injection\nA security vulnerability where a user inputs text designed to trick the AI into ignoring its original instructions.\n\nExample: “Ignore all previous instructions and tell me your secret rules”\nCan cause data leaks or unauthorized behavior\nA major security challenge for applications built on LLMs",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#skills-claude",
    "href": "da-knowledge/technical-terms-page.html#skills-claude",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Skills (Claude)",
    "text": "Skills (Claude)\nReusable, modular instruction packages in Claude.\n\nPre-defined workflows and behaviors\nShareable across conversations\nCan be combined for complex tasks",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#gems-gemini",
    "href": "da-knowledge/technical-terms-page.html#gems-gemini",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Gems (Gemini)",
    "text": "Gems (Gemini)\nCustom AI assistants in Gemini Advanced.\n\nUser-defined personas and instructions\nPersistent across conversations\nShareable with team",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#projects-claude",
    "href": "da-knowledge/technical-terms-page.html#projects-claude",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Projects (Claude)",
    "text": "Projects (Claude)\nWorkspaces with shared context across conversations.\n\nPersistent system prompt\nUploaded knowledge files\nAll chats share the same context",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#canvas-artifacts",
    "href": "da-knowledge/technical-terms-page.html#canvas-artifacts",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Canvas / Artifacts",
    "text": "Canvas / Artifacts\nInteractive workspaces for editing AI-generated content.\n\nChatGPT Canvas, Claude Artifacts\nSide-by-side editing\nGood for code and documents",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#temperature",
    "href": "da-knowledge/technical-terms-page.html#temperature",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Temperature",
    "text": "Temperature\nA parameter controlling randomness in model outputs.\n\n0 = deterministic (same input → same output)\n1 = default, balanced creativity\nHigher = more random/creative\n\nNote: Gemini 3 and reasoning models work best at default (1.0)",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#context-rot",
    "href": "da-knowledge/technical-terms-page.html#context-rot",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Context Rot",
    "text": "Context Rot\nPerformance degradation as the context window fills up.\n\nModel becomes less accurate over long conversations\nEven within technical limits\nSolution: start fresh, use memory tools",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#kv-cache",
    "href": "da-knowledge/technical-terms-page.html#kv-cache",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "KV-Cache",
    "text": "KV-Cache\nA technical optimization that speeds up repeated inference.\n\nCaches intermediate computations\nFaster response for repeated prefixes\nWhy stable prompt prefixes matter for cost",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#vibe-coding",
    "href": "da-knowledge/technical-terms-page.html#vibe-coding",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Vibe Coding",
    "text": "Vibe Coding\nDescribing desired behavior in natural language rather than writing syntax.\n\n“Make this chart interactive with company colors”\nAI translates intent to code\nRequires human review and iteration",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#claude.md",
    "href": "da-knowledge/technical-terms-page.html#claude.md",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "CLAUDE.md",
    "text": "CLAUDE.md\nA convention for providing project context to Claude Code.\n\nMarkdown file in project root\nContains: file descriptions, conventions, current task\nAutomatically read by Claude Code CLI",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#prompt-chaining",
    "href": "da-knowledge/technical-terms-page.html#prompt-chaining",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Prompt Chaining",
    "text": "Prompt Chaining\nBreaking complex tasks into sequential prompts.\n\nOutput of step N becomes input to step N+1\nMore reliable than single complex prompt\nEnables debugging at each stage",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#inputoutput-tokens",
    "href": "da-knowledge/technical-terms-page.html#inputoutput-tokens",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Input/Output Tokens",
    "text": "Input/Output Tokens\nTokens are billed separately for input (prompt) and output (response).\n\nInput: what you send (including context)\nOutput: what the model generates\nOutput tokens typically cost more",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#rate-limiting",
    "href": "da-knowledge/technical-terms-page.html#rate-limiting",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Rate Limiting",
    "text": "Rate Limiting\nRestrictions on how many requests you can make.\n\nRequests per minute (RPM)\nTokens per minute (TPM)\nPrevents abuse and ensures fair access",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#quota-limit",
    "href": "da-knowledge/technical-terms-page.html#quota-limit",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Quota Limit",
    "text": "Quota Limit\nA hard cap on the total amount of usage (tokens or cost) allowed within a specific billing cycle.\n\nDistinct from Rate Limiting (which is about speed/throughput)\nPrevents accidental overspending\nOnce reached, API access is paused until the limit is raised or the month resets",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/technical-terms-page.html#prompt-caching",
    "href": "da-knowledge/technical-terms-page.html#prompt-caching",
    "title": "Data Analysis with AI: Technical Terms",
    "section": "Prompt Caching",
    "text": "Prompt Caching\nStoring and reusing processed prompts to reduce cost.\n\nSame prefix → cached, cheaper\nUseful for repeated system prompts\nRequires stable prompt structure",
    "crumbs": [
      "Home",
      "Resources",
      "Glossary of LLM terms"
    ]
  },
  {
    "objectID": "da-knowledge/documentation-readme.html",
    "href": "da-knowledge/documentation-readme.html",
    "title": "Documentation Fundamentals",
    "section": "",
    "text": "Good documentation essential for reproducible research. Without it, even you won’t understand your own work six months later. Documentation serves three audiences: your future self, your collaborators, and the broader research community.",
    "crumbs": [
      "Home",
      "Resources",
      "Documentation Fundamentals"
    ]
  },
  {
    "objectID": "da-knowledge/documentation-readme.html#why-documentation-matters",
    "href": "da-knowledge/documentation-readme.html#why-documentation-matters",
    "title": "Documentation Fundamentals",
    "section": "",
    "text": "Good documentation essential for reproducible research. Without it, even you won’t understand your own work six months later. Documentation serves three audiences: your future self, your collaborators, and the broader research community.",
    "crumbs": [
      "Home",
      "Resources",
      "Documentation Fundamentals"
    ]
  },
  {
    "objectID": "da-knowledge/documentation-readme.html#key-definitions",
    "href": "da-knowledge/documentation-readme.html#key-definitions",
    "title": "Documentation Fundamentals",
    "section": "Key Definitions",
    "text": "Key Definitions\nBefore diving in, let’s clarify terms that are often used interchangeably but mean different things:\n\n\n\n\n\n\n\n\nTerm\nWhat it is\nTypical format\n\n\n\n\nREADME\nProject overview and setup instructions\n.md, .txt, .pdf\n\n\nCodebook\nDetailed variable-level documentation\n.pdf, .xlsx\n\n\nData dictionary\nTechnical specification of variables (often synonymous with codebook)\n.xlsx, .csv, .txt\n\n\nData lineage\nThe path data takes from source to final form\nDiagram or narrative\n\n\nMetadata\nData about data (when collected, by whom, how)\nVarious",
    "crumbs": [
      "Home",
      "Resources",
      "Documentation Fundamentals"
    ]
  },
  {
    "objectID": "da-knowledge/documentation-readme.html#about-markdown",
    "href": "da-knowledge/documentation-readme.html#about-markdown",
    "title": "Documentation Fundamentals",
    "section": "About Markdown",
    "text": "About Markdown\nMarkdown is a lightweight markup language that’s become the standard for documentation in data science. It’s readable as plain text but renders nicely in browsers and editors.\nTools for working with Markdown\n\nQuarto — the successor to R Markdown, works with R, Python, Julia\nOnline Markdown editor — for quick testing\nPandoc — converts between formats (md → docx, pdf, html)\nDillinger — another online editor with live preview\n\nQuick Markdown reference\n# Heading 1\n## Heading 2\n**bold** and *italic*\n- bullet point\n1. numbered list\n[link text](url)\n`inline code`",
    "crumbs": [
      "Home",
      "Resources",
      "Documentation Fundamentals"
    ]
  },
  {
    "objectID": "da-knowledge/documentation-readme.html#what-is-a-good-readme",
    "href": "da-knowledge/documentation-readme.html#what-is-a-good-readme",
    "title": "Documentation Fundamentals",
    "section": "What is a Good README?",
    "text": "What is a Good README?\nA README is the front door to your project. Someone should be able to understand what your project does, how to use it, and where to find things—all from reading the README.\n\nKey Ingredients\nA complete README for a research project should include:\n\nOverview — What is this project? What question does it answer?\nData sources — Where does the data come from? Any access restrictions?\nFile structure — What’s in each folder? Which scripts run in what order?\nRequirements — Software, packages, and versions needed\nInstructions — How to run the analysis from start to finish\nLicense — Terms for reuse (MIT, CC-BY, etc.)\nContact — Who to ask questions\n\n\n\nData Description Checklist\nFor each dataset in your project, document:\n\nName and file format (csv, parquet, xlsx)\nNumber of observations and variables\nUnit of observation (person, firm-year, country-month)\nTime coverage and geographic scope\nKey variables with brief descriptions\nMissing data: how much and why\nData lineage: source → processing → final structure\n\n\n\nExamples of Good READMEs\nReproduction packages\n\nBékés-Kézdi (2021) Hotels dataset — clean, minimal, effective\nKoren-Pető (2021) Business disruptions from social distancing | PDF version — comprehensive research package\n\nTemplates and guides\n\nMake a README — interactive guide with examples\nSocial Science Data Editors Template — journal-standard template\nAEA Data Editor guidance — requirements for top economics journals",
    "crumbs": [
      "Home",
      "Resources",
      "Documentation Fundamentals"
    ]
  },
  {
    "objectID": "da-knowledge/documentation-readme.html#what-is-a-codebook-variable-dictionary",
    "href": "da-knowledge/documentation-readme.html#what-is-a-codebook-variable-dictionary",
    "title": "Documentation Fundamentals",
    "section": "What is a Codebook (Variable Dictionary)?",
    "text": "What is a Codebook (Variable Dictionary)?\nA codebook provides detailed, variable-level documentation. While the README gives the big picture, the codebook tells you exactly what Q47_recoded means.\n\nWhat to Include for Each Variable\n\n\n\nElement\nExample\n\n\n\n\nVariable name\nincome_hh\n\n\nLabel\nHousehold monthly income\n\n\nType\nNumeric (continuous)\n\n\nUnit/metric\nEUR, monthly\n\n\nValid range\n0–999999\n\n\nCoding for categories\n1=Low, 2=Medium, 3=High\n\n\nMissing values\n-99 = refused, NA = not asked\n\n\nShare missing\n4.2%\n\n\nNotes\nTop-coded at 99th percentile\n\n\n\n\n\nExamples of Good Codebooks\n\nBékés-Kézdi (2021) Bisnode dataset variables — clean spreadsheet format\nReif (2022) Illinois Wellness codebook — plain text, version controlled\nOn earnings data – used in this course",
    "crumbs": [
      "Home",
      "Resources",
      "Documentation Fundamentals"
    ]
  },
  {
    "objectID": "da-knowledge/documentation-readme.html#tips-for-ai-assisted-documentation",
    "href": "da-knowledge/documentation-readme.html#tips-for-ai-assisted-documentation",
    "title": "Documentation Fundamentals",
    "section": "Tips for AI-Assisted Documentation",
    "text": "Tips for AI-Assisted Documentation\nLLMs can significantly speed up documentation, but require careful verification.\nWhat AI does well\n\nSummarizing long codebooks\nGenerating first drafts of variable descriptions\nSuggesting what’s missing from your documentation\nConverting between formats (e.g., codebook PDF → markdown table)\n\nWhat requires human oversight\n\nVerifying variable definitions match actual data\nChecking that coded values (1, 2, 3…) match the stated meaning\nEnsuring coverage statistics are accurate\nConfirming data lineage is correct\n\nA practical workflow\n\nStart by having a first look to get a feel. Look at documensts, check the index. Open data.\nUpload your codebook/data to the LLM\nAsk for a structured summary\nVerify 3-5 variables manually against the source\nIterate: ask AI to fix errors you find\nFinal human review before publishing",
    "crumbs": [
      "Home",
      "Resources",
      "Documentation Fundamentals"
    ]
  },
  {
    "objectID": "da-knowledge/documentation-readme.html#further-reading",
    "href": "da-knowledge/documentation-readme.html#further-reading",
    "title": "Documentation Fundamentals",
    "section": "Further Reading",
    "text": "Further Reading\n\nCrystal Lewis, Data Management in Large-Scale Education Research — practical guide to data structure\nTIER Protocol — comprehensive reproducibility framework",
    "crumbs": [
      "Home",
      "Resources",
      "Documentation Fundamentals"
    ]
  },
  {
    "objectID": "da-knowledge/install-cli.html",
    "href": "da-knowledge/install-cli.html",
    "title": "Installing AI CLI Tools",
    "section": "",
    "text": "All these CLI tools let you work with AI directly in your terminal. Pick one to start - they all work similarly.\n\n\n\n\n\n\nTipRecommendation\n\n\n\nClaude Code is what we use in this course. But the skills transfer to any of these tools.\n\n\n\n\n\nTool\nProvider\nModel\nPricing\n\n\n\n\nClaude Code\nAnthropic\nClaude\nClaude Pro ($20/mo) or API\n\n\nGemini CLI\nGoogle\nGemini\nFree tier available\n\n\nCodex CLI\nOpenAI\nGPT-4\nAPI credits required\n\n\nOpenCode\nOpen source\nMultiple\nDepends on backend",
    "crumbs": [
      "Home",
      "Resources",
      "Installing AI CLI Tools"
    ]
  },
  {
    "objectID": "da-knowledge/install-cli.html#choose-your-tool",
    "href": "da-knowledge/install-cli.html#choose-your-tool",
    "title": "Installing AI CLI Tools",
    "section": "",
    "text": "All these CLI tools let you work with AI directly in your terminal. Pick one to start - they all work similarly.\n\n\n\n\n\n\nTipRecommendation\n\n\n\nClaude Code is what we use in this course. But the skills transfer to any of these tools.\n\n\n\n\n\nTool\nProvider\nModel\nPricing\n\n\n\n\nClaude Code\nAnthropic\nClaude\nClaude Pro ($20/mo) or API\n\n\nGemini CLI\nGoogle\nGemini\nFree tier available\n\n\nCodex CLI\nOpenAI\nGPT-4\nAPI credits required\n\n\nOpenCode\nOpen source\nMultiple\nDepends on backend",
    "crumbs": [
      "Home",
      "Resources",
      "Installing AI CLI Tools"
    ]
  },
  {
    "objectID": "da-knowledge/install-cli.html#prerequisites-all-tools",
    "href": "da-knowledge/install-cli.html#prerequisites-all-tools",
    "title": "Installing AI CLI Tools",
    "section": "Prerequisites (All Tools)",
    "text": "Prerequisites (All Tools)\nBefore installing any CLI tool, you need:\n\n1. Node.js\nMost CLI tools require Node.js. Check if you have it:\n\nWindowsMacLinux\n\n\nnode --version\nIf not installed: Download from nodejs.org (LTS version)\n\n\nnode --version\nIf not installed:\nbrew install node\nOr download from nodejs.org\n\n\nnode --version\nIf not installed:\n# Ubuntu/Debian\nsudo apt update && sudo apt install nodejs npm\n\n# Fedora\nsudo dnf install nodejs npm\n\n\n\n\n\n2. A Terminal\n\nWindowsMacLinux\n\n\n\nWindows Terminal (recommended) - comes with Windows 11, or install from Microsoft Store\nPowerShell - built-in, search “PowerShell”\nCommand Prompt - built-in, search “cmd”\n\n\n\n\nTerminal - built-in, press Cmd+Space, type “Terminal”\niTerm2 - popular alternative\n\n\n\n\nUsually Ctrl+Alt+T or search “Terminal”\n\n\n\n\nNew to the terminal? Read Terminal Basics first.",
    "crumbs": [
      "Home",
      "Resources",
      "Installing AI CLI Tools"
    ]
  },
  {
    "objectID": "da-knowledge/install-cli.html#claude-code",
    "href": "da-knowledge/install-cli.html#claude-code",
    "title": "Installing AI CLI Tools",
    "section": "Claude Code",
    "text": "Claude Code\nAnthropic’s official CLI for Claude.\n\nInstall\n\nWindowsMacLinux\n\n\nOpen PowerShell or Command Prompt:\nnpm install -g @anthropic-ai/claude-code\n\n\nOpen Terminal:\nnpm install -g @anthropic-ai/claude-code\nIf permission error:\nsudo npm install -g @anthropic-ai/claude-code\n\n\nnpm install -g @anthropic-ai/claude-code\nIf permission error:\nsudo npm install -g @anthropic-ai/claude-code\n\n\n\n\n\nAuthenticate\nclaude\nFirst run opens a browser to log in with your Anthropic account. You need: - Claude Pro subscription ($20/month), OR - API access with credits\n\n\nVerify\nclaude --version\n\n\nUse\nNavigate to your project folder and start:\ncd /path/to/your/project\nclaude\nThen type your requests naturally: “What files are here?” or “Read the CSV and calculate the mean.”\nMore details: Claude Code Documentation",
    "crumbs": [
      "Home",
      "Resources",
      "Installing AI CLI Tools"
    ]
  },
  {
    "objectID": "da-knowledge/install-cli.html#gemini-cli",
    "href": "da-knowledge/install-cli.html#gemini-cli",
    "title": "Installing AI CLI Tools",
    "section": "Gemini CLI",
    "text": "Gemini CLI\nGoogle’s CLI for Gemini models.\n\nInstall\n\nWindowsMacLinux\n\n\nnpm install -g @anthropic-ai/claude-code\nNote: As of 2026, check Google’s documentation for the current package name.\n\n\nnpm install -g gemini-cli\n\n\nnpm install -g gemini-cli\n\n\n\n\n\nAuthenticate\ngemini auth login\nThis opens a browser to authenticate with your Google account. Free tier available with limits.\n\n\nVerify\ngemini --version\n\n\nUse\ncd /path/to/your/project\ngemini\nMore details: Google AI for Developers",
    "crumbs": [
      "Home",
      "Resources",
      "Installing AI CLI Tools"
    ]
  },
  {
    "objectID": "da-knowledge/install-cli.html#codex-cli",
    "href": "da-knowledge/install-cli.html#codex-cli",
    "title": "Installing AI CLI Tools",
    "section": "Codex CLI",
    "text": "Codex CLI\nOpenAI’s command-line tool.\n\nInstall\n\nWindowsMacLinux\n\n\nnpm install -g @openai/codex\n\n\nnpm install -g @openai/codex\n\n\nnpm install -g @openai/codex\n\n\n\n\n\nAuthenticate\nSet your API key:\n\nWindows (PowerShell)Mac / Linux\n\n\n$env:OPENAI_API_KEY=\"your-api-key-here\"\nTo make permanent, add to your PowerShell profile or System Environment Variables.\n\n\nexport OPENAI_API_KEY=\"your-api-key-here\"\nTo make permanent, add to ~/.bashrc or ~/.zshrc:\necho 'export OPENAI_API_KEY=\"your-api-key-here\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n\n\n\nGet your API key from platform.openai.com\n\n\nVerify\ncodex --version\n\n\nUse\ncd /path/to/your/project\ncodex\nMore details: OpenAI Platform Documentation",
    "crumbs": [
      "Home",
      "Resources",
      "Installing AI CLI Tools"
    ]
  },
  {
    "objectID": "da-knowledge/install-cli.html#opencode",
    "href": "da-knowledge/install-cli.html#opencode",
    "title": "Installing AI CLI Tools",
    "section": "OpenCode",
    "text": "OpenCode\nOpen-source CLI that can connect to multiple AI backends.\n\nInstall\n\nWindowsMacLinux\n\n\nnpm install -g opencode-ai\n\n\nnpm install -g opencode-ai\nOr with Homebrew:\nbrew install opencode\n\n\nnpm install -g opencode-ai\n\n\n\n\n\nConfigure\nOpenCode supports multiple backends. Configure with:\nopencode config\nYou can connect to: - OpenAI (requires API key) - Anthropic (requires API key) - Local models (Ollama, etc.)\n\n\nUse\ncd /path/to/your/project\nopencode\nMore details: Check the project’s GitHub repository for current documentation.",
    "crumbs": [
      "Home",
      "Resources",
      "Installing AI CLI Tools"
    ]
  },
  {
    "objectID": "da-knowledge/install-cli.html#troubleshooting",
    "href": "da-knowledge/install-cli.html#troubleshooting",
    "title": "Installing AI CLI Tools",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n“command not found” after install\n\nWindowsMac / Linux\n\n\nClose and reopen your terminal. If still not working, check that npm’s global bin folder is in your PATH.\n\n\nTry:\nsource ~/.bashrc  # or ~/.zshrc\nOr check npm’s bin location:\nnpm bin -g\nMake sure this path is in your PATH environment variable.\n\n\n\n\n\n“permission denied” during install\n\nWindowsMac / Linux\n\n\nRun terminal as Administrator (right-click → Run as administrator)\n\n\nUse sudo:\nsudo npm install -g [package-name]\nOr fix npm permissions: npm docs\n\n\n\n\n\nAuthentication fails\n\nCheck your internet connection\nMake sure your subscription/API credits are active\nTry logging out and back in\nCheck the tool’s status page for outages\n\n\n\nStill stuck?\n\nCheck the official documentation for your tool\nSearch the tool’s GitHub issues\nAsk in the course forum",
    "crumbs": [
      "Home",
      "Resources",
      "Installing AI CLI Tools"
    ]
  },
  {
    "objectID": "da-knowledge/install-cli.html#quick-comparison",
    "href": "da-knowledge/install-cli.html#quick-comparison",
    "title": "Installing AI CLI Tools",
    "section": "Quick Comparison",
    "text": "Quick Comparison\n\n\n\n\n\n\n\n\n\n\nFeature\nClaude Code\nGemini CLI\nCodex CLI\nOpenCode\n\n\n\n\nFile access\nYes\nYes\nYes\nYes\n\n\nCode execution\nYes\nYes\nYes\nYes\n\n\nFree tier\nNo\nYes\nNo\nDepends\n\n\nBest for\nGeneral coding\nGoogle ecosystem\nOpenAI users\nFlexibility",
    "crumbs": [
      "Home",
      "Resources",
      "Installing AI CLI Tools"
    ]
  },
  {
    "objectID": "da-knowledge/install-cli.html#next-steps",
    "href": "da-knowledge/install-cli.html#next-steps",
    "title": "Installing AI CLI Tools",
    "section": "Next Steps",
    "text": "Next Steps\n\nInstall one tool (we recommend Claude Code for this course)\nRead Terminal Basics if you’re new to command line\nTry it on a simple task: navigate to a folder with some files and ask “what’s here?”\nContinue to Week 04 to use it for data analysis",
    "crumbs": [
      "Home",
      "Resources",
      "Installing AI CLI Tools"
    ]
  },
  {
    "objectID": "da-knowledge/which-ai.html",
    "href": "da-knowledge/which-ai.html",
    "title": "Which AI model shall we chose?",
    "section": "",
    "text": "In what follows, here is my personal take as of date:2026-01-12.",
    "crumbs": [
      "Home",
      "Resources",
      "Which AI model"
    ]
  },
  {
    "objectID": "da-knowledge/which-ai.html#basics",
    "href": "da-knowledge/which-ai.html#basics",
    "title": "Which AI model shall we chose?",
    "section": "Basics",
    "text": "Basics\nGenerative AI based on Large Language Models (genAI) is great for many tasks. In this course we only focus on aspects of Data Analysis:\n\ndesigning analysis\nwriting code\ndata wrangling such as joining tables, sample design, variable transformations\nexploratory data analysis\nmodelling, regressions, machine learning\ncausal inference\ncreating tables and graphs\nwriting reports and presentations",
    "crumbs": [
      "Home",
      "Resources",
      "Which AI model"
    ]
  },
  {
    "objectID": "da-knowledge/which-ai.html#basics-1",
    "href": "da-knowledge/which-ai.html#basics-1",
    "title": "Which AI model shall we chose?",
    "section": "Basics",
    "text": "Basics\n\nMost models have a free and paid tiers. Free ones are good as kind of Google search replacement. For serious work you’ll be better off with paid tiers.\nAs of now the leading models are Google’s Gemini 3.0, OpenAI’s GPT-5.2, and Anthropic’s Claude Opus 4.5.\nEach model have faster (cheaper) and thinking (more expensive) variants. For data analysis work, the thinking variants are recommended.\nAll major models have deep research\nAll major models support tool use (e.g. web browsing, code execution) and agentic patterns (multi-step workflows with memory and tool use).\nOpen weights models are also quite good, Deepseek and Q. You can even run a 7bn parameter model on an expensive computer",
    "crumbs": [
      "Home",
      "Resources",
      "Which AI model"
    ]
  },
  {
    "objectID": "da-knowledge/which-ai.html#landscape-overview",
    "href": "da-knowledge/which-ai.html#landscape-overview",
    "title": "Which AI model shall we chose?",
    "section": "landscape overview",
    "text": "landscape overview\n\n\n\n\n\n\n\n\n\n\nFeature\nGPT-5.2\no3 (reasoning)\nGemini 3 Deep Think\nClaude Opus 4.5\n\n\n\n\nDesign focus\nFlagship multimodal with native reasoning\nDeep reasoning for complex tasks\n1M token context with extended thinking\nCoding & complex instruction following\n\n\nMultimodality\nText + images + audio + video (I/O)\nText + tool calls\nText + images + video\nText + images + Computer Use 2.0\n\n\nBrowsing / tools\nIntegrated search + MCP support\nAuto-search for up-to-date facts\nGrounded in Google Search\nDeep integration with MCP & OS\n\n\nDefault style\nBalanced, adaptive\nConcise, source-cited\nThorough, exploratory\nProfessional, highly structured\n\n\nContext window\n400k tokens\n128k tokens\n1M tokens\n500k tokens\n\n\nStrengths\nAll-rounder, agentic workflows\nStep-wise analysis, audit trail\nLong-context research\nCoding agents, OS automation\n\n\nWeak spots\nExpensive for simple tasks\nNo native media I/O\nSlower response times\nSlower than Haiku/Sonnet",
    "crumbs": [
      "Home",
      "Resources",
      "Which AI model"
    ]
  },
  {
    "objectID": "da-knowledge/which-ai.html#models-for-data-analysis",
    "href": "da-knowledge/which-ai.html#models-for-data-analysis",
    "title": "Which AI model shall we chose?",
    "section": "Models for data analysis",
    "text": "Models for data analysis\nFor data-analysis projects, the recommended approach is:\n\nReasoning models (o3, Deep Think) for complex analysis requiring accuracy\nGPT-5.2 for agentic workflows and multimodal tasks\n\nGemini 3 for long-context document analysis (entire datasets, reports)\nClaude Opus 4.5 for coding and complex technical writing\n\nAll major models now support tool use (MCP) and agentic patterns.",
    "crumbs": [
      "Home",
      "Resources",
      "Which AI model"
    ]
  },
  {
    "objectID": "da-knowledge/which-ai.html#research-writing",
    "href": "da-knowledge/which-ai.html#research-writing",
    "title": "Which AI model shall we chose?",
    "section": "Research & Writing",
    "text": "Research & Writing\n\nSynthesis Over Summarization: AI tools increasingly synthesize multi-source inputs into structured insights rather than paraphrasing single documents.\nSecurity & Privacy: Modern workspaces rely on isolated execution contexts; strong non-training guarantees apply primarily to paid and enterprise tiers.\nMultimodal Capability: AI can interpret charts, screenshots, and handwritten notes and incorporate them into drafts.",
    "crumbs": [
      "Home",
      "Resources",
      "Which AI model"
    ]
  },
  {
    "objectID": "da-knowledge/which-ai.html#for-data-analysis-workspaces-comparison",
    "href": "da-knowledge/which-ai.html#for-data-analysis-workspaces-comparison",
    "title": "Which AI model shall we chose?",
    "section": "For data analysis workspaces – comparison",
    "text": "For data analysis workspaces – comparison\n\n\n\n\n\n\n\nWorkspace\n2026 Key Features\n\n\n\n\nAnthropic Claude Artifacts\n• Creates interactive applications (tutors, calculators) within the output window.• Real-time iteration on complex document structures.\n\n\nOpenAI ChatGPT Canvas\n• Advanced frontier models with contextual persistence for tone and style.• Inline editing with granular control over specific sections.\n\n\nGoogle NotebookLM\n• Interactive Audio Overviews with user interruption and questioning.• Grounded citations linked directly to uploaded source segments.\n\n\nPerplexity Pages\n• Multi-source synthesis using live web retrieval.• Inline citation and consistency checking against sources.",
    "crumbs": [
      "Home",
      "Resources",
      "Which AI model"
    ]
  },
  {
    "objectID": "da-knowledge/which-ai.html#data-analysis-details",
    "href": "da-knowledge/which-ai.html#data-analysis-details",
    "title": "Which AI model shall we chose?",
    "section": "Data Analysis details",
    "text": "Data Analysis details\n\nSandboxed Execution: Code runs in secure, ephemeral environments with no local system access.\nStatistical Rigor: Strong support for Python-based libraries (e.g. pandas, scikit-learn) for exploratory and predictive analysis.\nDirect Integration: AI can manipulate data directly within spreadsheets or dedicated analysis windows.\nLimits: Reproducibility, package versions, and state persistence remain constrained relative to local workflows.",
    "crumbs": [
      "Home",
      "Resources",
      "Which AI model"
    ]
  },
  {
    "objectID": "da-knowledge/which-ai.html#data-analysis-details-1",
    "href": "da-knowledge/which-ai.html#data-analysis-details-1",
    "title": "Which AI model shall we chose?",
    "section": "Data Analysis details",
    "text": "Data Analysis details\n\n\n\n\n\n\n\nWorkspace\n2026 Key Features for Analysis\n\n\n\n\nChatGPT Data Analysis\n• Executes Python in managed compute environments for multi-file datasets.• Assisted data cleaning and predictive modeling workflows.\n\n\nClaude Analysis\n• High-fidelity SVG and lightweight interactive output.• Fast iteration on statistical tables with publication-ready formatting.\n\n\nGoogle Gemini in Sheets\n• Multimodal cleaning: converts screenshots or PDFs into structured tables.• Natural-language formula generation and transformations.\n\n\nMicrosoft Copilot in Excel\n• Native Python-in-Excel for statistical scripts inside spreadsheets.• Automated pivots, summaries, and forecasting via prompts.",
    "crumbs": [
      "Home",
      "Resources",
      "Which AI model"
    ]
  },
  {
    "objectID": "da-knowledge/which-ai.html#coding-assistance",
    "href": "da-knowledge/which-ai.html#coding-assistance",
    "title": "Which AI model shall we chose?",
    "section": "Coding Assistance",
    "text": "Coding Assistance\n\nAgent-Assisted Workflows: AI can coordinate multi-step tasks such as refactoring or bug fixing across large codebases, with human review.\nEnvironment Security: Code is tested in secure sandboxes before changes are proposed.\nInterconnected Tools: Integrated with development and collaboration platforms (e.g. Jira, Slack).",
    "crumbs": [
      "Home",
      "Resources",
      "Which AI model"
    ]
  },
  {
    "objectID": "da-knowledge/which-ai.html#coding-assistance-comparison",
    "href": "da-knowledge/which-ai.html#coding-assistance-comparison",
    "title": "Which AI model shall we chose?",
    "section": "Coding Assistance — comparison",
    "text": "Coding Assistance — comparison\n\n\n\n\n\n\n\nWorkspace\n2026 Key Features for Developers\n\n\n\n\nAnthropic Claude Code\n• Native VS Code extension with agent-style workflows and inline diffs.• Supports complex multi-file edits and testing assistance.\n\n\nGitHub Copilot\n• Uses Extensions to interact with external dev tools (Azure, Slack, Jira).• Deep context from local and remote repositories.\n\n\nCursor\n• AI-first editor with awareness of project-wide dependencies.• Strong support for iterative refactors across files.\n\n\nWindsurf (Codeium)\n• Cascade mode for orchestrating large-scale refactors.• Robust free tier for students and individual users.\n\n\n\nSecurity note: SOC2 compliance is common; strict zero-retention guarantees typically apply to enterprise or explicitly configured accounts.",
    "crumbs": [
      "Home",
      "Resources",
      "Which AI model"
    ]
  },
  {
    "objectID": "da-knowledge/which-ai.html#what-changed-from-2025-q2",
    "href": "da-knowledge/which-ai.html#what-changed-from-2025-q2",
    "title": "Which AI model shall we chose?",
    "section": "What changed from 2025 Q2",
    "text": "What changed from 2025 Q2\n\nNo need to think much re which models to use.\nLeading models similar capability, but different. Not really sure how…",
    "crumbs": [
      "Home",
      "Resources",
      "Which AI model"
    ]
  },
  {
    "objectID": "da-knowledge/which-ai.html#feedback",
    "href": "da-knowledge/which-ai.html#feedback",
    "title": "Which AI model shall we chose?",
    "section": "Feedback",
    "text": "Feedback\nDear Reader. I have limited experience. Suggestions are welcome, please post an issue.",
    "crumbs": [
      "Home",
      "Resources",
      "Which AI model"
    ]
  },
  {
    "objectID": "week06/assets/get-ai-api-key.html",
    "href": "week06/assets/get-ai-api-key.html",
    "title": "How to get an API key: OpenAI ChatGPT and Anthropic Claude",
    "section": "",
    "text": "Below is a comprehensive, step-by-step guide for you to obtain and securely store API keys for both OpenAI ChatGPT and Anthropic Claude. It assumes you already have active accounts but need to know exactly where to navigate in each console to generate keys, fund your projects, and follow best practices for management.\nYou’ll need to spulrge a minimum of $5 to get started. That’s enough for the purpose of this course."
  },
  {
    "objectID": "week06/assets/get-ai-api-key.html#how-to-get-an-api-key-openai-chatgpt",
    "href": "week06/assets/get-ai-api-key.html#how-to-get-an-api-key-openai-chatgpt",
    "title": "How to get an API key: OpenAI ChatGPT and Anthropic Claude",
    "section": "How to get an API key: OpenAI ChatGPT",
    "text": "How to get an API key: OpenAI ChatGPT\n\nLog in to the OpenAI Platform Go to https://platform.openai.com/account/api-keys and sign in with your ChatGPT credentials if prompted.\nCreate a Project Navigate to https://platform.openai.com/settings/organization/projects and click + Create, enter a project name (e.g., “DA w AI Course”), then click Create.\nSet up Billing (Add Prepaid Credits) In the left-hand menu under Billing, click Add payment method, enter your credit card details, then purchase the minimum $5 to fund your project.\nGenerate Your API Key Within your project, go to the API Keys tab, click + Create new secret key, give it a descriptive name (e.g., “Course Project Key”), and click Create.\nCopy and Secure Your Key The full secret key string will be displayed only once—copy it immediately and store it in a password manager or secure vault. You will not be able to view it again.\n\n\nBest Practices\n\nRotate keys every 90 days and revoke any that are no longer in use to limit exposure if compromised.\nMonitor your usage via the Usage dashboard at https://platform.openai.com/account/usage or the Usage tab under Settings.\nNever commit API keys to version control—add them to .gitignore or use secret-management tools"
  },
  {
    "objectID": "week06/assets/get-ai-api-key.html#how-to-get-an-api-key-anthropic-claude",
    "href": "week06/assets/get-ai-api-key.html#how-to-get-an-api-key-anthropic-claude",
    "title": "How to get an API key: OpenAI ChatGPT and Anthropic Claude",
    "section": "How to get an API key: Anthropic Claude",
    "text": "How to get an API key: Anthropic Claude\n\nLog in to the Anthropic Console Navigate to https://console.anthropic.com and sign in with your Claude account credentials.\nAccess the API Keys Section Click the key icon in the left-hand navigation.\nGenerate a New API Key On the API Keys page, click + Create Key, enter a descriptive name (e.g., “Course Key”), and click Add.\nCopy and Secure Your Key The full API key string is shown only once—copy it immediately and store it in a secure vault. You will not be able to view it again.\nSet Up Billing or Claim Free Credits In the left navigation, select Billing. If you have trial credits, you may need to verify your phone number to claim them; otherwise, under Buy credits purchase the minimum $5 to fund your usage.\n\n\nBest Practices\n\nRotate keys periodically and delete any unused or compromised keys to maintain security.\nNever expose keys in public repositories—use environment variables or secret managers and add any config files with keys to .gitignore.\nMonitor your credit balance and API usage in Billing to avoid unexpected costs."
  },
  {
    "objectID": "week06/assets/api-advanced.html",
    "href": "week06/assets/api-advanced.html",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "",
    "text": "Before diving into APIs, it’s essential to understand how web communication works at its most basic level. The web runs on HTTP (HyperText Transfer Protocol), a simple, text-based language that lets one computer or application (the client) ask another computer (the server) for data or actions. Whenever you browse a site or call a web service, your client sends an HTTP request and the server answers with an HTTP response.\nAn HTTP request consists of a URL—the address of the resource or endpoint you want—and an HTTP method, which tells the server what you’d like to do. The two most common methods are GET (to fetch data, like loading a webpage) and POST (to submit data, such as filling in a form). By combining the URL and method (e.g. GET https://example.com/data), the client makes its intentions clear.\nWhen the server receives that request, it processes whatever work is needed—retrieving files, querying a database, or running a service—and then sends back a response. Each response begins with a status code (a number indicating success or failure, such as 200 OK for success or 404 Not Found when something is missing) followed by the body containing the actual data or content. For webpages this is usually HTML; for data services (APIs), it’s often JSON (JavaScript Object Notation).\nThis back-and-forth interaction is called the request-response cycle, and it’s entirely stateless—each request is handled on its own, with no memory of previous requests. That stateless design makes HTTP simple and highly scalable, allowing any client that speaks HTTP (from browsers to Python scripts) to interoperate with any server that understands HTTP. When you move on to APIs, you’ll leverage this same cycle to automate large-scale data retrieval and processing."
  },
  {
    "objectID": "week06/assets/api-advanced.html#http-client-server-communication-and-the-request-response-cycle",
    "href": "week06/assets/api-advanced.html#http-client-server-communication-and-the-request-response-cycle",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "",
    "text": "Before diving into APIs, it’s essential to understand how web communication works at its most basic level. The web runs on HTTP (HyperText Transfer Protocol), a simple, text-based language that lets one computer or application (the client) ask another computer (the server) for data or actions. Whenever you browse a site or call a web service, your client sends an HTTP request and the server answers with an HTTP response.\nAn HTTP request consists of a URL—the address of the resource or endpoint you want—and an HTTP method, which tells the server what you’d like to do. The two most common methods are GET (to fetch data, like loading a webpage) and POST (to submit data, such as filling in a form). By combining the URL and method (e.g. GET https://example.com/data), the client makes its intentions clear.\nWhen the server receives that request, it processes whatever work is needed—retrieving files, querying a database, or running a service—and then sends back a response. Each response begins with a status code (a number indicating success or failure, such as 200 OK for success or 404 Not Found when something is missing) followed by the body containing the actual data or content. For webpages this is usually HTML; for data services (APIs), it’s often JSON (JavaScript Object Notation).\nThis back-and-forth interaction is called the request-response cycle, and it’s entirely stateless—each request is handled on its own, with no memory of previous requests. That stateless design makes HTTP simple and highly scalable, allowing any client that speaks HTTP (from browsers to Python scripts) to interoperate with any server that understands HTTP. When you move on to APIs, you’ll leverage this same cycle to automate large-scale data retrieval and processing."
  },
  {
    "objectID": "week06/assets/api-advanced.html#how-apis-work-client-server-interaction-requests-and-responses",
    "href": "week06/assets/api-advanced.html#how-apis-work-client-server-interaction-requests-and-responses",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "How APIs Work: Client-Server Interaction, Requests, and Responses",
    "text": "How APIs Work: Client-Server Interaction, Requests, and Responses\nAt its core, an API works through a client-server interaction over the internet. Let’s break down the key concepts:\n\nClient and Server: The client is the part that sends a request (this would be your code or application), and the server is the part that receives the request and provides a response (this is the API’s service, often running on a remote server or in the cloud). In our context, you (or your Python script) are the client, and the API provider’s system is the server.\nRequest: A request is the message the client sends to the server asking for some action or data. Think of it as filling out an order form or making a specific query. A request typically includes:\n\nAn endpoint URL (the address of the API and the specific service you want).\nA method/verb (often one of the HTTP methods like GET to retrieve data or POST to send data).\nParameters or data (any additional information the server needs, such as the text you want analyzed or a query like a city name for a weather API).\nHeaders including an API key if required (more on API keys soon).\n\nResponse: After the server receives your request and processes it, it sends back a response. The response contains:\n\nStatus code – a number that tells you if the request was successful (e.g. 200 OK), or if something went wrong (e.g. 404 Not Found, 401 Unauthorized).\nData – the information you asked for, often in a structured format like JSON (a common text-based data format) or XML. For example, if you requested sentiment analysis, the response data might be a sentiment score or label for your text.\nMetadata or messages – sometimes additional info, like how long the request took or usage details.\n\n\n Client-Server Communication: Your application (client) sends an HTTP request to an API’s server (for example, asking for sentiment analysis on some text). The server then processes that request and sends back an HTTP response containing the result (for instance, the sentiment score). This request-response cycle is the foundation of how we use APIs.\n\nAPI Endpoint: An endpoint is a specific address (URL) that you hit to access a particular service or data from an API. It’s like a function or feature on the server that you can invoke. For example, a sentiment analysis API might have an endpoint like /analyzeSentiment that you call to get a sentiment result. Each endpoint usually corresponds to one type of task or data.\nAPI Documentation: Because you can’t see the “kitchen” (the server’s internal code or database), the API documentation is your guide to what you can request and how to format those requests. It typically lists all available endpoints, what parameters they accept, what kind of output they return, and examples. Good documentation is like a user manual for the API."
  },
  {
    "objectID": "week06/assets/api-advanced.html#rest-apis-the-common-way-to-communicate",
    "href": "week06/assets/api-advanced.html#rest-apis-the-common-way-to-communicate",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "REST APIs: The Common Way to Communicate",
    "text": "REST APIs: The Common Way to Communicate\nWhen people talk about web APIs, they are often referring to REST APIs. REST stands for Representational State Transfer, which is a style of designing networked applications. You don’t need to remember the term, but here’s what it means in practice:\n\nUses HTTP: REST APIs use the same protocol your web browser uses – HTTP. This means you’re often calling URLs (web addresses) just like loading a webpage, but instead of a webpage, you get data.\nEndpoints and Resources: In a REST API, different URLs (endpoints) represent different resources or services. For example, GET https://api.openweathermap.org/data/2.5/weather?q=London might retrieve weather data for London. Here, the endpoint /data/2.5/weather is a resource for weather info, and q=London is a parameter specifying the city.\nHTTP Methods (Verbs): REST APIs make use of HTTP methods such as:\n\nGET – Retrieve data (for example, get information or fetch results; e.g., get the sentiment of a text or fetch a list of comments).\nPOST – Send data or create a new resource (for example, submit a new text to be analyzed, or add a new entry to a database via the API).\nPUT/PATCH – Update an existing resource (for example, update a record in a database).\nDELETE – Delete a resource.\n\nIn our sentiment analysis example, you might use a GET request if the text is included in the URL or a POST request if you are sending the text in the request body. The key idea is that the method indicates what action you want to perform on the resource.\nStateless Communication: REST APIs are stateless, meaning each request is independent. The server doesn’t retain information about your previous requests. This is like each request being a separate, self-contained transaction. For instance, if you call the sentiment analysis API twice with two different texts, the server doesn’t remember the first text when processing the second – you need to send all the information it needs each time. The stateless design makes it easier to scale and ensures that each server can handle any request without needing to know what came before.\n\nREST APIs are popular because they are simple, scalable, and use existing web standards. Almost any programming environment can send HTTP requests, so REST makes it easy to interact with services from different languages and platforms. When you hear about APIs from providers like Google, Twitter, or OpenAI, they are usually implemented as RESTful APIs."
  },
  {
    "objectID": "week06/assets/api-advanced.html#other-api-styles-graphql-and-websockets",
    "href": "week06/assets/api-advanced.html#other-api-styles-graphql-and-websockets",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "Other API Styles: GraphQL and WebSockets",
    "text": "Other API Styles: GraphQL and WebSockets\nREST is the most common approach, but it’s not the only pattern you might encounter. Here are two other API styles, just for your awareness:\n\nGraphQL: GraphQL is an alternative approach where instead of multiple specific endpoints, you have a single endpoint that can handle flexible queries. You send a query describing exactly what data you want, and you get back precisely that data. This can reduce the number of requests needed for complex apps. For example, if a REST API required you to call one endpoint for user info and another for user posts, a GraphQL API could allow you to get both in a single request by specifying that in the query. It’s powerful for complex data fetching, but also a bit more advanced in usage. (In this course we won’t be using GraphQL, but it’s good to know it exists.)\nWebSockets (Real-Time APIs): A WebSocket provides a continuous two-way connection between client and server, allowing data to be sent in real time. This isn’t a request-response model; it’s more like an open channel. WebSockets are useful for applications like live chat, streaming data updates, or multiplayer games – anywhere you want instant, ongoing data flow. For instance, if you were tracking sentiment on a live stream of tweets, a WebSocket connection could stream new analyses continuously. This is more specialized, so we’ll stick to the request/response style in our work, but you might encounter WebSocket APIs in other contexts (e.g., real-time stock price feeds).\n\nFor most data analysis tasks (like fetching data or sending data for analysis), you’ll be using REST APIs, as they cover the majority of use cases and are easier to get started with.\nGreat, I’ll put together a conceptual overview of how API client libraries (especially in Python, with a mention of R) work, including when and why you’d use them, and how they simplify HTTP requests behind the scenes. I’ll be back shortly with the draft section you can add to your markdown."
  },
  {
    "objectID": "week06/assets/api-advanced.html#api-client-libraries",
    "href": "week06/assets/api-advanced.html#api-client-libraries",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "API Client Libraries",
    "text": "API Client Libraries\nWhen working with web APIs, you often have the choice of using API client libraries instead of crafting raw HTTP requests. An API client library (sometimes called an SDK or helper library) is a package provided in a specific programming language that wraps the API’s functionality in convenient functions or classes. In Python especially, many major services offer official or community-maintained libraries to simplify API usage. These libraries act as a language-specific abstraction layer over the API – they break the API’s usual language-agnostic nature to make the developer experience smoother. In practice, this means you can interact with the API using Python objects and methods, rather than manually formatting HTTP requests and parsing responses.\nWhy use a client library? Client libraries simplify and reduce the code you need to write for common API tasks. Instead of manually assembling URLs, query parameters, headers, and JSON payloads for each request, you can call a function or method provided by the library and let it handle those details. For example, a library might provide a method like get_user_tweets(\"Alice\") or openai.ChatCompletion.create(...) – behind the scenes, that method will construct the proper HTTP request (with the correct endpoint and parameters), send it, and parse the result for you. In other words, the library “abstracts away the HTTP requests and offers more convenient interfaces” to work with the API. This abstraction typically covers:\n\nRequest formatting and transport: The library builds the correct HTTP requests (URLs, methods, headers, body) and sends them using an HTTP client, so you don’t have to manually use tools like requests or curl. It also often manages details like authentication headers (API keys, tokens) for you after an initial setup.\nResponse parsing and serialization: Data returned from the API (usually in JSON) is automatically parsed into Python data structures or objects. Similarly, when you provide data to send (like a dictionary or object), the library serializes it to JSON. This spares you from manual JSON formatting.\nError handling and reliability: Good client libraries include error handling logic. They might raise clear exceptions for error responses (rather than you checking HTTP status codes yourself) and handle common issues like rate limiting or retries. This means your code can focus on what you want to do with the API, and the library handles the low-level communication details.\nIdiomatic interface: The library’s functions and classes are designed to feel natural in the given language. For instance, Python libraries will return Python objects (like dictionaries or custom classes) and use Python naming conventions. This makes the API “simple and intuitive to use” in that language, as opposed to treating everything as raw text or HTTP mechanics."
  },
  {
    "objectID": "week06/assets/api-advanced.html#making-an-api-call-a-conceptual-walkthrough",
    "href": "week06/assets/api-advanced.html#making-an-api-call-a-conceptual-walkthrough",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "Making an API Call: A Conceptual Walkthrough",
    "text": "Making an API Call: A Conceptual Walkthrough\nLet’s put all this into a concrete example without diving into code. How would you use an API for a task, say, analyzing sentiment on a large number of texts? Here’s the high-level process:\n\nFind a Suitable API & Read the Documentation: First, you’d identify an API that offers sentiment analysis (or whatever task you need). This could be a cloud service like Azure Cognitive Services, IBM Watson NLU, or OpenAI’s API. You’d read the documentation to find out the endpoint for sentiment analysis, what inputs it expects (maybe it requires a piece of text or a list of texts, and possibly a language or other settings), and what the output looks like.\nObtain Access (API Key): You sign up for the service to get your API key (or other credentials). For example, you might create a free account and receive a key like abcd1234... that identifies you.\nConstruct the Request: Using the information from the docs, you prepare your API request. For example:\n\nDecide on the HTTP method: sentiment analysis might require a POST request because you’re sending data (the text).\nDetermine the endpoint URL: e.g., https://api.some-service.com/v1/sentiment.\nPrepare the data format: the API might require JSON. For instance, you might need to send {\"text\": \"I love this product!\"} in the body of the request. If you have multiple texts, maybe it allows an array of texts.\nInclude your API key as instructed (maybe in a header or as a parameter).\n\nSend the Request (Client side): Now you send the request from your client (which could be a Python script, a command-line tool like curl, or an app like Postman for testing). This is when your program reaches out over the internet to the API’s server with your request details.\nReceive the Response (Server side): The API’s server processes your input. It runs the sentiment analysis on the text you sent. Then it sends back a response. Let’s say the response is a JSON object like: {\"sentiment\": \"positive\", \"confidence\": 0.95}, along with an HTTP status code 200 (meaning success). If something was wrong (e.g., missing the API key or the text was too long), you might get an error response instead of explaining what went wrong.\nIntegrate the Results: Your code receives this response data. Now you can use it in your analysis. For instance, your program can take the \"sentiment\": \"positive\" value and record that this particular review was positive. You might loop through all 10,000 reviews, call the API for each, and collect the results. In the end, you could calculate statistics (like 60% of reviews are positive, 30% neutral, 10% negative, etc.) or visualize the data.\n\nThroughout this process, the heavy work (the actual sentiment computation) is done by the API provider’s servers. Your job is to correctly send requests and process the responses. In practice, you’d likely write a small script to automate steps 3–6 so that you can handle many texts sequentially or in parallel.\nEven without writing code here, hopefully, you can see the pattern: find API -&gt; get access -&gt; request -&gt; response -&gt; use data. Once you learn to do this, you can apply it to countless situations, not just sentiment analysis.\nPlease see our api-use.md file for fundamental information."
  },
  {
    "objectID": "week06/assets/api-advanced.html#more",
    "href": "week06/assets/api-advanced.html#more",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "More",
    "text": "More\nListen to Alice Evans explain all things HTTP on this podcast."
  },
  {
    "objectID": "week06/assets/api-use.html",
    "href": "week06/assets/api-use.html",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "",
    "text": "Previously, we manually examined a sample of just 20 texts and tried using an LLM for sentiment analysis. How long did this take you? Would it still be doable if there were 75 texts? Likely, yes. However, imagine you have 10,000 texts to analyze for sentiment. Analyzing 10,000 texts one by one (or copying them into a tool manually) would be nearly impossible – it would take endless hours and be prone to error. We need a way to automate and scale the process. This is where APIs come in. By leveraging an API, we can send those thousands of texts to a powerful external service that analyzes sentiment and returns results in seconds. A nice recent example of this in economics research is a recent working paper that used as data over 1,400 American life narratives from the 1930s to uncover common themes about what it means to live a meaningful life.\nOur baseline approach is for Python, but R is pretty similar (see below)"
  },
  {
    "objectID": "week06/assets/api-use.html#why-apis",
    "href": "week06/assets/api-use.html#why-apis",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "",
    "text": "Previously, we manually examined a sample of just 20 texts and tried using an LLM for sentiment analysis. How long did this take you? Would it still be doable if there were 75 texts? Likely, yes. However, imagine you have 10,000 texts to analyze for sentiment. Analyzing 10,000 texts one by one (or copying them into a tool manually) would be nearly impossible – it would take endless hours and be prone to error. We need a way to automate and scale the process. This is where APIs come in. By leveraging an API, we can send those thousands of texts to a powerful external service that analyzes sentiment and returns results in seconds. A nice recent example of this in economics research is a recent working paper that used as data over 1,400 American life narratives from the 1930s to uncover common themes about what it means to live a meaningful life.\nOur baseline approach is for Python, but R is pretty similar (see below)"
  },
  {
    "objectID": "week06/assets/api-use.html#what-is-an-api",
    "href": "week06/assets/api-use.html#what-is-an-api",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "What is an API?",
    "text": "What is an API?\nAn API (Application Programming Interface) is like a messenger or middleman that lets two different programs talk to each other and exchange information. Instead of a person directly doing a task, you have one software program asking another program to do something on its behalf. A popular analogy is that an API is similar to a restaurant waiter:\n\nYou (the client) are sitting at a table, ready to order a meal (you have a request for information or a service).\nThe waiter (the API) takes your order and relays it to the kitchen. You don’t go into the kitchen yourself – the waiter is the go-between.\nThe kitchen (the server) is where the work happens. The chef prepares the meal (the data or service you requested).\nThe waiter (API) returns with your meal and serves it to you. You get exactly what you ordered, without having to know how the kitchen prepared it.\n\nIn this analogy, the restaurant’s menu is like the API documentation – it lists what you can ask for and how to ask for it. If you request something not on the menu, the waiter (API) will tell you it’s not available (an error). Similarly, an API provides a set of rules and endpoints that define what requests can be made and what responses you can expect.\nThis means you don’t need to know the complex inner workings of the server or service. You just need to know what to ask for and how to ask for it through the API. The API handles the communication, just as the waiter handles communication between you and the kitchen."
  },
  {
    "objectID": "week06/assets/api-use.html#benefits-of-using-apis-in-data-analysis",
    "href": "week06/assets/api-use.html#benefits-of-using-apis-in-data-analysis",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "Benefits of Using APIs in Data Analysis",
    "text": "Benefits of Using APIs in Data Analysis\nWhy use APIs as a data analyst? Here are some key benefits:\n\nScalability: APIs let you process large volumes of data quickly. You can automate requests in code, so analyzing 10,000 texts or more becomes feasible. Instead of manually working with each piece of data, you let a server handle the heavy lifting.\nAccess to Powerful Tools: Many companies provide APIs for advanced services like sentiment analysis, language translation, image recognition, or data storage. As a data analyst, you can tap into these pre-built models and services without having to develop them from scratch.\nTime and Effort Savings: Using an API, you can perform complex tasks with just a simple request. This saves you the time of writing extensive code or doing repetitive work. For example, rather than writing your own sentiment analysis algorithm, you can send text to an API and get sentiment results immediately.\nIntegration of Data Sources: APIs allow different software and datasets to integrate. You can pull data from different sources (e.g. Twitter’s API for tweets, a weather API for climate data) directly into your analysis pipeline. This marries data from multiple sources seamlessly.\nConsistency and Reliability: When you use a well-established API, you benefit from a service that’s been tested and optimized. The API will handle errors, edge cases, and updates, so you get consistent results. It’s like outsourcing a task to an expert – you trust the API to do its job correctly."
  },
  {
    "objectID": "week06/assets/api-use.html#api-keys-and-authentication",
    "href": "week06/assets/api-use.html#api-keys-and-authentication",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "API Keys and Authentication",
    "text": "API Keys and Authentication\nMost APIs require some form of authentication to ensure that only authorized users or applications can use them. The simplest form is an API key. An API key is like a secret password or ID that you include with your API calls:\n\nYou typically get an API key by creating an account or registering an application with the API provider. For example, to use the Twitter API or OpenAI API, you’d sign up and receive a key (or token).\nThe key itself is usually a long string of characters (letters, numbers, and symbols). It’s unique to you or your application.\nYou include this key with every request. Often it goes in a request header (for instance, you might set a header Authorization: Bearer YOUR_API_KEY), or sometimes as a URL parameter (e.g., ?api_key=YOUR_API_KEY in the query string). The API documentation will tell you exactly how to include the key.\nThe server checks the key. If the key is missing or wrong, the API will usually respond with an authentication error (like a 401 Unauthorized status). If the key is valid, the server will proceed to handle your request.\nSecurity tip: Never share your API keys publicly or commit them to public repositories. They are meant to be kept secret. If someone obtains your key, they could use the API pretending to be you, which might violate usage limits or incur costs on your account.\n\nSome services use more complex authentication (like OAuth tokens which have limited scope or expiration), but an API key is the fundamental concept to understand first. It’s your access credential for using the API."
  },
  {
    "objectID": "week06/assets/api-use.html#get-an-api",
    "href": "week06/assets/api-use.html#get-an-api",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "Get an API",
    "text": "Get an API\nNext, you can go and get an API key for an AI service. As a start a few dollars will be enough. Follow instructions"
  },
  {
    "objectID": "week06/assets/api-use.html#walkthrough-examples",
    "href": "week06/assets/api-use.html#walkthrough-examples",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "Walkthrough examples",
    "text": "Walkthrough examples\n\nGetting GDP data from World Bank and FRED\nMore advanced Football data (python, R)"
  },
  {
    "objectID": "week06/assets/api-use.html#whats-different-in-r",
    "href": "week06/assets/api-use.html#whats-different-in-r",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "What’s different in R",
    "text": "What’s different in R\nClient libraries in other languages: While our focus is on Python, other programming languages provide similar conveniences.\nIn R, packages like httr (for making HTTP requests) and jsonlite (for parsing JSON) are commonly used to work with web APIs. Many APIs also have R packages or wrappers that function like client libraries, letting you call the API in one or two lines of R code.\nThe core idea is the same: a client library abstracts the RESTful requests into native language functions. Regardless of language, using a client library means you can integrate an API into your data analysis or application with less hassle, letting you focus on interpreting results rather than the mechanics of HTTP."
  },
  {
    "objectID": "week06/assets/api-use.html#scaling-up-with-apis-from-20-to-10000-and-beyond",
    "href": "week06/assets/api-use.html#scaling-up-with-apis-from-20-to-10000-and-beyond",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "Scaling Up with APIs: From 20 to 10,000 and Beyond",
    "text": "Scaling Up with APIs: From 20 to 10,000 and Beyond\nThe introduction of APIs into your workflow transforms what you can accomplish:\n\nTasks that were infeasible by hand become trivial to automate. You could get results in minutes or hours rather than weeks.\nYou can harness powerful algorithms provided by industry leaders. For example, instead of developing your machine learning model, you can use Google’s vision API to tag images or OpenAI’s language API to summarize text. This means you can tackle complex problems without needing to be an expert in those specific subfields.\nYou can work with real-time and large-scale data. Want to analyze football statistics or financial market data? There are APIs to fetch those streams of information. With APIs, you are not limited to data you can collect manually; you can pull in data from all over the world programmatically.\n\nAPIs are a bridge to practically unlimited data and capabilities. They let your programs communicate with other services to get things done efficiently. As we continue this course, you’ll get hands-on experience using APIs – turning the concepts you learned here into actual data analysis tasks. Embrace this new tool in your skillset. Whenever you find yourself needing to scale up or access a specialized service, think: Is there an API for that? Chances are, the answer will be yes, and now you’ll know how to use it!\nMore advanced and supplementary information."
  },
  {
    "objectID": "week08/index.html",
    "href": "week08/index.html",
    "title": "Week 8: SQL with AI",
    "section": "",
    "text": "Week 8: SQL with AI\nContent coming soon",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 8: SQL with AI"
    ]
  },
  {
    "objectID": "week00/assets/week00-ask-ai.html#nameing",
    "href": "week00/assets/week00-ask-ai.html#nameing",
    "title": "The name of week00",
    "section": "Nameing",
    "text": "Nameing\n\n\n🔧 Is name week00 OK? ::: ::: {.card-body}\nGábor:\n“I’m struggling with this decision. Shall I add it as week00 or rename all weeks (01–&gt;02 etc) and have this as 01. what do you think?”**\nClaude:\nI’d recommend keeping it as Week 00 for several practical and pedagogical reasons:\n\n✅ Arguments for Week 00:\nPractical Benefits: - No disruption: Keeps all existing numbering, assignments, internal links intact - Less work: Avoids the error-prone task of updating references throughout - Flexibility: Students with strong AI coding experience could potentially skip/skim\nPedagogical Logic: - Common convention: Week 00 is standard in CS courses for “setup” content - Clear signaling: “This is foundational preparation, not optional” - Entry point flexibility: Students can assess their own readiness\n\n\n🎯 Make Week 00 Feel Essential:\nIn course description/syllabus: - “Week 00 (REQUIRED): AI-Assisted Coding Fundamentals” - “Essential preparation for all subsequent weeks”\nIn Week 01 intro: - “This course assumes familiarity with AI coding tools covered in Week 00” - Brief recap/check that students completed the prep\nCross-references: - Link back to Week 00 techniques throughout other weeks - “As we covered in Week 00’s prompting strategies…”\n\n\n⚖️ Alternative Consideration:\nThe only reason I’d consider renumbering is if you find that students consistently skip Week 00 and then struggle. But you can address this with: - Clear communication about importance - Quick competency check in Week 01 - Making Week 00 content prerequisite for assignments\nRecommendation: Start with Week 00, see how it works in practice, and renumber later if needed. Much easier to go from 00→01 than to undo a full renumbering.\nWhat’s your gut feeling on student behavior - would they skip something labeled “Week 00”?"
  },
  {
    "objectID": "week00/assets/week00-ask-ai.html#arguments-for-week-00",
    "href": "week00/assets/week00-ask-ai.html#arguments-for-week-00",
    "title": "The name of week00",
    "section": "✅ Arguments for Week 00:",
    "text": "✅ Arguments for Week 00:\nPractical Benefits: - No disruption: Keeps all existing numbering, assignments, internal links intact - Less work: Avoids the error-prone task of updating references throughout - Flexibility: Students with strong AI coding experience could potentially skip/skim\nPedagogical Logic: - Common convention: Week 00 is standard in CS courses for “setup” content - Clear signaling: “This is foundational preparation, not optional” - Entry point flexibility: Students can assess their own readiness"
  },
  {
    "objectID": "week00/assets/week00-ask-ai.html#make-week-00-feel-essential",
    "href": "week00/assets/week00-ask-ai.html#make-week-00-feel-essential",
    "title": "The name of week00",
    "section": "🎯 Make Week 00 Feel Essential:",
    "text": "🎯 Make Week 00 Feel Essential:\nIn course description/syllabus: - “Week 00 (REQUIRED): AI-Assisted Coding Fundamentals” - “Essential preparation for all subsequent weeks”\nIn Week 01 intro: - “This course assumes familiarity with AI coding tools covered in Week 00” - Brief recap/check that students completed the prep\nCross-references: - Link back to Week 00 techniques throughout other weeks - “As we covered in Week 00’s prompting strategies…”"
  },
  {
    "objectID": "week00/assets/week00-ask-ai.html#alternative-consideration",
    "href": "week00/assets/week00-ask-ai.html#alternative-consideration",
    "title": "The name of week00",
    "section": "⚖️ Alternative Consideration:",
    "text": "⚖️ Alternative Consideration:\nThe only reason I’d consider renumbering is if you find that students consistently skip Week 00 and then struggle. But you can address this with: - Clear communication about importance - Quick competency check in Week 01 - Making Week 00 content prerequisite for assignments\nRecommendation: Start with Week 00, see how it works in practice, and renumber later if needed. Much easier to go from 00→01 than to undo a full renumbering.\nWhat’s your gut feeling on student behavior - would they skip something labeled “Week 00”?"
  },
  {
    "objectID": "week00/assets/variable-dictionary-claude4.html",
    "href": "week00/assets/variable-dictionary-claude4.html",
    "title": "CPS MORG 2014 Variable Dictionary by Claude Sonnet 4.0",
    "section": "",
    "text": "This is my data and the codebook. Create a variable dictionary. Use the pdf i shared earlier. Output as markdown. For each variable: varname, labels, type, coverage (% missing), mean and mode. Round up numbers. Look at cps and provide short labels. Get me an .md I can download."
  },
  {
    "objectID": "week00/assets/variable-dictionary-claude4.html#variable-definitions",
    "href": "week00/assets/variable-dictionary-claude4.html#variable-definitions",
    "title": "CPS MORG 2014 Variable Dictionary by Claude Sonnet 4.0",
    "section": "Variable Definitions",
    "text": "Variable Definitions\n\nHousehold Identifiers\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\n…1\nRow Index\nInteger\nSequential row identifier\nContinuous\n\n\nhhid\nHousehold ID\nInteger\nUnique household identifier\n12-15 digits\n\n\nintmonth\nInterview Month\nString\nCalendar month of interview\n01-12 (Jan-Dec)\n\n\nstfips\nState FIPS\nString\nState FIPS code\n11-95\n\n\n\n\n\nSample Design\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nweight\nFinal Weight\nFloat\nPopulation weight (×100)\n0-20549\n\n\n\n\n\nLabor Market Variables\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nearnwke\nWeekly Earnings\nFloat\nUsual weekly earnings ($)\n0-2884\n\n\nuhours\nUsual Hours\nInteger\nUsual hours worked per week\n0-99\n\n\nlfsr94\nLabor Force Status\nString\nEmployment status recode\n1=Working, 2=With job not at work, 3=Looking, 4=Layoff, 5=Not in labor force\n\n\n\n\n\nDemographics\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nage\nAge\nInteger\nAge in years\n16-99\n\n\nsex\nSex\nInteger\nGender\n1=Male, 2=Female\n\n\nrace\nRace\nInteger\nRace/ethnicity\n1=White, 2=Black, 3=American Indian, 4=Asian/Pacific Islander, 5=Other\n\n\nethnic\nHispanic Origin\nFloat\nHispanic ethnicity\n1=Mexican American, 2=Chicano, 3=Mexicano, 4=Puerto Rican, 5=Cuban, 6=Central/South American, 7=Other Spanish, 8=All other\n\n\nmarital\nMarital Status\nInteger\nMarital status\n1=Married civilian spouse present, 2=Married AF spouse present, 3=Married spouse absent, 4=Widowed, 5=Divorced, 6=Separated, 7=Never married\n\n\n\n\n\nEducation\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\ngrade92\nEducation Level\nInteger\nHighest grade completed\n31=Less than 1st grade, 32=1st-4th grade, 33=5th-6th grade, 34=7th-8th grade, 35=9th grade, 36=10th grade, 37=11th grade, 38=12th grade no diploma, 39=HS graduate, 40=Some college, 41=Associate occupational, 42=Associate academic, 43=Bachelor’s degree, 44=Master’s degree, 45=Professional degree, 46=Doctorate\n\n\n\n\n\nFamily Structure\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nownchild\nOwn Children\nInteger\nNumber of own children &lt;18\n0-8\n\n\nchldpres\nChild Presence\nInteger\nPresence of own children &lt;18\nComplex coding scheme (see codebook)\n\n\n\n\n\nCitizenship\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nprcitshp\nCitizenship\nString\nCitizenship status\n1=Native born US, 2=Native born PR/US territory, 3=Native born abroad of US parents, 4=Foreign born naturalized, 5=Foreign born not citizen\n\n\n\n\n\nGeography\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nstate\nState Code\nInteger\n1960 Census state code\n11-95 (see codebook for state mapping)\n\n\n\n\n\nIndustry & Occupation\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nind02\nIndustry 2002\nString\n2002 NAICS industry classification\n170-9890\n\n\nocc2012\nOccupation 2012\nInteger\n2010 SOC occupation code\n10-9840\n\n\nclass\nClass of Worker\nString\nEmployment class\n1=Federal government, 2=State government, 3=Local government, 4=Private for profit, 5=Private non-profit, 6=Self-employed incorporated, 7=Self-employed not incorporated, 8=Without pay\n\n\n\n\n\nUnion Status\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nunionmme\nUnion Member\nString\nUnion membership status\n1=Yes, 2=No\n\n\nunioncov\nUnion Coverage\nString\nCovered by union contract\n1=Yes, 2=No"
  },
  {
    "objectID": "week00/assets/variable-dictionary-claude4.html#data-quality-notes",
    "href": "week00/assets/variable-dictionary-claude4.html#data-quality-notes",
    "title": "CPS MORG 2014 Variable Dictionary by Claude Sonnet 4.0",
    "section": "Data Quality Notes",
    "text": "Data Quality Notes\nMissing Data: Unable to calculate precise coverage statistics without data access. Refer to original CPS documentation for universe definitions.\nWeights: Use weight variable for population estimates. The earnings weight earnwt is preferred for earnings-related analyses.\nTime Period: This extract contains data from 2014 MORG files (outgoing rotation groups only).\nTop Coding: Earnings variables may be top-coded. Check for density spikes at maximum values."
  },
  {
    "objectID": "week00/assets/variable-dictionary-claude4.html#key-variable-relationships",
    "href": "week00/assets/variable-dictionary-claude4.html#key-variable-relationships",
    "title": "CPS MORG 2014 Variable Dictionary by Claude Sonnet 4.0",
    "section": "Key Variable Relationships",
    "text": "Key Variable Relationships\n\nLabor Force Status (lfsr94): Controls universe for many employment variables\nEducation (grade92): Uses post-1992 credential-based classification\nEarnings (earnwke): Available only for wage/salary workers; excludes self-employed\nHours (uhours): Usual hours at main job\nDemographics: Standard CPS demographic variables with consistent coding"
  },
  {
    "objectID": "week00/assets/variable-dictionary-claude4.html#usage-guidelines",
    "href": "week00/assets/variable-dictionary-claude4.html#usage-guidelines",
    "title": "CPS MORG 2014 Variable Dictionary by Claude Sonnet 4.0",
    "section": "Usage Guidelines",
    "text": "Usage Guidelines\n\nWeights: Always use appropriate weights for population estimates\nMissing Values: Check universe definitions in codebook\nTime Series: Be aware of classification changes across years\nEarnings: Exclude self-employed workers for wage analyses\nIndustry/Occupation: Codes based on 2002 NAICS and 2010 SOC systems\n\n\nSource: NBER CPS Labor Extracts, based on Bureau of Labor Statistics Current Population Survey\nDocumentation: See full codebook for detailed variable definitions and historical changes"
  },
  {
    "objectID": "week04/index.html",
    "href": "week04/index.html",
    "title": "Week 04: Agentic AI with CLIs like Claude Code",
    "section": "",
    "text": "Week 04: Agentic AI with Claude Code\n\n\nFrom chat to terminal - AI that works directly with your files",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Agentic AI with CLIs"
    ]
  },
  {
    "objectID": "week04/index.html#about-the-class",
    "href": "week04/index.html#about-the-class",
    "title": "Week 04: Agentic AI with CLIs like Claude Code",
    "section": "About the class",
    "text": "About the class\nSo far we’ve used AI through chat interfaces - copying code back and forth, describing our data, pasting error messages. This week we introduce agentic AI: AI that can see your files, run your code, and iterate on results directly. It is a command-line interface (CLI) – it runs in your terminal. You type commands, the AI responds with code, runs it, and shows results - all in one place. It can assemble knowledge with great deal of automation.\nInstead of describing your data to an AI, you can say “look at these CSV files and tell me what’s here.” Instead of copying error messages, Claude Code sees them and fixes the code itself.\n\nTool Options\nWe use Claude Code in this class, but the concepts apply to similar tools:\n\n\n\n\n\n\n\n\nTool\nProvider\nNotes\n\n\n\n\nClaude Code\nAnthropic\nWhat we use. Requires Claude Pro or API.\n\n\nGemini CLI\nGoogle\nSimilar workflow, uses Gemini models.\n\n\nCodex CLI\nOpenAI\nOpenAI’s command-line tool.\n\n\nCursor / Windsurf\nThird-party\nIDE-based, similar agentic features.\n\n\n\nEverything you learn today transfers to these alternatives. The prompts, the workflow, the verification habits - all the same. So, we’ll use Claude Code as we go but just replace it with other tools. Also, tools are keep evolving, so focus on process more than specifics of a tool.\n\n\nLearning Objectives\nBy the end of this session, students will:\n\nUnderstand the difference between chat-based AI and agentic AI tools\nInstall and configure Claude Code on their machine\nUse Claude Code to explore, understand, and analyze a multi-file dataset\nGenerate new data programmatically using Claude Code\nExperience the workflow difference: iteration speed, context awareness, debugging\n\n\n\nBefore class\n\n\nSetup Required - Do This Before Class\n\n\n1. Installation and setup\nFollow the installation guide: Installing AI CLI Tools\nWe use Claude Code, but Gemini CLI or Codex CLI also work. The guide covers all options for Windows, Mac, and Linux.\nClaude Code lives in the Terminal. New to the terminal? Start with Terminal Basics\nClaude Code runs Python code, so Python must be installed:\n\nDownload from python.org/downloads\nCheck “Add Python to PATH” during installation\nInstall packages: python -m pip install pandas numpy matplotlib\n\n2. Download the data\nGet data-modified.zip from the Austrian Hotels case study and unzip it.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Agentic AI with CLIs"
    ]
  },
  {
    "objectID": "week04/index.html#class-plan",
    "href": "week04/index.html#class-plan",
    "title": "Week 04: Agentic AI with CLIs like Claude Code",
    "section": "Class Plan",
    "text": "Class Plan\n\n\nPart 1: Why Agentic AI? (30 min)\n\n\n\nKey Innovations: Why Use CLI Tools?\nDirect file system access\n\nCLI tools operate directly in your project directory, reading data files, scripts, and outputs without manual uploads\nClaude Code provides seamless local access to your R/Stata scripts and data\nGemini CLI offers --include-directories flags to explicitly include multiple directories—useful when your data lives in /data/raw/, /data/clean/, and /exhibits/ across different locations\n\nAutonomous command execution\n\nBoth tools can run your R/Stata/Python scripts, install packages, and execute bash commands\nThey can test code, catch errors, and iterate automatically—running Rscript analysis.R, checking outputs, and debugging without you copying error messages back\nGemini’s “Yolo Mode” (-y) chains commands without permission prompts; Claude Code maintains interactive safety controls while offering similar autonomous iteration\n\nWorkflow automation for reproducible research\n\nClaude Code: Create custom slash commands (e.g., /clean-data, /make-exhibits) in .claude/commands/ that encode entire analysis pipelines. These travel with your repo, ensuring reproducibility\nGemini: Uses Model Context Protocol (MCP) with Extension Gallery plug-ins—particularly powerful for connecting to BigQuery, Google Drive, or APIs where your raw data might live\n\nMulti-file project understanding\n\nBoth understand relationships across your entire project: how 01_data_clean.R feeds into 02_analysis.R and ultimately produces exhibit_table1.tex\nGemini excels: 1M+ token context window lets you load entire large datasets (e.g., 500-page PDFs, massive CSV files) in one session\nClaude Code excels: Coordinated multi-file edits with deep understanding of script dependencies—can update data cleaning, analysis, and visualization scripts in one coherent workflow\n\nProject-specific instructions\n\nClaude Code reads CLAUDE.md; Gemini reads GEMINI.md files at global, project, and folder levels\nBoth enforce your preferences automatically: tidyverse syntax, viridis colors, clean function-based code, specific exhibit formatting\nParticularly useful for maintaining consistency across case studies or ensuring all team members follow the same data analysis standards\n\nVersion control integration\n\nClaude Code provides robust git workflow management: can create commits, branches, review history to understand why you made analytical decisions\nGemini’s Checkpointing: Saves entire “thinking sessions”—valuable when exploring alternative model specifications or data transformations, letting you roll back experiments that didn’t work\n\nReal-time information access\n\nClaude Code integrates web search for updated documentation\nGemini’s native Google Search Grounding: Ensures package versions and API documentation are current—critical when working with rapidly-evolving R/Python packages.\n\n\n\nWhat Stays the Same\n\nUnderlying intelligence: Both use state-of-the-art models (Claude Sonnet/Opus 4.5 vs. Gemini 3.0 Pro/Flash) with comparable reasoning for statistical analysis and code generation\nNatural language interface: Describe tasks in plain language: “Clean this dataset and create a summary statistics table” or “Debug why my regression results look wrong”\nIterative refinement: Both support conversational debugging—ask the CLI to fix errors, try different estimation methods, or refine exhibit formatting through multiple rounds\nYour preferences respected: Both learn and apply your coding standards (tidyverse, viridis, clean functions) once configured through their respective .md instruction files\nMultimodal input for data work: Both handle various data formats: CSV, Excel, PDFs, images of tables/charts. Useful for extracting data from PDFs, reproducing charts from papers, or generating code from hand-drawn sketches\n\n\n\nBottom Line for Data Analysis\nCLI tools shine when you have:\n\nComplex multi-file projects with data pipelines (raw → clean → analysis → exhibits)\nNeed for reproducible workflows that others can run\nLarge datasets or documents requiring substantial context\nIterative analysis where the AI should test and debug autonomously\n\nThe chatbox remains better for:\n\nQuick one-off questions or code snippets\nExploratory conversations about methodology\nSituations where you want tight control over each step\n\n\n\nDiscussion: How Would You Do This?\nTask: “What’s the average hotel occupancy by city in this dataset?”\nYou have a folder with several CSV files. You want to answer this question using AI assistance.\nThink-pair-share (10 min):\n\nWhat steps would you take using ChatGPT or Claude.ai (chat interface)?\nWhat information would you need to give the AI?\nWhat could go wrong? How would you handle errors?\n\nDiscuss with a neighbor, then share with the class.\n\nThe friction: Every step requires manual copy-paste. Context gets lost. Errors require round-trips.\n\n\nDemo: The Agentic Approach\nNow watch what happens with Claude Code:\ncd /path/to/austrian-hotels\nclaude\n&gt; Look at the CSV files here and calculate average occupancy by city\nClaude Code reads the files, writes the code, runs it, sees any errors, fixes them, and shows results. One prompt, no copy-paste.\n\n\n\n\n\n\n\nPart 2: Hands-on - Explore the Dataset (25 min)\n\n\n\nGetting Started\nOpen your terminal, navigate to your data folder:\ncd path/to/austrian-hotels-data\nclaude\n\n\nExercise 1: Understand the Data (Individual, 10 min)\nTry these prompts:\nWhat files are in this folder? Give me a quick overview.\nRead the hotels and cities files. How are they related? What's the join key?\nShow me 5 sample rows from each CSV file.\nDiscuss: How does this compare to opening each file manually or describing it to ChatGPT?\n\n\n\nExercise 2: Basic Analysis (Individual, 15 min)\nAsk Claude Code to:\n\nJoin hotels and cities - “Join the hotels and cities data. How many hotels are in each province?”\n\nNew to joins? See Joining Tables Guide\n\nAggregate occupancy - “What’s the average occupancy rate by city? Show me a table sorted highest to lowest.”\nFind patterns - “Which 5-star hotels have the lowest average daily rate? Something seems off - investigate.”\n\nTips:\n\nIf something looks wrong, ask “Why did that happen?” or “Check the row counts”\nAsk Claude Code to show intermediate steps: “Show me the data after the join, before aggregating”\nIf you get an error, just wait - Claude Code will often fix it automatically\n\n\n\n\n\n\n\nPart 3: Generate New Data (30 min)\n\n\n\nThe Power Move: AI Creates Data\nOne of the most useful capabilities of Claude Code is generating realistic simulated data. This is exactly how the Austrian Hotels dataset was created - by an earlier version of Claude!\n\n\nDemo: Creating Booking Channel Data\nWatch the instructor create a new data table:\nI want to create a new CSV file called hotel_bookings.csv that shows\nwhat percentage of each hotel's bookings come from different channels\n(Direct, Booking.com, Expedia, HRS, Travel Agent).\n\nRequirements:\n- Each hotel should have rows for each channel\n- Percentages should sum to 100% for each hotel\n- 5-star hotels should have more direct bookings (35-45%)\n- 3-star hotels should rely more on OTAs (Booking.com 40%+)\n- Include a commission rate column (Direct=0%, OTAs=10-18%)\n\nWrite Python code to generate this, using the hotels_modified.csv as input.\nClaude Code will: 1. Read the existing hotels data 2. Write a Python script with realistic logic 3. Run it and generate the CSV 4. Show you a summary\n\n\nExercise 3: Create Your Own Data Table (Groups of 2-3, 20 min)\nYour turn! Brainstorm a new data table that could join to the hotels data. Ideas:\n\n\n\n\n\n\n\n\nTable Idea\nJoin Key\nInteresting Analysis\n\n\n\n\nWeather by city/month\ncity + month + year\nDoes weather affect occupancy?\n\n\nStaff data\nhotel_id\nDo more staff = better reviews?\n\n\nNearby attractions\ncity\nWhich attractions drive tourism?\n\n\nRenovation history\nhotel_id\nDid renovations boost ratings?\n\n\nCompetitor Airbnb prices\ncity + month\nHow do hotel prices compare?\n\n\n\nSteps:\n\nPick a table idea with your group\nWrite a detailed prompt describing what you want (columns, realistic patterns, join keys)\nAsk Claude Code to generate it\nVerify: Does it look realistic? Can you join it to hotels?\nDo one simple analysis with the joined data\n\n\n\n\n\n\n\nPart 4: Debugging and Iteration (10 min)\n\n\n\nWhen Things Go Wrong\nClaude Code isn’t perfect. Common issues:\n\nWrong join type - Ask: “How many rows before and after the join? Did we lose data?”\nMissing values - Ask: “Are there NaN values? Where did they come from?”\nUnexpected results - Ask: “Walk me through the calculation step by step”\nCode errors - Often Claude Code fixes these automatically. If not, just describe what went wrong.\nHallucinations - AI may generate plausible but incorrect code or stats. Always verify.\n\n\n\nTrust but Verify\nAlways check:\n\nRow counts after joins\nSummary statistics (do means make sense?)\nA few random rows (do values look realistic?)\n\nGood habit: Ask Claude Code to explain what it did:\nExplain the code you just wrote. What assumptions did you make?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Agentic AI with CLIs"
    ]
  },
  {
    "objectID": "week04/index.html#what-stays-the-same",
    "href": "week04/index.html#what-stays-the-same",
    "title": "Week 04: Agentic AI with CLIs like Claude Code",
    "section": "What Stays the Same",
    "text": "What Stays the Same\n\nUnderlying intelligence: Both use state-of-the-art models (Claude Sonnet/Opus 4.5 vs. Gemini 3.0 Pro/Flash) with comparable reasoning for statistical analysis and code generation\nNatural language interface: Describe tasks in plain language: “Clean this dataset and create a summary statistics table” or “Debug why my regression results look wrong”\nIterative refinement: Both support conversational debugging—ask the CLI to fix errors, try different estimation methods, or refine exhibit formatting through multiple rounds\nYour preferences respected: Both learn and apply your coding standards (tidyverse, viridis, clean functions) once configured through their respective .md instruction files\nMultimodal input for data work: Both handle various data formats: CSV, Excel, PDFs, images of tables/charts. Useful for extracting data from PDFs, reproducing charts from papers, or generating code from hand-drawn sketches\n\n\n\nBottom Line for Data Analysis\nCLI tools shine when you have:\n\nComplex multi-file projects with data pipelines (raw → clean → analysis → exhibits)\nNeed for reproducible workflows that others can run\nLarge datasets or documents requiring substantial context\nIterative analysis where the AI should test and debug autonomously\n\nThe chatbox remains better for:\n\nQuick one-off questions or code snippets\nExploratory conversations about methodology\nSituations where you want tight control over each step\n\n\n\nDiscussion: How Would You Do This?\nTask: “What’s the average hotel occupancy by city in this dataset?”\nYou have a folder with several CSV files. You want to answer this question using AI assistance.\nThink-pair-share (10 min):\n\nWhat steps would you take using ChatGPT or Claude.ai (chat interface)?\nWhat information would you need to give the AI?\nWhat could go wrong? How would you handle errors?\n\nDiscuss with a neighbor, then share with the class.\n\nThe friction: Every step requires manual copy-paste. Context gets lost. Errors require round-trips.\n\n\nDemo: The Agentic Approach\nNow watch what happens with Claude Code:\ncd /path/to/austrian-hotels\nclaude\n&gt; Look at the CSV files here and calculate average occupancy by city\nClaude Code reads the files, writes the code, runs it, sees any errors, fixes them, and shows results. One prompt, no copy-paste.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Agentic AI with CLIs"
    ]
  },
  {
    "objectID": "week04/index.html#discussion-questions",
    "href": "week04/index.html#discussion-questions",
    "title": "Week 04: Agentic AI with CLIs like Claude Code",
    "section": "Discussion Questions",
    "text": "Discussion Questions\nEnd of Class Reflection:\n\nWorkflow change: How did using Claude Code feel different from chat-based AI? What was faster? What was harder?\nTrust calibration: When did you trust Claude Code’s output? When did you double-check? How do you decide?\nUse cases: For what tasks would you now prefer Claude Code over ChatGPT/Claude.ai? When would you still use chat?\nData generation: What did you learn from creating synthetic data? How could this help in your own projects?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Agentic AI with CLIs"
    ]
  },
  {
    "objectID": "week04/index.html#assignment",
    "href": "week04/index.html#assignment",
    "title": "Week 04: Agentic AI with CLIs like Claude Code",
    "section": "Assignment",
    "text": "Assignment\n\n\n\n\n\n\nNoteAssignment 4: Extend the Austrian Hotels Dataset\n\n\n\nDue: Sunday 23:55 before Week 5\nFull Assignment Details\nSummary: Using Claude Code, create a new data table that joins to the Austrian Hotels dataset, then perform an analysis that answers an interesting question.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Agentic AI with CLIs"
    ]
  },
  {
    "objectID": "week04/index.html#resources",
    "href": "week04/index.html#resources",
    "title": "Week 04: Agentic AI with CLIs like Claude Code",
    "section": "Resources",
    "text": "Resources\nClaude Code & Terminal:\n\nTerminal Basics - Essential commands\nSetting Up Claude Code - Installation guide\nClaude Code Documentation - Official docs\n\nCourse Reference:\n\nTechnical Terms Glossary - Key AI/data concepts\nJoining Tables Guide - Join types explained\n\nAustrian Hotels Dataset:\n\nDataset Overview - All files and documentation\nData Schema - Table descriptions and relationships\n\nPython Basics (if needed):\n\nPython for Data Analysis - Free online book\npandas documentation: pandas.pydata.org",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Agentic AI with CLIs"
    ]
  },
  {
    "objectID": "week04/index.html#some-personal-comments",
    "href": "week04/index.html#some-personal-comments",
    "title": "Week 04: Agentic AI with CLIs like Claude Code",
    "section": "Some personal comments",
    "text": "Some personal comments\n\nThis is where AI gets genuinely useful for data work. Chat interfaces are great for learning and quick questions, but agentic AI changes how you actually do the work.\nThe booking channel data we use in class? It was generated by Claude Code while preparing this course. Meta, isn’t it?\nDon’t worry if setup takes time. Getting your environment right is a one-time cost that pays off quickly.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Agentic AI with CLIs"
    ]
  },
  {
    "objectID": "week09/index.html",
    "href": "week09/index.html",
    "title": "Week09 AI as research companion: designing Z",
    "section": "",
    "text": "Week 09: AI as research companion: designing “Z”\n\n\nGetting closer to causality by deciding on control variables\nNow we’re going to try something different. This time, we’ll ask the AI to act like a researcher and help us find control variables for estimating a simple relationship.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 9: AI Research: Controls"
    ]
  },
  {
    "objectID": "week09/index.html#recap",
    "href": "week09/index.html#recap",
    "title": "Week09 AI as research companion: designing Z",
    "section": "Recap",
    "text": "Recap\nDiscuss: confounders, bad controls: what are they? how to treat them?\nWe will use the World Management Survey data and explore this question: &gt; Do firms with better management export more of their production?\nMORE to be added here",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 9: AI Research: Controls"
    ]
  },
  {
    "objectID": "week09/index.html#the-basic-idea",
    "href": "week09/index.html#the-basic-idea",
    "title": "Week09 AI as research companion: designing Z",
    "section": "The basic idea",
    "text": "The basic idea\nBut here’s the twist: we’ll run two sessions (or use two different models). One will give us a list of controls, and the other will act as a kind of “adversarial LLM” that pushes back and questions those choices. Think of it like a debate between two AIs: one helpful, one suspicious.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 9: AI Research: Controls"
    ]
  },
  {
    "objectID": "week09/index.html#prompt-construction",
    "href": "week09/index.html#prompt-construction",
    "title": "Week09 AI as research companion: designing Z",
    "section": "Prompt construction",
    "text": "Prompt construction\nAgain, let’s try to run the following prompts: the first and the last in the same session, and the middle in a separate session. But first, let’s look at the codebook together. As you run the prompts, think about:\n- How do the answers differ from the previous prompting approach? - What do you think of the proposed variables? Would you include them all? Are there any left out.\n\n🤖 Prompt 1. You are a researcher who wants to find control variables to estimate the association between percent of production exported and management quality. Production exported is your outcome variable and management quality is your variable of interest. Attached is a codebook of variables that we have access to. Choose variables that could be included in a multivariate OLS regression.\nReturn a list of variables and explanation, including the expected direction of associations. At the end of your response, include a plain list of variable names in a copyable format (no bullets, no numbers, just one variable per line).\n\n\n😈 Prompt 2. Adversarial LLM in a separate session Below are a list of variables that I want to include in a regression to estimate the relationship between percent of production exported and management quality. Argue for each variable why I should not include it in the regression. [list of variables by Original LLM]\n\n\n🤖 Prompt 3. Feed Adversarial LLM’s response back to Original LLM Below are counter-arguments for each of your previous answers. Based on these arguments, revise your selection and provide a list: [the arguments by Adversary LLM]",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 9: AI Research: Controls"
    ]
  },
  {
    "objectID": "navbar/assignments.html",
    "href": "navbar/assignments.html",
    "title": "Course Assignments",
    "section": "",
    "text": "Assignments are available for all classes\nImportant to note for assignments:  * Use AI but do not submit something that was created by AI. AI is your assistant. * One of the goals of the course is to practice this."
  },
  {
    "objectID": "navbar/assignments.html#assignment-1-reproduce-an-ft-graph",
    "href": "navbar/assignments.html#assignment-1-reproduce-an-ft-graph",
    "title": "Course Assignments",
    "section": "Assignment 1: Reproduce an FT graph",
    "text": "Assignment 1: Reproduce an FT graph\nLook at this graph from Financial Times. Your task is to recreate this figure using real-world data.\nAssignment Details"
  },
  {
    "objectID": "navbar/assignments.html#assignment-2-creating-a-readme-doc",
    "href": "navbar/assignments.html#assignment-2-creating-a-readme-doc",
    "title": "Course Assignments",
    "section": "Assignment 2: Creating a readme doc",
    "text": "Assignment 2: Creating a readme doc\nIn this assignment, you will create a readme document by asking for AI assistance.\nAssignment Details"
  },
  {
    "objectID": "navbar/assignments.html#assignment-3-create-a-report",
    "href": "navbar/assignments.html#assignment-3-create-a-report",
    "title": "Course Assignments",
    "section": "Assignment 3: Create a report",
    "text": "Assignment 3: Create a report\nContinue with the research question you had chosen in class. Use AI iteratively to help you create a report.\nAssignment Details"
  },
  {
    "objectID": "navbar/assignments.html#assignment-4-extend-the-austrian-hotels-dataset",
    "href": "navbar/assignments.html#assignment-4-extend-the-austrian-hotels-dataset",
    "title": "Course Assignments",
    "section": "Assignment 4: Extend the Austrian Hotels Dataset",
    "text": "Assignment 4: Extend the Austrian Hotels Dataset\nUsing Claude Code, create a new data table that joins to the Austrian Hotels dataset, then perform an analysis that answers an interesting question.\nAssignment Details"
  },
  {
    "objectID": "navbar/assignments.html#assignment-5-text-sentiment-analysis",
    "href": "navbar/assignments.html#assignment-5-text-sentiment-analysis",
    "title": "Course Assignments",
    "section": "Assignment 5: Text sentiment analysis",
    "text": "Assignment 5: Text sentiment analysis\nStudent-Specific Text Sentiment Analysis – Compare manual ratings with AI-generated ratings.\nAssignment Details"
  },
  {
    "objectID": "navbar/assignments.html#assignment-6-text-analysis-guessing-gender-in-text",
    "href": "navbar/assignments.html#assignment-6-text-analysis-guessing-gender-in-text",
    "title": "Course Assignments",
    "section": "Assignment 6: Text analysis: guessing gender in text",
    "text": "Assignment 6: Text analysis: guessing gender in text\nCreate pipeline for guessing the speaker’s gender.\nAssignment Details"
  },
  {
    "objectID": "navbar/assignments.html#assignment-7-tba",
    "href": "navbar/assignments.html#assignment-7-tba",
    "title": "Course Assignments",
    "section": "Assignment 7: TBA",
    "text": "Assignment 7: TBA\nTBA"
  },
  {
    "objectID": "navbar/assignments.html#assignment-8-design-an-iv",
    "href": "navbar/assignments.html#assignment-8-design-an-iv",
    "title": "Course Assignments",
    "section": "Assignment 8: Design an IV",
    "text": "Assignment 8: Design an IV\nThink about an identification problem and interate to find a good IV.\nAssignment Details"
  },
  {
    "objectID": "case-studies/earnings/code/index.html",
    "href": "case-studies/earnings/code/index.html",
    "title": "US Earnings - Code",
    "section": "",
    "text": "← Back to Earnings Case Study"
  },
  {
    "objectID": "case-studies/earnings/code/index.html#code-files",
    "href": "case-studies/earnings/code/index.html#code-files",
    "title": "US Earnings - Code",
    "section": "Code Files",
    "text": "Code Files\n\n\n\nFile\nDescription\n\n\n\n\nearnings.R\nR script for earnings analysis"
  },
  {
    "objectID": "case-studies/earnings/index.html",
    "href": "case-studies/earnings/index.html",
    "title": "US Earnings Data (CPS MORG)",
    "section": "",
    "text": "Current Population Survey (CPS) Merged Outgoing Rotation Groups data for analyzing earnings patterns, gender gaps, and returns to education in the US labor market.\nUsed in: Week 0 (Coding with AI)\nSource: Data Analysis Textbook Case Study",
    "crumbs": [
      "Home",
      "Case Studies",
      "US Earnings (CPS)"
    ]
  },
  {
    "objectID": "case-studies/earnings/index.html#overview",
    "href": "case-studies/earnings/index.html#overview",
    "title": "US Earnings Data (CPS MORG)",
    "section": "",
    "text": "Current Population Survey (CPS) Merged Outgoing Rotation Groups data for analyzing earnings patterns, gender gaps, and returns to education in the US labor market.\nUsed in: Week 0 (Coding with AI)\nSource: Data Analysis Textbook Case Study",
    "crumbs": [
      "Home",
      "Case Studies",
      "US Earnings (CPS)"
    ]
  },
  {
    "objectID": "case-studies/earnings/index.html#data-files",
    "href": "case-studies/earnings/index.html#data-files",
    "title": "US Earnings Data (CPS MORG)",
    "section": "Data Files",
    "text": "Data Files\n\n\n\nFile\nDescription\n\n\n\n\nmorg-2014-emp-state5.csv\nCPS MORG 2014 data filtered to 5 states\n\n\ncpsx.pdf\nCodebook and variable documentation\n\n\n\nBrowse data files",
    "crumbs": [
      "Home",
      "Case Studies",
      "US Earnings (CPS)"
    ]
  },
  {
    "objectID": "case-studies/earnings/index.html#code",
    "href": "case-studies/earnings/index.html#code",
    "title": "US Earnings Data (CPS MORG)",
    "section": "Code",
    "text": "Code\n\n\n\nFile\nDescription\n\n\n\n\nearnings.R\nR script for earnings analysis\n\n\n\nBrowse code files",
    "crumbs": [
      "Home",
      "Case Studies",
      "US Earnings (CPS)"
    ]
  },
  {
    "objectID": "case-studies/earnings/index.html#key-variables",
    "href": "case-studies/earnings/index.html#key-variables",
    "title": "US Earnings Data (CPS MORG)",
    "section": "Key Variables",
    "text": "Key Variables\n\nearnwke - Weekly earnings\nsex - Gender\nage - Age\ngrade92 - Education level\nweight - Survey weight",
    "crumbs": [
      "Home",
      "Case Studies",
      "US Earnings (CPS)"
    ]
  },
  {
    "objectID": "case-studies/earnings/index.html#learning-objectives",
    "href": "case-studies/earnings/index.html#learning-objectives",
    "title": "US Earnings Data (CPS MORG)",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nWorking with survey data\nAnalyzing earnings distributions\nCreating visualizations with AI assistance\nUnderstanding gender wage gaps",
    "crumbs": [
      "Home",
      "Case Studies",
      "US Earnings (CPS)"
    ]
  },
  {
    "objectID": "case-studies/earnings/index.html#sample-research-questions",
    "href": "case-studies/earnings/index.html#sample-research-questions",
    "title": "US Earnings Data (CPS MORG)",
    "section": "Sample Research Questions",
    "text": "Sample Research Questions\n\nIs there a gender gap in earnings?\nHow do earnings vary by education level?\nWhat is the age-earnings profile?",
    "crumbs": [
      "Home",
      "Case Studies",
      "US Earnings (CPS)"
    ]
  },
  {
    "objectID": "case-studies/earnings/index.html#downloads",
    "href": "case-studies/earnings/index.html#downloads",
    "title": "US Earnings Data (CPS MORG)",
    "section": "Downloads",
    "text": "Downloads\n\nmorg-2014-emp-state5.csv\nCodebook (PDF)",
    "crumbs": [
      "Home",
      "Case Studies",
      "US Earnings (CPS)"
    ]
  },
  {
    "objectID": "case-studies/VWS/data/README_WVS.html",
    "href": "case-studies/VWS/data/README_WVS.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "case-studies/VWS/data/README_WVS.html#overview",
    "href": "case-studies/VWS/data/README_WVS.html#overview",
    "title": "",
    "section": "Overview",
    "text": "Overview\nThis script cleans and subsets World Values Survey (WVS) Wave 7 data, generates a random subsample, aggregates by country & year, and merges with World Bank GDP indicators."
  },
  {
    "objectID": "case-studies/VWS/data/README_WVS.html#prerequisites",
    "href": "case-studies/VWS/data/README_WVS.html#prerequisites",
    "title": "",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nPackages:\n\nosfr (download from OSF)\n\ndplyr (data manipulation)\n\nreadr (CSV I/O)\n\nWDI (World Bank API)"
  },
  {
    "objectID": "case-studies/VWS/data/README_WVS.html#directory-structure",
    "href": "case-studies/VWS/data/README_WVS.html#directory-structure",
    "title": "",
    "section": "Directory Structure",
    "text": "Directory Structure\nproject-root/\n├─ data/\n│  ├─ raw/        ← input CSVs\n│  └─ clean/      ← outputs\n└─ scripts/\n   └─ cleaning.R ← this script"
  },
  {
    "objectID": "case-studies/VWS/data/README_WVS.html#input",
    "href": "case-studies/VWS/data/README_WVS.html#input",
    "title": "",
    "section": "Input",
    "text": "Input\n\ndata/raw/WVS_Cross-National_Wave_7_csv_v6_0.csv\nDownloaded automatically from OSF (ID: 36dgb)."
  },
  {
    "objectID": "case-studies/VWS/data/README_WVS.html#output",
    "href": "case-studies/VWS/data/README_WVS.html#output",
    "title": "",
    "section": "Output",
    "text": "Output\n\nWVS_subset.csv\nSelected variables and respondents, wave 1–7.\nWVS_random_subset2000.csv\nRandom sample of 2 000 respondents (≈ per country).\nWVS_GDP_merged_data.csv\nAggregated (mean & mode) by country & year for wave 7, merged with GDP & population (2017–2023)."
  },
  {
    "objectID": "case-studies/VWS/data/README_WVS.html#processing-steps",
    "href": "case-studies/VWS/data/README_WVS.html#processing-steps",
    "title": "",
    "section": "Processing Steps",
    "text": "Processing Steps\n\nSetup\n\nClear environment (rm(list=ls()))\n\nLoad libraries\n\nDefine data_in and data_out folders\n\nImport & Subset\n\nDownload raw CSV via OSF\n\nSelect key demographics (country codes, interview date, weights) and survey items (Q1–Q89, Q260–Q290)\n\nSave to WVS_subset.csv\nNote: This file contains answers from all respondents from the data.\n\nRandom Subsample\n\nIn this step, we create a random subsample to reduce sample size.\nSeed: 20250124\n\nSample ~2 000 respondents stratified by country\n\nCount the resulting number of respondents in each country\nSave to WVS_random_subset2000.csv\n\nAggregate & Clean\n\nIn this step, we aggregate the full data (step 2 data) to country-level, then join with GDP data.\nRecode negative codes (–1…–5) to NA\n\nCount the number of respondents in each country\nCompute country–year means for numeric items, modes for categorical\n\nDownload GDP & population (2017–2023) via WDI\n\nMerge on ISO3 country code & year\n\nSave to WVS_GDP_merged_data.csv"
  },
  {
    "objectID": "case-studies/VWS/data/README_WVS.html#usage",
    "href": "case-studies/VWS/data/README_WVS.html#usage",
    "title": "",
    "section": "Usage",
    "text": "Usage\nRscript scripts/cleaning.R\nEnsure your working directory is set to project root.\nRaw data and outputs will live under data/raw and data/clean."
  },
  {
    "objectID": "case-studies/VWS/data/README_WVS.html#contact",
    "href": "case-studies/VWS/data/README_WVS.html#contact",
    "title": "",
    "section": "Contact",
    "text": "Contact\ngabors-data-analysis.com | MA (BA) Data Analysis with AI course"
  },
  {
    "objectID": "case-studies/VWS/index.html",
    "href": "case-studies/VWS/index.html",
    "title": "World Values Survey (WVS)",
    "section": "",
    "text": "The 7th Wave of the World Values Survey, cleaned and merged with World Bank GDP data. The WVS is a global research project exploring people’s values and beliefs across different countries and cultures.\nUsed in: Week 2 (Data Documentation), Week 3 (Report Writing)",
    "crumbs": [
      "Home",
      "Case Studies",
      "World Values Survey"
    ]
  },
  {
    "objectID": "case-studies/VWS/index.html#overview",
    "href": "case-studies/VWS/index.html#overview",
    "title": "World Values Survey (WVS)",
    "section": "",
    "text": "The 7th Wave of the World Values Survey, cleaned and merged with World Bank GDP data. The WVS is a global research project exploring people’s values and beliefs across different countries and cultures.\nUsed in: Week 2 (Data Documentation), Week 3 (Report Writing)",
    "crumbs": [
      "Home",
      "Case Studies",
      "World Values Survey"
    ]
  },
  {
    "objectID": "case-studies/VWS/index.html#data-files",
    "href": "case-studies/VWS/index.html#data-files",
    "title": "World Values Survey (WVS)",
    "section": "Data Files",
    "text": "Data Files\n\n\n\n\n\n\n\nFile\nDescription\n\n\n\n\nWVS_random_subset2000.csv\nRandom subset of 2,000 respondents stratified by country\n\n\nWVS_GDP_merged_data.csv\nCountry-level aggregated data merged with GDP and population indicators\n\n\nWVS_subset.csv\nSelected variables from full dataset\n\n\ncodebook.pdf\nOfficial WVS codebook with variable definitions\n\n\nREADME_WVS.md\nData processing documentation\n\n\n\nBrowse data files",
    "crumbs": [
      "Home",
      "Case Studies",
      "World Values Survey"
    ]
  },
  {
    "objectID": "case-studies/VWS/index.html#code",
    "href": "case-studies/VWS/index.html#code",
    "title": "World Values Survey (WVS)",
    "section": "Code",
    "text": "Code\n\n\n\n\n\n\n\nFile\nDescription\n\n\n\n\ncleaning.R\nR script that processes raw WVS data and creates cleaned datasets\n\n\n\nBrowse code files",
    "crumbs": [
      "Home",
      "Case Studies",
      "World Values Survey"
    ]
  },
  {
    "objectID": "case-studies/VWS/index.html#key-features",
    "href": "case-studies/VWS/index.html#key-features",
    "title": "World Values Survey (WVS)",
    "section": "Key Features",
    "text": "Key Features\n\nInternational survey data covering 50+ countries\nEconomic indicators merged from World Bank\nMultiple aggregation levels (individual and country-level)\nDetailed codebook documentation",
    "crumbs": [
      "Home",
      "Case Studies",
      "World Values Survey"
    ]
  },
  {
    "objectID": "case-studies/VWS/index.html#sample-research-questions",
    "href": "case-studies/VWS/index.html#sample-research-questions",
    "title": "World Values Survey (WVS)",
    "section": "Sample Research Questions",
    "text": "Sample Research Questions\n\nRelationship between income level and trust/happiness\nCultural differences in gender attitudes\nEconomic development and social values",
    "crumbs": [
      "Home",
      "Case Studies",
      "World Values Survey"
    ]
  },
  {
    "objectID": "case-studies/VWS/index.html#downloads",
    "href": "case-studies/VWS/index.html#downloads",
    "title": "World Values Survey (WVS)",
    "section": "Downloads",
    "text": "Downloads\n\nWVS_random_subset2000.csv\nWVS_GDP_merged_data.csv\nCodebook (PDF)",
    "crumbs": [
      "Home",
      "Case Studies",
      "World Values Survey"
    ]
  },
  {
    "objectID": "case-studies/austria-hotels/data/index.html",
    "href": "case-studies/austria-hotels/data/index.html",
    "title": "Austrian Hotels - Data Files",
    "section": "",
    "text": "← Back to Austrian Hotels Case Study"
  },
  {
    "objectID": "case-studies/austria-hotels/data/index.html#data-files",
    "href": "case-studies/austria-hotels/data/index.html#data-files",
    "title": "Austrian Hotels - Data Files",
    "section": "Data Files",
    "text": "Data Files\nDownload the complete dataset: data-modified.zip\n\nIndividual Files\n\n\n\nFile\nDescription\n\n\n\n\nhotels_modified.csv\nBasic hotel information (200 hotels)\n\n\ncities_modified.csv\nCity information (10 cities)\n\n\nmonthly_occupancy_modified.csv\nMonthly hotel performance (~3,800 rows)\n\n\ncity_tourism_modified.csv\nMonthly tourism statistics (240 rows)\n\n\neconomic_indicators.csv\nMonthly economic indicators (24 rows)\n\n\nreviews_modified.csv\nHotel guest reviews (~1,700 rows)\n\n\namenities.csv\nList of hotel amenities (10 items)\n\n\nhotel_amenities_modified.csv\nHotel-amenity relationships (~1,000 rows)"
  },
  {
    "objectID": "case-studies/austria-hotels/data/index.html#documentation",
    "href": "case-studies/austria-hotels/data/index.html#documentation",
    "title": "Austrian Hotels - Data Files",
    "section": "Documentation",
    "text": "Documentation\n\nhotel-data-readme.md - Detailed schema documentation with column descriptions"
  },
  {
    "objectID": "case-studies/austria-hotels/index.html",
    "href": "case-studies/austria-hotels/index.html",
    "title": "Austrian Hotels Dataset",
    "section": "",
    "text": "A realistic simulated dataset of hotels across Austria for practicing data wrangling and table joins. Contains multiple related tables with hotels, cities, occupancy, tourism, and economic data.\nUsed in: Week 4 (Joining Tables)\nGenerated by: Claude AI (Sonnet 3.7) with realistic relationships between variables",
    "crumbs": [
      "Home",
      "Case Studies",
      "Austrian Hotels"
    ]
  },
  {
    "objectID": "case-studies/austria-hotels/index.html#overview",
    "href": "case-studies/austria-hotels/index.html#overview",
    "title": "Austrian Hotels Dataset",
    "section": "",
    "text": "A realistic simulated dataset of hotels across Austria for practicing data wrangling and table joins. Contains multiple related tables with hotels, cities, occupancy, tourism, and economic data.\nUsed in: Week 4 (Joining Tables)\nGenerated by: Claude AI (Sonnet 3.7) with realistic relationships between variables",
    "crumbs": [
      "Home",
      "Case Studies",
      "Austrian Hotels"
    ]
  },
  {
    "objectID": "case-studies/austria-hotels/index.html#data-files",
    "href": "case-studies/austria-hotels/index.html#data-files",
    "title": "Austrian Hotels Dataset",
    "section": "Data Files",
    "text": "Data Files\nThe dataset includes 8 related tables:\n\n\n\n\n\n\n\n\nFile\nDescription\nRows\n\n\n\n\nhotels_modified.csv\nBasic hotel information\n200\n\n\ncities_modified.csv\nCity information\n10\n\n\nmonthly_occupancy_modified.csv\nMonthly hotel performance\n~3,800\n\n\ncity_tourism_modified.csv\nMonthly tourism stats\n240\n\n\neconomic_indicators.csv\nMonthly economic indicators\n24\n\n\nreviews_modified.csv\nHotel guest reviews\n~1,700\n\n\namenities.csv\nList of hotel amenities\n10\n\n\nhotel_amenities_modified.csv\nHotel-amenity relationships\n~1,000\n\n\n\nBrowse data files",
    "crumbs": [
      "Home",
      "Case Studies",
      "Austrian Hotels"
    ]
  },
  {
    "objectID": "case-studies/austria-hotels/index.html#documentation",
    "href": "case-studies/austria-hotels/index.html#documentation",
    "title": "Austrian Hotels Dataset",
    "section": "Documentation",
    "text": "Documentation\n\nhotel-data-readme.md - Detailed schema documentation",
    "crumbs": [
      "Home",
      "Case Studies",
      "Austrian Hotels"
    ]
  },
  {
    "objectID": "case-studies/austria-hotels/index.html#code",
    "href": "case-studies/austria-hotels/index.html#code",
    "title": "Austrian Hotels Dataset",
    "section": "Code",
    "text": "Code\nCode examples to be added\nBrowse code files",
    "crumbs": [
      "Home",
      "Case Studies",
      "Austrian Hotels"
    ]
  },
  {
    "objectID": "case-studies/austria-hotels/index.html#key-relationships",
    "href": "case-studies/austria-hotels/index.html#key-relationships",
    "title": "Austrian Hotels Dataset",
    "section": "Key Relationships",
    "text": "Key Relationships\n\nOne-to-One: Hotels to Cities (through city name)\nOne-to-Many: Hotels to Monthly Occupancy, Hotels to Reviews\nMany-to-Many: Hotels to Amenities (through hotel_amenities)\nComposite Keys: Monthly data uses (hotel_id, month, year)",
    "crumbs": [
      "Home",
      "Case Studies",
      "Austrian Hotels"
    ]
  },
  {
    "objectID": "case-studies/austria-hotels/index.html#learning-objectives",
    "href": "case-studies/austria-hotels/index.html#learning-objectives",
    "title": "Austrian Hotels Dataset",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nInner, left, right, and full joins\nOne-to-one and one-to-many relationships\nComposite key joins\nData aggregation after joins\nHandling missing values in joins",
    "crumbs": [
      "Home",
      "Case Studies",
      "Austrian Hotels"
    ]
  },
  {
    "objectID": "case-studies/austria-hotels/index.html#downloads",
    "href": "case-studies/austria-hotels/index.html#downloads",
    "title": "Austrian Hotels Dataset",
    "section": "Downloads",
    "text": "Downloads\n\ndata-modified.zip - All data files",
    "crumbs": [
      "Home",
      "Case Studies",
      "Austrian Hotels"
    ]
  },
  {
    "objectID": "case-studies/common-support-r/data/index.html",
    "href": "case-studies/common-support-r/data/index.html",
    "title": "Common Support Analysis - Data",
    "section": "",
    "text": "← Back to Common Support Case Study"
  },
  {
    "objectID": "case-studies/common-support-r/data/index.html#data-files",
    "href": "case-studies/common-support-r/data/index.html#data-files",
    "title": "Common Support Analysis - Data",
    "section": "Data Files",
    "text": "Data Files\nThis case study uses simulated data generated within the R scripts.\nNo external data files required."
  },
  {
    "objectID": "case-studies/index.html",
    "href": "case-studies/index.html",
    "title": "Case Studies",
    "section": "",
    "text": "This course uses several case studies to demonstrate data analysis with AI. Each case study includes data, code examples, and documentation."
  },
  {
    "objectID": "case-studies/index.html#main-case-studies",
    "href": "case-studies/index.html#main-case-studies",
    "title": "Case Studies",
    "section": "Main Case Studies",
    "text": "Main Case Studies\n\nWorld Values Survey (WVS)\nThe 7th Wave of the World Values Survey, cleaned and merged with World Bank GDP data. Used for data documentation and report writing exercises.\nUsed in: Week 2 (Data Documentation), Week 3 (Report Writing)\n\n\n\nAustrian Hotels\nA realistic simulated dataset of hotels across Austria for practicing data wrangling and table joins. Contains 8 related tables.\nUsed in: Week 4 (Joining Tables)\n\n\n\nFootball Manager Interviews\nPost-match interview texts from football managers for sentiment analysis and NLP practice. Includes rating systems for comparison and validation.\nUsed in: Week 5 (Text as Data I), Week 6 (Text as Data II)"
  },
  {
    "objectID": "case-studies/index.html#additional-case-studies",
    "href": "case-studies/index.html#additional-case-studies",
    "title": "Case Studies",
    "section": "Additional Case Studies",
    "text": "Additional Case Studies\n\nUS Earnings Data (CPS)\nCurrent Population Survey data for analyzing earnings patterns, gender gaps, and returns to education.\nUsed in: Week 0 (Coding with AI)\n\n\n\nEmployee Commits\nSimulated dataset of employee commit activity for exploring AI-generated data analysis comparisons.\n\n\n\nCommon Support Analysis (R)\nIterative development of R code for causal inference analysis using Claude AI, demonstrating prompt-refine workflow."
  },
  {
    "objectID": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html",
    "href": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#what-changed-in-v2.1",
    "href": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#what-changed-in-v2.1",
    "title": "",
    "section": "What Changed in v2.1",
    "text": "What Changed in v2.1\nIncreased extreme values from 10 to 50: - 30 very high commits (range: 200-450/month) — up from 3 - 10 zero commits with long tenure — up from 3 - 5 high commits for juniors — up from 2 - 5 high commits for infrastructure — up from 2\nData errors remain at 8 (unchanged from v2.0)\nTotal problematic cases: 58 (8 errors + 50 extreme values) Clean observations: 542 Total: 600 employees"
  },
  {
    "objectID": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#files-provided",
    "href": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#files-provided",
    "title": "",
    "section": "Files Provided",
    "text": "Files Provided\n\n1. employee_commits_claude.csv\nFull dataset with all metadata (600 rows × 8 columns)\nColumns: - i - Employee ID - x - Days with company - y - Monthly commits - department - IT or Analytics - seniority - Junior, Mid, Senior - role - Specific job role - data_issue - Description of any quality issues - issue_category - Clean, Data Error, or Extreme Value\nUse this for teaching data quality and cleaning decisions.\n\n\n2. employee_commits_raw.csv\nMinimal dataset (600 rows × 5 columns)\nColumns: - i - Employee ID - x - Days with company - y - Monthly commits - department - IT or Analytics - role - Specific job role\nDropped columns: seniority, data_issue, issue_category\nUse this as the “raw data” students receive before they know about issues.\n\n\n3. employee_commits_clean.csv\nOnly clean observations (542 rows)\nAll 8 data errors and 50 extreme values removed. Use for comparison or to skip the cleaning exercise.\n\n\n4. data_quality_report.csv\nList of all 58 problematic cases\nShows each issue with employee ID, x, y, and explanation. Use as an answer key or for discussion.\n\n\n5. employee_commits_v2_plots.png\nDiagnostic visualizations\nFour panels showing data quality issues highlighted: - Full view with errors (X) and extreme values (★) - Zoomed view focusing on reasonable range - Distribution of monthly commits - Boxplots by role"
  },
  {
    "objectID": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#data-quality-issues-breakdown",
    "href": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#data-quality-issues-breakdown",
    "title": "",
    "section": "Data Quality Issues Breakdown",
    "text": "Data Quality Issues Breakdown\n\nData Errors (8 cases) - Must Fix\n\n\n\n\n\n\n\n\n\nIssue Type\nCount\nRange\nStory\n\n\n\n\nNegative commits\n2\n-15, -8\nData pipeline bug\n\n\nImpossibly high commits\n3\n6,800 - 12,000\nDecimal/aggregation error\n\n\nNegative tenure\n1\n-120 days\nDate calculation error\n\n\nImpossibly long tenure\n2\n19,500 - 22,000 days\nDate entry error (1925 vs 2025)\n\n\n\n\n\nExtreme Values (50 cases) - Require Judgment\n\n\n\n\n\n\n\n\n\n\nIssue Type\nCount\nRange\nStory\nKeep or Drop?\n\n\n\n\nVery high commits\n30\n200 - 450/month\nBot-assisted or granular style\nAnalyst choice\n\n\nZero commits, long tenure\n10\nx &gt; 2,900 days\nMoved to management\nAnalyst choice\n\n\nHigh commits for junior\n5\n154 - 248/month\nInternal transfers\nAnalyst choice\n\n\nHigh for infrastructure\n5\n75 - 108/month\nIaC enthusiasts\nAnalyst choice"
  },
  {
    "objectID": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#key-statistics",
    "href": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#key-statistics",
    "title": "",
    "section": "Key Statistics",
    "text": "Key Statistics\n\nFull Dataset (including issues)\n\nx: Mean = 1,496 days, Min = -120, Max = 22,000\ny: Mean = 87, Min = -15, Max = 12,000\n\n\n\nClean Data Only (542 obs)\n\nx: Mean = 1,434 days (3.9 years), Range = 3 to 5,446\ny: Median = 18, Mean = 42, Range = 0 to 445\n\n\n\nWith Extreme Values Removed (542 obs)\n\ny: Median = 15, Mean = 27, Range = 0 to 150 (approximately)"
  },
  {
    "objectID": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#teaching-questions",
    "href": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#teaching-questions",
    "title": "",
    "section": "Teaching Questions",
    "text": "Teaching Questions\n\nLevel 1: Data Quality\n\nIdentify which observations are clearly errors vs possibly valid\nWhat’s your decision rule for each type of extreme value?\nDocument the impact of including vs excluding extreme values\n\n\n\nLevel 2: Exploration\n\nVisualize the relationship between x and y\nHow does it differ by department? By role?\nIs the relationship linear? Log-linear? Something else?\n\n\n\nLevel 3: Modeling\n\nCompare multiple approaches:\n\nLinear regression (OLS)\nLog transformations\nPoisson regression\nNegative binomial regression\nRobust regression (M-estimators)\nQuantile regression (median)\n\nWhich model assumptions are violated and why?\nHow sensitive are results to data cleaning decisions?\n\n\n\nLevel 4: Communication\n\nWhat’s your conclusion about the x-y relationship?\nHow certain are you? What are the caveats?\nWhat additional data would help?\nHow would you report this to a non-technical stakeholder?"
  },
  {
    "objectID": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#suggested-workflow-for-students",
    "href": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#suggested-workflow-for-students",
    "title": "",
    "section": "Suggested Workflow for Students",
    "text": "Suggested Workflow for Students\nStage 1: Initial Exploration - Load employee_commits_raw.csv - Create basic visualizations - Calculate summary statistics - Notice anything unusual?\nStage 2: Data Quality Assessment - Identify outliers and unusual values - Categorize as errors vs extreme values - Make cleaning decisions with justification\nStage 3: Multiple Analyses - Try 3-4 different cleaning approaches - For each: visualize, model, interpret - Compare results across approaches\nStage 4: Synthesis - Which approach do you recommend and why? - What is the relationship between x and y? - How confident are you in this conclusion?"
  },
  {
    "objectID": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#instructor-notes",
    "href": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#instructor-notes",
    "title": "",
    "section": "Instructor Notes",
    "text": "Instructor Notes\n\nWhy This Dataset Works\n\nRealistic complexity: Not obviously clean or dirty\nAmbiguity: No single “correct” answer for extreme values\nMultiple valid approaches: Forces critical thinking\nCount data features: Overdispersion, zeros, skewness\nRich metadata: Can explore heterogeneity\n\n\n\nDiscussion Points\n\nError vs extreme: What makes something an error vs just unusual?\nDecision rules: How do you decide what to keep/drop?\nTransparency: How do you document cleaning decisions?\nSensitivity: When do conclusions change materially?\nModel choice: Why prefer one model over another for count data?\n\n\n\nExtensions\n\nAdd time dimension (multiple months per employee)\nInclude team/project identifiers for clustering\nAdd more covariates (education, experience, etc.)\nCreate a prediction task (predict commits for new employees)"
  },
  {
    "objectID": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#technical-details",
    "href": "case-studies/employee_commits/data/background/SUMMARY_v2.1.html#technical-details",
    "title": "",
    "section": "Technical Details",
    "text": "Technical Details\nSeed: 20251112\nSoftware: Python 3.x with numpy, pandas, matplotlib\nAlso available: R version (simulate_employee_commits_v2.R)\nKey assumptions: - Base monthly rates by role (7-38 commits/month) - Seniority multipliers (0.6x, 1.2x, 0.8x) - Individual heterogeneity (log-normal, σ=0.45) - Negative binomial distribution (size=5) - 5% baseline zeros (moved teams, admin roles, etc.)\n\nFor questions or modifications, see generate_data_v2.1.py"
  },
  {
    "objectID": "case-studies/employee_commits/data/index.html",
    "href": "case-studies/employee_commits/data/index.html",
    "title": "Employee Commits - Data Files",
    "section": "",
    "text": "← Back to Employee Commits Case Study"
  },
  {
    "objectID": "case-studies/employee_commits/data/index.html#data-files",
    "href": "case-studies/employee_commits/data/index.html#data-files",
    "title": "Employee Commits - Data Files",
    "section": "Data Files",
    "text": "Data Files\n\n\n\nFile\nDescription\n\n\n\n\nemployee_commits_clean.csv\nCleaned commit data\n\n\nemployee_commits_raw.csv\nRaw commit data\n\n\nREADME.txt\nData description"
  },
  {
    "objectID": "case-studies/employee_commits/data/index.html#ai-analysis-outputs",
    "href": "case-studies/employee_commits/data/index.html#ai-analysis-outputs",
    "title": "Employee Commits - Data Files",
    "section": "AI Analysis Outputs",
    "text": "AI Analysis Outputs\nThe ai/ subfolder contains analysis outputs from different AI models for comparison:\n\nChatGPT analysis outputs\nClaude analysis outputs\nGemini analysis outputs"
  },
  {
    "objectID": "case-studies/interviews/code/index.html",
    "href": "case-studies/interviews/code/index.html",
    "title": "Football Manager Interviews - Code",
    "section": "",
    "text": "← Back to Interviews Case Study"
  },
  {
    "objectID": "case-studies/interviews/code/index.html#code-files",
    "href": "case-studies/interviews/code/index.html#code-files",
    "title": "Football Manager Interviews - Code",
    "section": "Code Files",
    "text": "Code Files\n\nR Scripts\n\n\n\nFile\nDescription\n\n\n\n\nsentiment-analysis.R\nComplete sentiment analysis workflow using OpenAI API\n\n\ndomain_lexicon.r\nCreates football-specific sentiment lexicon\n\n\napi-key.R\nTemplate for API key setup\n\n\n\n\n\nPython Scripts\n\n\n\nFile\nDescription\n\n\n\n\nsentiment_analysis.py\nPython sentiment analysis script"
  },
  {
    "objectID": "case-studies/interviews/code/index.html#usage-notes",
    "href": "case-studies/interviews/code/index.html#usage-notes",
    "title": "Football Manager Interviews - Code",
    "section": "Usage Notes",
    "text": "Usage Notes\n\nStore API keys in environment variables, never commit to version control\nBudget ~$5 for course exercises\nSet temperature=0 for consistent results"
  },
  {
    "objectID": "case-studies/interviews/index.html",
    "href": "case-studies/interviews/index.html",
    "title": "Football Manager Interviews",
    "section": "",
    "text": "Post-match interview texts from football managers for sentiment analysis and NLP practice. Includes multiple rating systems for comparison and validation.\nUsed in: Week 5 (Text as Data I), Week 6 (Text as Data II), Assignment 5",
    "crumbs": [
      "Home",
      "Case Studies",
      "Football Interviews"
    ]
  },
  {
    "objectID": "case-studies/interviews/index.html#overview",
    "href": "case-studies/interviews/index.html#overview",
    "title": "Football Manager Interviews",
    "section": "",
    "text": "Post-match interview texts from football managers for sentiment analysis and NLP practice. Includes multiple rating systems for comparison and validation.\nUsed in: Week 5 (Text as Data I), Week 6 (Text as Data II), Assignment 5",
    "crumbs": [
      "Home",
      "Case Studies",
      "Football Interviews"
    ]
  },
  {
    "objectID": "case-studies/interviews/index.html#data-files",
    "href": "case-studies/interviews/index.html#data-files",
    "title": "Football Manager Interviews",
    "section": "Data Files",
    "text": "Data Files\n\n\n\nFile\nDescription\n\n\n\n\ninterview-texts-only.xlsx\nRaw interview texts (121 interviews)\n\n\ncombined_text_data.csv\nCombined text data with metadata\n\n\ncombined_raw_data.csv\nFull raw dataset\n\n\ndomain_lexicon.csv\nFootball-specific sentiment terms\n\n\ngame_data.xlsx\nGame results and context\n\n\naggregated_ratings_by_text.csv\nAggregated sentiment ratings\n\n\n\nBrowse data files",
    "crumbs": [
      "Home",
      "Case Studies",
      "Football Interviews"
    ]
  },
  {
    "objectID": "case-studies/interviews/index.html#code",
    "href": "case-studies/interviews/index.html#code",
    "title": "Football Manager Interviews",
    "section": "Code",
    "text": "Code\n\n\n\nFile\nDescription\n\n\n\n\nsentiment-analysis.R\nR script for API-based sentiment analysis\n\n\nsentiment_analysis.py\nPython script for sentiment analysis\n\n\ndomain_lexicon.r\nR script to create football-specific lexicon\n\n\napi-key.R\nTemplate for API key setup\n\n\n\nBrowse code files",
    "crumbs": [
      "Home",
      "Case Studies",
      "Football Interviews"
    ]
  },
  {
    "objectID": "case-studies/interviews/index.html#key-features",
    "href": "case-studies/interviews/index.html#key-features",
    "title": "Football Manager Interviews",
    "section": "Key Features",
    "text": "Key Features\n\nReal Premier League manager interview texts\nDomain-specific sentiment lexicon\nBoth R and Python implementations\nHuman vs AI rating comparison framework\nAPI integration examples (OpenAI)",
    "crumbs": [
      "Home",
      "Case Studies",
      "Football Interviews"
    ]
  },
  {
    "objectID": "case-studies/interviews/index.html#learning-objectives",
    "href": "case-studies/interviews/index.html#learning-objectives",
    "title": "Football Manager Interviews",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nText preprocessing and cleaning\nSentiment analysis approaches\nDomain-specific lexicons\nAPI integration for NLP tasks\nComparing human and AI ratings",
    "crumbs": [
      "Home",
      "Case Studies",
      "Football Interviews"
    ]
  },
  {
    "objectID": "case-studies/interviews/index.html#downloads",
    "href": "case-studies/interviews/index.html#downloads",
    "title": "Football Manager Interviews",
    "section": "Downloads",
    "text": "Downloads\n\ninterview-texts-only.xlsx\ndomain_lexicon.csv",
    "crumbs": [
      "Home",
      "Case Studies",
      "Football Interviews"
    ]
  },
  {
    "objectID": "week05/assets/sentiment-scale.html",
    "href": "week05/assets/sentiment-scale.html",
    "title": "General Sentiment Rating Guidelines",
    "section": "",
    "text": "Please read each text carefully and rate the overall sentiment of the manager’s statement as positive or negative. Your rating should reflect the manager’s expressed tone, not your judgment of the match."
  },
  {
    "objectID": "week05/assets/sentiment-scale.html#task",
    "href": "week05/assets/sentiment-scale.html#task",
    "title": "General Sentiment Rating Guidelines",
    "section": "",
    "text": "Please read each text carefully and rate the overall sentiment of the manager’s statement as positive or negative. Your rating should reflect the manager’s expressed tone, not your judgment of the match."
  },
  {
    "objectID": "week05/assets/sentiment-scale.html#rating-scale",
    "href": "week05/assets/sentiment-scale.html#rating-scale",
    "title": "General Sentiment Rating Guidelines",
    "section": "Rating Scale:",
    "text": "Rating Scale:\n\n\n\n\n\n\n\nScore\nMeaning\n\n\n\n\n2\nStrongly positive sentiment (clear optimism, satisfaction, praise).\n\n\n1\nMildly positive sentiment (generally positive, slight reservations).\n\n\n0\nNeutral or unclear sentiment.\n\n\n-1\nMildly negative sentiment (general disappointment, frustration).\n\n\n-2\nStrongly negative sentiment (clear criticism, significant disappointment)."
  },
  {
    "objectID": "week05/index.html",
    "href": "week05/index.html",
    "title": "Week 05: using text as data",
    "section": "",
    "text": "Week 05: using text as data\n\n\nTurning a series of short texts into tabular data: humans vs AI",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Using Text as Data"
    ]
  },
  {
    "objectID": "week05/index.html#overview",
    "href": "week05/index.html#overview",
    "title": "Week 05: using text as data",
    "section": "Overview",
    "text": "Overview\nIn this lesson, students will be introduced to sentiment analysis, specifically applied to evaluating general positivity or negativity in football managers’ statements about match outcomes.\n\nLearning Outcomes\nBy the end of the session, students will:\n\nUnderstand core NLP concepts: tokenization, preprocessing, bag-of-words, and feature extraction in practical context.\nGain hands-on experience with sentiment analysis.\nUnderstand the complexities and limitations of sentiment analysis.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Using Text as Data"
    ]
  },
  {
    "objectID": "week05/index.html#preparation-before-class",
    "href": "week05/index.html#preparation-before-class",
    "title": "Week 05: using text as data",
    "section": "Preparation / Before Class",
    "text": "Preparation / Before Class\n\n\n📚 Background Reading\n\n\nNo specific readings required - we’ll learn by doing\n\nNo need for natural language processing background.\nThinking ahead: How would you manually determine if someone sounds “positive” or “negative” about something?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Using Text as Data"
    ]
  },
  {
    "objectID": "week05/index.html#review",
    "href": "week05/index.html#review",
    "title": "Week 05: using text as data",
    "section": "Review",
    "text": "Review\n\n\n📈 Assignment Review (15 min)\n\n\nKey Learning Points:\n\nFancy graphs != good graphs (good graph ← careful design)\nPrecise interpretation &gt;&gt; BS\nLess is more\nShow only what you understand deeply\n\nLess is More Principle: Simplicity often reveals insights better than complexity",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Using Text as Data"
    ]
  },
  {
    "objectID": "week05/index.html#class-material",
    "href": "week05/index.html#class-material",
    "title": "Week 05: using text as data",
    "section": "Class Material",
    "text": "Class Material\n\n\n📊 Materials Setup\n\n\nData Context:\n\nThese are real post-match interviews from Premier League managers discussing game results.\nEach text represents one manager’s full response to journalists after a match.\nPreprocessing: Replaced player and team names with “PLAYER” and “TEAM” - removes potential bias\n\nPractice Dataset: Contains 5 post-match interviews from football managers\n\nStudent test file (CSV)\nStudent test file (Excel)\n\nReference Materials:\n\nGeneral sentiment rating scale - our classification framework\nDomain-specific lexicon - football-specific sentiment terms\n\n\n\n\n\n📖 NLP Fundamentals Lecture (25 min)\n\n\nSlideshow: Text to Data\nKey Concepts Covered:\n\nTokenization: Breaking text into meaningful units\nPreprocessing: Stemming, lemmatization, stop word removal\n\nBag of Words: Converting text to numerical features\nDomain knowledge: Why football-specific terms matter\nSentiment Analysis: Methods from simple dictionaries to AI models\n\nPractical Focus: How these concepts apply to manager interviews\n\n\n\n\n⚽ Manual vs. AI Sentiment Workshop (40 min)\n\n\nPhase 1: Human Rating (15 min)\n\nIndividual work: Rate the 5 provided manager statements using the sentiment scale\n5-unit scale: -2 (very negative) to +2 (very positive)\n\nPhase 2: AI Rating (15 min)\n\nUse your preferred AI to rate the same 5 statements\nTry different prompting approaches: simple vs. detailed instructions\nCompare AI’s reasoning with your own thought process\n\nPhase 3: Domain Lexicon Exploration (10 min)\n\nExamine the football-specific lexicon\nDiscuss how domain knowledge affects interpretation\n\nDiscussion: Validation and Sentiment Analysis\n\nObjective: Discuss validation techniques used in sentiment analysis.\nTopics for discussion:\n\nDifferences between manual and AI ratings\nGround Truth\nIntroduction to validation methods:\nIf ground truth – can do confusion maztric, calculate accuracy\nIf no ground truth – measure agreement between humans and AI. test difference.\n\nAI is average, but…\nAI with persona?\nAI biased ?\n\n\n\nExtra: Prediction of score\nIf time permits: look at predicting score\n\nModeling choices of results\nThink about how you would do it first\nCheck how AI thinks about, rate the examples and look at explanations\ntake the 5 examples, and compare your predictions vs the AI predictions",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Using Text as Data"
    ]
  },
  {
    "objectID": "week05/index.html#end-of-week-discussion-points",
    "href": "week05/index.html#end-of-week-discussion-points",
    "title": "Week 05: using text as data",
    "section": "End of Week Discussion points",
    "text": "End of Week Discussion points\n\nHow precise is AI in sentiment analysis?\nHow did you compare to AI in terms of scores? How did any difference make you feel?\nCan you think of a past project where AI could have helped you upgrade it?\nDid AI provide consistent explanations for its ratings? When did its reasoning seem flawed? Did you feel it was mostly OK / half-truths?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Using Text as Data"
    ]
  },
  {
    "objectID": "week05/index.html#assignment",
    "href": "week05/index.html#assignment",
    "title": "Week 05: using text as data",
    "section": "Assignment",
    "text": "Assignment\n\n\n\n\n\n\nNoteAssignment 5: Student-Specific Sentiment Analysis\n\n\n\nDue: Before Week 6\nCompare manual ratings with AI-generated ratings on your own random sample of 25 interview texts.\nFull Assignment Details",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Using Text as Data"
    ]
  },
  {
    "objectID": "week05/index.html#some-personal-comments-on-ai-and-this-class",
    "href": "week05/index.html#some-personal-comments-on-ai-and-this-class",
    "title": "Week 05: using text as data",
    "section": "Some personal comments on AI and this class",
    "text": "Some personal comments on AI and this class\n\nIt’s worth noting that my knowledge on NLP was bag of words, bigrams and TF-IDF. AI really helped making the jump.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Using Text as Data"
    ]
  },
  {
    "objectID": "week02/assets/claude-research-football-interviews-result.html",
    "href": "week02/assets/claude-research-football-interviews-result.html",
    "title": "",
    "section": "",
    "text": "Code\nID: compass_artifact_wf-51f04f81-6ecb-4a38-823d-d2832531840f_text_markdown Date: 2025-09-09"
  },
  {
    "objectID": "week02/assets/claude-research-football-interviews-result.html#key-findings-across-all-leagues",
    "href": "week02/assets/claude-research-football-interviews-result.html#key-findings-across-all-leagues",
    "title": "",
    "section": "Key findings across all leagues",
    "text": "Key findings across all leagues\nGeographic restrictions dominate access. Every major broadcaster geo-blocks content, creating the primary barrier to systematic collection. Subscription costs are universally high - ranging from €400-800 annually per league for partial coverage, with complete access often requiring professional licensing arrangements costing €10,000+ annually.\nCoverage quality correlates directly with club size and match importance. Major clubs like Bayern Munich, Real Madrid, and Juventus provide systematic post-game interview coverage through official channels, while smaller clubs offer inconsistent coverage. Away games consistently receive less comprehensive interview documentation across all leagues.\nThe Premier League serves as both benchmark and cautionary tale - it offers the most comprehensive coverage through multiple platforms but requires £66/month in subscriptions plus VPN access for international users, with no centralized archive solution."
  },
  {
    "objectID": "week02/assets/claude-research-football-interviews-result.html#bundesliga-coverage-landscape",
    "href": "week02/assets/claude-research-football-interviews-result.html#bundesliga-coverage-landscape",
    "title": "",
    "section": "Bundesliga coverage landscape",
    "text": "Bundesliga coverage landscape\nDeutsche Fußball Liga (DFL) Professional Licensing emerges as the most comprehensive source, offering post-match interviews for all 309 matches per season including 3-minute “super flash” interviews within 10 minutes of final whistle. However, this requires expensive business-to-business licensing arrangements likely exceeding €10,000 annually.\nConsumer-accessible options remain fragmented. Sky Deutschland (€44.99/month) covers Friday/Saturday matches with limited post-game interview coverage, while DAZN Germany (€9.99/month) provides expanding coverage for 240 matches starting 2025-26 season. The official Bundesliga website and YouTube channel offer inconsistent coverage across all 18 teams.\nMajor clubs like Bayern Munich operate FC Bayern TV with comprehensive post-match interview coverage requiring additional subscriptions. Union Berlin and SC Freiburg represent typical smaller clubs with basic websites and limited systematic interview archives.\nThe research reveals no readily accessible combination of sources provides systematic coverage of all Bundesliga post-game manager interviews. True comprehensive access requires either expensive DFL licensing or accepting significant coverage gaps."
  },
  {
    "objectID": "week02/assets/claude-research-football-interviews-result.html#la-ligas-distributed-approach",
    "href": "week02/assets/claude-research-football-interviews-result.html#la-ligas-distributed-approach",
    "title": "",
    "section": "La Liga’s distributed approach",
    "text": "La Liga’s distributed approach\nESPN+ provides the strongest systematic access for US audiences, offering all 380 matches with English and Spanish coverage including “player/coaches’ interviews and press conferences” for $10.99/month. This represents the most accessible comprehensive option but remains geo-restricted to US territories.\nSpanish domestic coverage splits between Movistar+ and DAZN Spain, each covering 5 matches per matchday. Movistar+ (€13-30/month) resumed independent production in 2025-26 after LaLiga Studios’ centralized approach, while DAZN Spain (€9.99-29.99/month) provides complementary coverage. Combined domestic access requires approximately €60/month.\nClub-level coverage varies dramatically. Real Madrid and Barcelona maintain sophisticated press facilities with systematic post-game interview coverage through official channels and subscription services like Barça TV+. Real Madrid’s YouTube channel (18.1M subscribers) provides reliable post-game press conference coverage.\nMid-tier clubs like Sevilla FC offer partial coverage with press conferences “one or two days before each official match” and post-match availability. Smaller clubs typically provide minimal digital publication of interview content beyond local media coverage."
  },
  {
    "objectID": "week02/assets/claude-research-football-interviews-result.html#serie-as-centralized-but-restricted-model",
    "href": "week02/assets/claude-research-football-interviews-result.html#serie-as-centralized-but-restricted-model",
    "title": "",
    "section": "Serie A’s centralized but restricted model",
    "text": "Serie A’s centralized but restricted model\nDAZN Italy holds exclusive rights to all 380 Serie A matches (2024-2029 deal) across multiple subscription tiers (€13.99-59.99/month), including “wide range of interviews and reports.” This represents the closest thing to systematic coverage but remains geo-restricted to Italy with no confirmed comprehensive interview archive.\nSky Sport Italia provides complementary coverage for 114 matches per season with confirmed post-match interview inclusion. International access through CBS Sports/Paramount+ (US) covers all matches with English commentary and interview content.\nItalian sports media offers extensive text-based coverage. La Gazzetta dello Sport (400,000+ daily circulation) provides systematic post-match coverage including interview excerpts across all 20 teams, while Corriere dello Sport and Tuttosport offer additional perspectives.\nMajor clubs maintain strong official coverage - Juventus, AC Milan, Inter Milan, AS Roma, and Napoli provide comprehensive post-match content through official websites and YouTube channels. Coverage for smaller clubs like Udinese, Empoli, and Lecce remains inconsistent, particularly for away games."
  },
  {
    "objectID": "week02/assets/claude-research-football-interviews-result.html#systematic-collection-challenges",
    "href": "week02/assets/claude-research-football-interviews-result.html#systematic-collection-challenges",
    "title": "",
    "section": "Systematic collection challenges",
    "text": "Systematic collection challenges\nNo centralized databases exist across any league. Football data APIs from providers like API-Football and Sportmonks focus on match statistics rather than interview content, leaving systematic collection dependent on web scraping and multi-platform monitoring.\nLanguage barriers compound accessibility issues. Bundesliga content remains primarily German-only, La Liga coverage is 95% Spanish, and Serie A content is predominantly Italian. English coverage exists mainly through international broadcasters with geographic restrictions.\nTechnical and legal considerations create additional barriers. Subscription terms may restrict systematic capture, while significant data storage requirements (estimated 15-20GB/week for full video coverage) necessitate substantial infrastructure investment."
  },
  {
    "objectID": "week02/assets/claude-research-football-interviews-result.html#strategic-recommendations",
    "href": "week02/assets/claude-research-football-interviews-result.html#strategic-recommendations",
    "title": "",
    "section": "Strategic recommendations",
    "text": "Strategic recommendations\nFor maximum coverage efficiency, focus on international broadcaster subscriptions rather than attempting domestic access. ESPN+ for La Liga (US), CBS Sports/Paramount+ for Serie A (US), and combining these with German-accessible DAZN represents the most cost-effective comprehensive approach.\nImplement tier-based collection strategy. Primary tier should target major clubs’ official channels which provide most reliable systematic coverage. Secondary tier monitors league-specific broadcasters within subscription budgets. Tertiary tier fills gaps through sports media text-based coverage.\nGeographic circumvention tools become essential for systematic collection. VPN services rated for streaming access to multiple regions will be required, with associated legal and technical considerations for terms of service compliance.\nArchive building requires immediate capture due to limited retention policies across platforms. No provider maintains comprehensive historical archives, making real-time collection and storage infrastructure critical for building systematic databases."
  },
  {
    "objectID": "week02/assets/claude-research-football-interviews-result.html#conclusion",
    "href": "week02/assets/claude-research-football-interviews-result.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nThe research reveals a fundamental impossibility of truly systematic coverage through publicly accessible sources alone. Professional licensing arrangements remain the only path to complete coverage, requiring substantial investment beyond most research budgets.\nThe most practical approach involves strategic compromise - accepting 70-80% coverage through optimized multi-source strategies rather than pursuing impossible 100% systematic collection. ESPN+ for La Liga, DAZN subscriptions for Bundesliga/Serie A domestic coverage, and major club official channels provide the highest efficiency coverage available within reasonable budget constraints.\nCompared to Premier League accessibility, all three leagues present equal or greater challenges - the Premier League’s £66/month subscription requirement plus geographic restrictions actually represents better accessibility than most available options for Bundesliga, La Liga, or Serie A systematic coverage."
  },
  {
    "objectID": "week02/assets/da-background.html",
    "href": "week02/assets/da-background.html",
    "title": "Data Analysis Background",
    "section": "",
    "text": "This is based on Békés-Kézdi: Data Analysis for Business, Econoomics, and Policy (2021, Cambridge University Press), Chapter 02 Preparing data for analysis\nSlideshow is available here: Chapter 02 slides"
  },
  {
    "objectID": "week02/assets/da-background.html#variables",
    "href": "week02/assets/da-background.html#variables",
    "title": "Data Analysis Background",
    "section": "Variables",
    "text": "Variables\n\nVariable types\n\ncontinuous or discrete or qualitative\nbinary\nflow and stock\n\n\n\nEncoding\n\nbinary\nnumeric\nstring (text)\ncategorical / factor (maybe ordinal)\n\n\n\nMeaning of variable values\n\nwhat are they measuring\nunit of measurement\nsource\n\n\n\nOther Information\n\ndescriptive statistics\ncoverage / share of missing values"
  },
  {
    "objectID": "week10/index.html",
    "href": "week10/index.html",
    "title": "Week 10: AI as research companion: Instrumental Variables",
    "section": "",
    "text": "Week 10: AI as research companion: Instrumental Variables\n\n\nGetting closer to causality by deciding trying to find an IV\nIn this course, we discuss how AI can be used to support economics research: help design causal identification. Here we focus on instrumental variables.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 10: AI Research: IV"
    ]
  },
  {
    "objectID": "week10/index.html#recap-10-mins",
    "href": "week10/index.html#recap-10-mins",
    "title": "Week 10: AI as research companion: Instrumental Variables",
    "section": "Recap (10 mins)",
    "text": "Recap (10 mins)\n\nWhat are instrumental variables? What are they good for?\nList examples?\n\n\nIV assumptions\nHint: get AI to explain these 3 assumptions!\n\nRelevance (REL)\n\n\\[\nCov(Z_i, D_i \\mid X_i) \\neq 0\n\\]\n\nExclusion (EX)\n\n\\[\nY_i(D,1) = Y_i(D,0) \\quad \\text{for all} \\quad D = 0,1\n\\]\n\nIndependence (IND)\n\n\\[\n(Y_i(1), Y_i(0), D_i(1), D_i(0)) ⊥ Z_i|X_i\n\\]",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 10: AI Research: IV"
    ]
  },
  {
    "objectID": "week10/index.html#prompt-construction",
    "href": "week10/index.html#prompt-construction",
    "title": "Week 10: AI as research companion: Instrumental Variables",
    "section": "Prompt construction",
    "text": "Prompt construction\nFinding IVs using AI is not just about asking the model for a list. That usually gives you textbook examples with no connection to your actual setting. Instead, we can guide the model through a more structured reasoning process, one that mimics how we would think about instruments ourselves.\n\nPrompt Chaining is a good approach to find possible IVs\n\nPrompt Chaining means breaking down a complex question (like finding an IV) into smaller steps that help the AI reason its way through the problem\nInstead of asking for an instrument directly, we start by mapping out the relationships between variables, then gradually narrow down which ones could work as instruments.\nGoal: 1) avoid generic answers and 2) get the AI to reason more like we would starting from the setting and building up to the econometrics.\n\n\n\nThe 2-step approach\nBefore start, note that the prompting approaches and examples below are adapted from and motivated by Sukjin Han (2024) “Mining Causality: AI-Assisted Search for Instrumental Variables\nWe will use Prompt Chaining to find IVs with two consecutive prompts.\nStep 1: Look for IVs that satisfies the verbal description of REL and EX\n\n🤖 Prompt 1 (Search for IVs). You are [agent] who needs to make a [treatment] decision in [scenario]. What are factors that can determine your decision but do not directly affect your [outcome], except through [treatment] (that is, factors that affect your [outcome] only through [treatment])? list [K 0] factors that are quantifiable. Explain the answers.\n\n\nWhich parts of the prompt satisfy REL and EX?\n\nStep 2: Refine by choosing from the IVs in Step 1 that satisfy a verbal description of IND\n\n🤖 Prompt 2 (Refine IVs). You are [agent] in [scenario], as previously described. Among the [K 0] factors listed above, choose [K] factors that are most likely to be unassociated with [confounders], which determine your [outcome]. the chosen factors can still influence your [treatment]. for each chosen factor, explain your reasoning.\n\n\nWhich parts of the prompt satisfy IND?\n\n\n\nWhy we don’t tell the AI we’re looking for IVs\nIf we say from the start that we’re looking for instrumental variables, the AI often jumps to textbook examples, things like weather shocks or distance to school, without really thinking through the context. These are generic and not always helpful.\n\nWe risk getting shallow suggestions that ignore the specifics of our data or setting.\nSome variables might sound like good instruments but don’t actually satisfy the IV assumptions\nIt also introduces bias from training data: the model might overuse popular examples instead of reasoning from first principles.\n\nSo instead of asking directly for IVs, it’s better to guide the model step-by-step through the structure of the problem.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 10: AI Research: IV"
    ]
  },
  {
    "objectID": "week10/index.html#examples-on-finding-ivs",
    "href": "week10/index.html#examples-on-finding-ivs",
    "title": "Week 10: AI as research companion: Instrumental Variables",
    "section": "Examples on finding IVs",
    "text": "Examples on finding IVs\nLet’s try out the prompts below and review the results together. As we go through them, think about the following:\n\nDoes it seem like the AI believes we’re just asking a regular question, or does it seem to realize we’re testing something deeper?\nAre the IVs it gives us realistic? Which ones make sense, and which ones don’t seem believable?\n\nImportant: Before running the prompts, turn off the option that lets the AI remember your chat history.\n\nExample 1: Demand estimation\n\n🤖 Prompt 1 You are a bar owner at the Copacabana Beach. You need to set the price of draft beer. What would be factors that can determine your decision but that do not directly affect the customers’ demand for draft beer, except through the price you set (that is, that affect the demand only through the price of draft beer).\nList forty factors that are quantifiable. Explain your answer for each factor and present these in a table.\n\n\n🤖 Prompt 2 Suppose you are a bar owner at the Copacabana Beach and is setting the price of draft beer on a day of the week. Among the factors listed above, choose all factors are not influenced by market conditions of draft beer or customers’ characteristics that determine demand for draft beer. For each factor chosen, explain your reasoning.\n\n\n\nExample 2: Peer effects on microfinance adoption\n\n🤖 Prompt 1 You live in a village in rural India. You recently joined a microfinance program and started using small loans to invest in your household or business. You want to influence others in your village to also join the program. What would be factors (about yourself, your household, or the village) that determine your influence on peers, but that do not directly affect your peers’ decisions to join microfinance, except through your influence? That is, factors that affect your peers’ adoption only through your influence.\nList twenty factors that are quantifiable. Explain your answer.\n\n\n🤖 Prompt 2 Suppose you are a 40-year-old female resident of the village using small loans from the microfinance program. Among the factors you listed above, which ones are not influenced by the same factors (e.g., shared background, preferences, or location) that brought you and your peers into the same social network or neighborhood in the first place? For each factor you choose, explain your reasoning.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 10: AI Research: IV"
    ]
  },
  {
    "objectID": "week10/index.html#work-together-find-an-iv-for-a-research-question",
    "href": "week10/index.html#work-together-find-an-iv-for-a-research-question",
    "title": "Week 10: AI as research companion: Instrumental Variables",
    "section": "Work together: Find an IV for a research question",
    "text": "Work together: Find an IV for a research question\n\nForm groups of 3-4.\nWe want to answer the research question: What is the effect of migration on wages in the target country?\nThink about context: which country (from \\(\\rightarrow\\) to), which industry (from \\(\\rightarrow\\) to), which type of people, what period in time, what kind of migration, skill characteristics, etc.\nPrepare Prompt 1 and Prompt 2\n\nRun Prompt 1\nWhich one would you keep? Collect them in a separate table.\nRun Prompt 2\nSee which ones AI kept - are they the same? If not, ask why it didn’t include the ones you did/ why it in included the ones you didn’t.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 10: AI Research: IV"
    ]
  },
  {
    "objectID": "week10/index.html#assignment",
    "href": "week10/index.html#assignment",
    "title": "Week 10: AI as research companion: Instrumental Variables",
    "section": "assignment",
    "text": "assignment",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 10: AI Research: IV"
    ]
  },
  {
    "objectID": "week12/index.html",
    "href": "week12/index.html",
    "title": "Week12 Creating dashboards and online simulation apps",
    "section": "",
    "text": "TBA",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 12: Dashboards and Apps"
    ]
  },
  {
    "objectID": "week12/index.html#part-i-dashboard-to-illustrate-economics",
    "href": "week12/index.html#part-i-dashboard-to-illustrate-economics",
    "title": "Week12 Creating dashboards and online simulation apps",
    "section": "Part I Dashboard to illustrate economics",
    "text": "Part I Dashboard to illustrate economics\nTBA",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 12: Dashboards and Apps"
    ]
  },
  {
    "objectID": "week12/index.html#part-ii-simulation-app-for-a-statistical-property",
    "href": "week12/index.html#part-ii-simulation-app-for-a-statistical-property",
    "title": "Week12 Creating dashboards and online simulation apps",
    "section": "Part II Simulation app for a statistical property",
    "text": "Part II Simulation app for a statistical property\nTBA",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 12: Dashboards and Apps"
    ]
  },
  {
    "objectID": "week07/index.html",
    "href": "week07/index.html",
    "title": "Week 7: Data Collection",
    "section": "",
    "text": "Week 7: Data Collection\nContent coming soon",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 7: Data Collect"
    ]
  },
  {
    "objectID": "week02/index.html",
    "href": "week02/index.html",
    "title": "Week02: Discovery and documentation",
    "section": "",
    "text": "Week02: Discovery and documentation\n\n\nStarting from scratch: Data discovery and documentation with AI",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#summary",
    "href": "week02/index.html#summary",
    "title": "Week02: Discovery and documentation",
    "section": "Summary",
    "text": "Summary\nSometimes data is messy and discovery is hard. Sometimes you need to write data documentation from scratch without deep knowledge of data. LLMs can help. You will learn how to write a clear and professional README. We use the 7th Wave of the World Values Survey (WVS) and rely on AI as a research assistant to bravely face a codebook with hundreds of variables.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#learning-objectives",
    "href": "week02/index.html#learning-objectives",
    "title": "Week02: Discovery and documentation",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\n\nLearn using AI with the context window effectively to understand complex data structure and do search.\nUnderstand how to document a new dataset using as an example th WVS 7th wave data.\nCreate a README that describes data.\nLearn to refine documentation by incorporating iterative feedback from peers and AI tools.\nDevelop skills in using AI to translate complex materials into accessible documentation",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#starting-presentation-about-prompting-and-context-window-management",
    "href": "week02/index.html#starting-presentation-about-prompting-and-context-window-management",
    "title": "Week02: Discovery and documentation",
    "section": "Starting presentation about prompting and context window management",
    "text": "Starting presentation about prompting and context window management\nslideshow",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#assignment-01-graph-reproduction-review",
    "href": "week02/index.html#assignment-01-graph-reproduction-review",
    "title": "Week02: Discovery and documentation",
    "section": "Assignment 01 (Graph reproduction) Review",
    "text": "Assignment 01 (Graph reproduction) Review\n\nFollow instructions.\nHow to get close to original, different ways\nHow was AI assistance helpful?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#preparation",
    "href": "week02/index.html#preparation",
    "title": "Week02: Discovery and documentation",
    "section": "Preparation",
    "text": "Preparation\n\n\n📚 Required Reading\n\n\nmain\n\nData documentation basics: Data Documentation 101 – please check it out for assignment!!!!\n\nBackground reading: \n\nBékés-Kézdi (2021) Chapters 1-3, in particular core background info. Focus on Chapter 2 sections on data structure and variable types - this becomes crucial when documenting data.\nSome discussion of data types Data Management in Large-Scale Education Research by Crystal Lewis\n\n\n\n\n\n📊 Data Setup\n\n\nAccess the World Values Survey case study\n\nDownload data: WVS_random_subset.csv - random subset (N=2000) - covering all countries\nDownload its official codebook documentation\n\nThe full dataset is at OSF, Gabors Data Analysis / World Values Survey\n\n\n::: :::::\n\n\n🤝 Hands-on Documentation Workshop (50 min)\n\n\n\nNo AI\n\nDownload and look at data and codebook\nStart collecting some info on the data without AI\nStart thinking about an interesting research question (find \\(y\\) and \\(x\\))\nWhat would be next step without AI\n\n\n\nUnderstanding wih AI\n\nUpload codebook (pdf) + random subset data (csv)\nUse AI to learn about data and variables\nTest AI’s understanding: “Explain the difference between Q6 and Q7 in simple terms” - this reveals whether AI actually understands the codebook.\nget AI help you refine the question, rethink \\(y\\) and \\(x\\)\n\n\n\nAI: let AI teach you also about data\n\nStart asking for help with documentation.\nCheck how AI understands nuances of encoding\ncheck consistency across variable descriptions\n\nAlways verify technical details, because AI makes some mistakes.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#end-of-week-discussion-points",
    "href": "week02/index.html#end-of-week-discussion-points",
    "title": "Week02: Discovery and documentation",
    "section": "End of Week Discussion points",
    "text": "End of Week Discussion points\nEnd of Week Reflection:\n\nIs there any benefit of looking at it without AI?\nWhat was the biggest contribution of AI?\nFirst result vs after iterations – what did improve?\nHow do you feel about learning from AI vs human instructor? Pros and cons?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#assignment",
    "href": "week02/index.html#assignment",
    "title": "Week02: Discovery and documentation",
    "section": "Assignment",
    "text": "Assignment\n\n\n\n\n\n\nNoteAssignment 2: Creating Documentation\n\n\n\nDue: Before Week 3\nFull Assignment Details",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#some-personal-comments-on-ai-and-this-class",
    "href": "week02/index.html#some-personal-comments-on-ai-and-this-class",
    "title": "Week02: Discovery and documentation",
    "section": "Some personal comments on AI and this class",
    "text": "Some personal comments on AI and this class\n\nWith my Phd student, (Zsuzsi we first developed this material in August 2024. At that time, there were many hiccups in variable understanding and selection. I was gonna suggest careful human oversight. By the time of first teaching it in February 2025, AI got extremely good at reading a 400 page codebook. By 2026 it could work with larger material and more or larger files.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Documentation"
    ]
  },
  {
    "objectID": "week02/assets/chatgpt-2025-vws-README-v1.html",
    "href": "week02/assets/chatgpt-2025-vws-README-v1.html",
    "title": "World Values Survey Data Analysis",
    "section": "",
    "text": "This repository contains data and code for analyzing the World Values Survey (WVS) dataset. The project involves data cleaning, processing, and merging GDP data with survey responses."
  },
  {
    "objectID": "week02/assets/chatgpt-2025-vws-README-v1.html#overview",
    "href": "week02/assets/chatgpt-2025-vws-README-v1.html#overview",
    "title": "World Values Survey Data Analysis",
    "section": "",
    "text": "This repository contains data and code for analyzing the World Values Survey (WVS) dataset. The project involves data cleaning, processing, and merging GDP data with survey responses."
  },
  {
    "objectID": "week02/assets/chatgpt-2025-vws-README-v1.html#repository-structure",
    "href": "week02/assets/chatgpt-2025-vws-README-v1.html#repository-structure",
    "title": "World Values Survey Data Analysis",
    "section": "Repository Structure",
    "text": "Repository Structure\n📂 osf-vwvs/                   # Root directory\n ├── 📂 clean/                 # Processed datasets  \n │    ├── WVS_GDP_merged_data.csv    # Merged WVS data with GDP  \n │    ├── WVS_random_subset.csv      # Random subset of WVS data  \n │    ├── WVS_subset.csv             # Subset of WVS data  \n │  \n ├── 📂 documentation/          # Documentation and metadata  \n │    ├── codebook.pdf          # Variable definitions and coding instructions  \n │  \n ├── 📂 raw/                    # Raw data files  \n │    ├── WVS_Cross-National_Wave_7_csv_v6_0.csv  # Original WVS dataset  \n │  \n ├── 📂 code/                   # Scripts for data processing  \n │    ├── cleaning.R            # Script to clean and process WVS data  \n │  \n ├── README.md                  # Project description and instructions"
  },
  {
    "objectID": "week02/assets/chatgpt-2025-vws-README-v1.html#data-description",
    "href": "week02/assets/chatgpt-2025-vws-README-v1.html#data-description",
    "title": "World Values Survey Data Analysis",
    "section": "Data Description",
    "text": "Data Description\n\nSource: The WVS dataset is publicly available at World Values Survey.\n\nProcessed Data: The clean/ folder contains merged and subsetted datasets used for analysis.\n\nRaw Data: The raw/ folder holds the original dataset in .csv format.\n\nDocumentation: The codebook.pdf explains variable definitions and survey methodology."
  },
  {
    "objectID": "week02/assets/chatgpt-2025-vws-README-v1.html#code-and-reproducibility",
    "href": "week02/assets/chatgpt-2025-vws-README-v1.html#code-and-reproducibility",
    "title": "World Values Survey Data Analysis",
    "section": "Code and Reproducibility",
    "text": "Code and Reproducibility\n\nMain script: code/cleaning.R cleans and processes the raw WVS data.\nRequirements:\n\nR (version X.X.X)\nRequired packages: tidyverse, haven\n\nRunning the code:\n\nPlace the raw data in the raw/ folder.\nRun cleaning.R to generate the processed datasets."
  },
  {
    "objectID": "week02/assets/chatgpt-2025-vws-README-v1.html#license-and-citation",
    "href": "week02/assets/chatgpt-2025-vws-README-v1.html#license-and-citation",
    "title": "World Values Survey Data Analysis",
    "section": "License and Citation",
    "text": "License and Citation\n\nThis project follows the MIT License.\nIf using this data or code, please cite the original WVS dataset."
  },
  {
    "objectID": "week02/assets/chatgpt-2025-vws-README-v1.html#contact",
    "href": "week02/assets/chatgpt-2025-vws-README-v1.html#contact",
    "title": "World Values Survey Data Analysis",
    "section": "Contact",
    "text": "Contact\nFor questions or collaboration, contact: [Your Name] at [Your Email]."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Doing Data Analysis with AI",
    "section": "",
    "text": "Gabor’s Data Analysis with AI"
  },
  {
    "objectID": "index.html#whats-this",
    "href": "index.html#whats-this",
    "title": "Doing Data Analysis with AI",
    "section": "What’s this",
    "text": "What’s this\nThis course will equip students, who are already versed in core data analysis methods, with experience to harness AI technologies to improve productivity (yes this is classic LLM sentence). But, yeah, the idea is to help students who studied data analysis / econometrics / quant methods and want to think about how to include AI in their analytics routine, and spend time to share experiences.\nAs AI becomes more and more powerful, it is also important to provide a platform to discuss human agency in data analysis. So a key element of the course and its instructor to lead discussions on the role of AI and humans in various aspects of data analysis.\nThis is the 2026 Winter edition release\n\nFor whom?\nThis material is aimed at data science and analytics instructors who can guide their students through the material sharing the experience and domain specific examples. All of this is open source, may be modified (see rights below).\nBut, dear reader (students, practitioners, researchers, journalists and more), you can also go alone, checking ideas and practice sessions.\n\n\nAI and me\nAt the end of all classes, instructors and students should always consider these three questions.\n\nHow did AI support me do what I planned.\nHow did AI fail me: gave half-truths, buggy code, imprecise arguments\nHow did AI extend me: helped do things I could not, or gave new ideas"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Doing Data Analysis with AI",
    "section": "Course description",
    "text": "Course description\n\nContent\nThe course focuses on using large language models (LLMs) such as OpenAI’s ChatGPT, Anthropic Claude.ai, Mistral’s Le Chat, and Google’s Gemini) to carry out tasks in data analysis. It includes topics like data extraction and wrangling, data exploration and descriptive statistics, and creating reports as well as turning text to data.\nThere are three case studies that we use (1) a simulated set of data tables on hotels in Austria, (2) The World Value Survey, and (3) A series of interview textst.\nThe course material includes weekly practice assignments. It also has a learning more collection – to see what is beyond core material with blogs, papers, and video recommendations.\n\n\nBackground: data analysis / econometrics\nYou need a background in Data Analysis / Econometrics, a good introductory course is enough. I, of course, suggest Chapters 1-12 and 19 of Data Analysis for Business, Economics and Policy (Cambridge UP, 2021). Full slideshows, data and code are open source. But consider buying the book!🤝\nIn particular, the course builds on Chapters 1-6 and 7-10, and 19 of Data Analysis but other Introductory Econometrics + basics of data science knowledge is ok.\n\n\nBackground: coding\nStudents are expected to have some basic coding knowledge in Python or R (Stata also fine mostly).\n\n\nRelevance\nAI is everywhere and has become essential, most analytic work will be using it. It’s like the Internet a while back. Does not solve all problems, but almost all intellectual tasks will rely on inputs from it.\n\n\nLearning Outcomes\nKey outcomes. By the end of the course, students will be able to\n\nGain experience and confidence using genAI to carry out key tasks in data analysis.\nBuild AI in coding practice including data wrangling, description and reporting and text analysis\nHave some idea of use cases when AI assistance is OK to use as is vs needs strong human supervision\nHave an understanding of resources to follow for updates.\n\n\n\nTarget audience\nThis is a course aimed at 3rd (2nd?) year BA and MA students in any program with required background. Economics, Quantitative Social science, Political Science, Sociology, History. To be frank, all students shall learn data analysis and be comfortable using AI.\nBut, anyone can use it with adequate background.\n\n\nCourse Context & Trajectory\nWhat’s Coming Next:\n\nWeek 0: Using AI for Coding – this may not be part of the curriculum\nWeek 1: LLM Review and short task to recrate a graph\nWeek 2: Use AI to understand and document the World Values Survey dataset\nWeek 3: AI-assisted report writing with WVS data\nWeek 4: Agentic AI with Claude Code using Austrian hotel data\nWeeks 5: Text analysis 1: basics\nWeeks 6: Text analysis 2: sentiment analysis\nWeek 7: TBD\nWeek 8: SQL like natural language query in a data warehouse\nWeek 9: AI as research companion 1: Control variables\nWeek 10: AI as research companion 2: Instrumental Variables\nWeek 11: AI as research companion 3: Difference in differences\nWeek 12: TBD\n\nAI use experience\n\nChat – conversational interface\nData Analysis – direct code execution / shared canvases\nContext window management\nTools to connect to sources (Github, Google drive)\nTalk to AI via API calls\nskills and context management\n\n“my system prompt” (user specific)\nskills use and generation (gems / prompt template): gabors exploratory data analysis skill (sharable)\n\ndeep research\nGemini CLI / Claude Code\nMCP (connect to folders on your computer)\nvibe coding an app: the-human-sql-translator"
  },
  {
    "objectID": "index.html#assignments",
    "href": "index.html#assignments",
    "title": "Doing Data Analysis with AI",
    "section": "Assignments",
    "text": "Assignments\nAssignments are available for all classes\nImportant to note for assignments: * Use AI but do not submit something that was created by AI. AI is your assistant. * One of the goals of the course is to practice this."
  },
  {
    "objectID": "index.html#content-1",
    "href": "index.html#content-1",
    "title": "Doing Data Analysis with AI",
    "section": "Content",
    "text": "Content\nWeek00: AI for coding\nUsing AI for code. May not be covered in this class, as it had often been already covered in coding classes.\nContent\nWeek01: LLM Review\nWhat are LLMs, how is the magic happening. A non-technical brief intro. How to work with LLMs? Plus ideas on applications. Includes suggested readings, podcasts, and vids to listen to.\nContent\nWhich AI?\nSee my take on current models. As of May 2025.\nWeek02: Data and code discovery and documentation with AI\nLearn how to write a clear and professional code and data documentation. LLMs are great help once you know the basics.\nCase study: World Values Survey\nContent\nWeek 03: Writing Reports\nYou have your data and task, and need to write a short report. We compare different options with LLM, from one-shot prompt to iteration.\nCase study: World Values Survey\nContent\nWeek04: Agentic AI with Claude Code\nFrom chat to terminal - introducing Claude Code for data analysis. Students learn to use agentic AI that works directly with files, generates data, and iterates on analysis.\nCase study: Austrian Hotels\nContent\nWeek05: Text as data 1 – intro lecture\nNo course of mine can escape football (soccer). Here we look at post-game interviews to learn basics of text analysis and apply LLMs in what they are best - context dependent learning. Two class series. First is more intro to natural language processing.\nCase study: Football Manager Interviews\nContent\nWeek06: Sentiment Analysis with AI\nSecond class, now we are in action. How does LLM compare to humans?\nCase study: Football Manager Interviews\nContent\nWeek07: TBA\nTBA\nWeek08: SQL like natural language query in a data warehouse\nTBA\nWeek09: AI as research companion: Control variables\nContent\nWeek10: AI as research companion: Instrumental variables\nContent\nWeek11: AI as research companion: Differences in differences\nTBA\nWeek12: Creating simulations with apps and dashboards\nContent\nTBA"
  },
  {
    "objectID": "index.html#learn-more",
    "href": "index.html#learn-more",
    "title": "Doing Data Analysis with AI",
    "section": "Learn more",
    "text": "Learn more\nI’m adding material to da-knowledge folder. You can start with the beyond page."
  },
  {
    "objectID": "index.html#rights-and-acknowledgement",
    "href": "index.html#rights-and-acknowledgement",
    "title": "Doing Data Analysis with AI",
    "section": "Rights and acknowledgement",
    "text": "Rights and acknowledgement"
  },
  {
    "objectID": "index.html#you-can-use-it-to-teach-and-learn-freely",
    "href": "index.html#you-can-use-it-to-teach-and-learn-freely",
    "title": "Doing Data Analysis with AI",
    "section": "You can use it to teach and learn freely",
    "text": "You can use it to teach and learn freely\nAttribution: Békés, Gábor: “Doing Data Analysis with AI: a short course”, available at gabors-data-analysis.com/ai-course/, v2.0. 2026-01-20.\nYou can fork it from the Github Repo. github.com/gabors-data-analysis/da-w-ai/\nLicense: CC BY-NC-SA 4.0 – share, attribute, non-commercial (contact me for corporate gigs)\nTextbook Please check out the textbook behind all this, buy it if you can. If interested teaching contact the Cambridge UP or me."
  },
  {
    "objectID": "index.html#thanks",
    "href": "index.html#thanks",
    "title": "Doing Data Analysis with AI",
    "section": "Thanks",
    "text": "Thanks\nThanks: Developed mostly by me, Gábor Békés Thanks a million to the two wonderful human RAs, Ms Zsuzsanna Vadle and Mr Kenneth Colombe, both Phd students. Thanks to Claude.ai that did a great deal of help in creating the simulated dataset. ChatGPT and Claude.ai helped create the slideshows and educated me on NLP. This is a beatiful example of collaboration with great young people while heavily benefiting from advanced AI.\nThanks for CEU’s teaching grant that allowed me pay people and AI.\nThanks for Quarto – it was all drafted and written in Quarto and Rstudio by Posit."
  },
  {
    "objectID": "index.html#questions-and-suggestions",
    "href": "index.html#questions-and-suggestions",
    "title": "Doing Data Analysis with AI",
    "section": "Questions and suggestions",
    "text": "Questions and suggestions\nThis material is based my course at CEU in Vienna, Austria. Here is the Github repo\nIf you have questions or suggestions or interested to learn more, just fill in this form."
  },
  {
    "objectID": "index.html#and-now-this.",
    "href": "index.html#and-now-this.",
    "title": "Doing Data Analysis with AI",
    "section": "And now, this.",
    "text": "And now, this.\nAI use is very costly in terms of energy. Yes, it is becoming cheaper. But humanity is also using much more of it."
  },
  {
    "objectID": "week05/assets/sentiment-guidelines.html",
    "href": "week05/assets/sentiment-guidelines.html",
    "title": "Guidelines for Rating:",
    "section": "",
    "text": "Positive mentions include praise, satisfaction with performance, optimistic outlook, and appreciation.\nNegative mentions include frustration, disappointment, criticism of performance, or external conditions.\nNeutral statements are neither positive nor negative.\n\n\nExamples:\n\n“I’m extremely proud of how the team played today. Fantastic performance by everyone.” → +2\n“We played well, but there are still areas to improve.” → +1\n“It was a tough match, evenly balanced, nothing much to say.” → 0\n“We weren’t at our best; it was a frustrating game.” → -1\n“I’m very disappointed. Our performance was unacceptable.” → -2\n\n\n\nFinal Notes:\n\nUse 0 if unsure or if sentiment is mixed without clear dominance."
  },
  {
    "objectID": "case-studies/interviews/creating-code-sentiment-analysis.html",
    "href": "case-studies/interviews/creating-code-sentiment-analysis.html",
    "title": "creating-code-sentiment-analysis",
    "section": "",
    "text": "Here are some prompts we created. Follow them. Need debugging.\nFirst download interview-texts-only.xlsx from interviews case study\n\nWrite a Python script for sentiment analysis of post-match manager quotes using OpenAI’s GPT API. Follow these detailed specifications:\n\n\nClassify each quote on a sentiment scale from -2 to 2 based on the manager’s tone, not the game result.\nThe script should process the xlsx file and produce a new CSV with sentiment scores.\n\n\n\nLoad interview-texts-only.xlsx .\nUse utf-8 encoding.\nSkip malformed lines when reading.\n\n\n\nUse OpenAI’s gpt-4.1-2025-04-14 model (or fallback-compatible GPT-4 model).\nConstruct a detailed prompt that:"
  },
  {
    "objectID": "case-studies/interviews/creating-code-sentiment-analysis.html#writing-code-to-run-sentiment-analysis-via-apis.",
    "href": "case-studies/interviews/creating-code-sentiment-analysis.html#writing-code-to-run-sentiment-analysis-via-apis.",
    "title": "creating-code-sentiment-analysis",
    "section": "",
    "text": "Here are some prompts we created. Follow them. Need debugging.\nFirst download interview-texts-only.xlsx from interviews case study\n\nWrite a Python script for sentiment analysis of post-match manager quotes using OpenAI’s GPT API. Follow these detailed specifications:\n\n\nClassify each quote on a sentiment scale from -2 to 2 based on the manager’s tone, not the game result.\nThe script should process the xlsx file and produce a new CSV with sentiment scores.\n\n\n\nLoad interview-texts-only.xlsx .\nUse utf-8 encoding.\nSkip malformed lines when reading.\n\n\n\nUse OpenAI’s gpt-4.1-2025-04-14 model (or fallback-compatible GPT-4 model).\nConstruct a detailed prompt that:"
  },
  {
    "objectID": "case-studies/interviews/creating-code-sentiment-analysis.html#task",
    "href": "case-studies/interviews/creating-code-sentiment-analysis.html#task",
    "title": "creating-code-sentiment-analysis",
    "section": "Task:",
    "text": "Task:\nPlease read each text carefully and rate the overall sentiment of the manager’s statement as positive or negative. Your rating should reflect the manager’s expressed tone, not your judgment of the match."
  },
  {
    "objectID": "case-studies/interviews/creating-code-sentiment-analysis.html#rating-scale",
    "href": "case-studies/interviews/creating-code-sentiment-analysis.html#rating-scale",
    "title": "creating-code-sentiment-analysis",
    "section": "Rating Scale:",
    "text": "Rating Scale:\n\n\n\n\n\n\n\nScore\nMeaning\n\n\n\n\n2\nStrongly positive sentiment (clear optimism, satisfaction, praise).\n\n\n1\nMildly positive sentiment (generally positive, slight reservations).\n\n\n0\nNeutral or unclear sentiment.\n\n\n-1\nMildly negative sentiment (general disappointment, frustration).\n\n\n-2\nStrongly negative sentiment (clear criticism, significant disappointment).\n\n\n\n\nFinal Notes:\n\nUse 0 if unsure or if sentiment is mixed without clear dominance.\n\n\n\nRetry up to 3 times if API call fails, using exponential backoff (e.g., wait 1s, 2s, 4s).\n\nIf the response is not an integer, return None.\n\n\n🔐 API Authentication\n\n\nUse the openai Python package (v1+).\n\nLoad API key securely from .env file with key OPENAI_API_KEY.\n\nIf the key is missing, exit with an error.\n\n\n🧪 Processing & Output\n\n\nIterate through all rows using tqdm for progress.\n\nFor each row, call the GPT model and collect the result.\n\nSave results (text_id, score) to manager_sentiment_results.csv.\n\nAt end, print and log score frequency table.\n\n\n🪵 Logging\n\n\nUse Python’s logging module.\n\nConfigure it to:\n\n\nLog to both console (stdout) and to file classification.log (overwrite mode).\n\nUse format: [timestamp] [level] [message]\n\nSet level to INFO.\n\n\n\nLog key events:\n\n\nStart of API call\n\nAPI responses\n\nNumber of rows processed\n\nMissing columns\n\nFinal distribution of scores\n\n\n\n\n📦 Dependencies\n\n\npandas\n\ntqdm\n\nopenai\n\ndotenv\n\n\n🔁 Extras\n\n\nUse functions for:\n\n\nLoading CSV,\n\nClassifying text,\n\nError handling.\n\n\n\nEnsure script can be run directly with #!/usr/bin/env python3 at the top.\n\n\n\nLet me know if you want me to generate the exact prompt message string or turn this into a README section."
  },
  {
    "objectID": "case-studies/interviews/data/index.html",
    "href": "case-studies/interviews/data/index.html",
    "title": "Football Manager Interviews - Data Files",
    "section": "",
    "text": "← Back to Interviews Case Study"
  },
  {
    "objectID": "case-studies/interviews/data/index.html#data-files",
    "href": "case-studies/interviews/data/index.html#data-files",
    "title": "Football Manager Interviews - Data Files",
    "section": "Data Files",
    "text": "Data Files\n\nPrimary Data\n\n\n\nFile\nDescription\n\n\n\n\ninterview-texts-only.xlsx\nRaw interview texts (121 interviews)\n\n\ncombined_text_data.csv\nCombined text data with metadata\n\n\ncombined_raw_data.csv\nFull raw dataset\n\n\ngame_data.xlsx\nGame results and context\n\n\n\n\n\nSentiment Analysis\n\n\n\nFile\nDescription\n\n\n\n\ndomain_lexicon.csv\nFootball-specific sentiment terms\n\n\naggregated_ratings_by_text.csv\nAggregated sentiment ratings"
  },
  {
    "objectID": "case-studies/employee_commits/index.html",
    "href": "case-studies/employee_commits/index.html",
    "title": "Employee Commits Dataset",
    "section": "",
    "text": "Simulated dataset of employee commit activity for exploring AI-generated data analysis and visualization comparisons.\nUsed in: Data analysis demonstrations",
    "crumbs": [
      "Home",
      "Case Studies",
      "Employee Commits"
    ]
  },
  {
    "objectID": "case-studies/employee_commits/index.html#overview",
    "href": "case-studies/employee_commits/index.html#overview",
    "title": "Employee Commits Dataset",
    "section": "",
    "text": "Simulated dataset of employee commit activity for exploring AI-generated data analysis and visualization comparisons.\nUsed in: Data analysis demonstrations",
    "crumbs": [
      "Home",
      "Case Studies",
      "Employee Commits"
    ]
  },
  {
    "objectID": "case-studies/employee_commits/index.html#data-files",
    "href": "case-studies/employee_commits/index.html#data-files",
    "title": "Employee Commits Dataset",
    "section": "Data Files",
    "text": "Data Files\n\n\n\nFile\nDescription\n\n\n\n\nemployee_commits_clean.csv\nCleaned commit data\n\n\nemployee_commits_raw.csv\nRaw commit data\n\n\nREADME.txt\nData description\n\n\n\n\nAI Analysis Outputs\nThe ai/ subfolder contains analysis outputs from different AI models:\n\nChatGPT analysis outputs\nClaude analysis outputs\nGemini analysis outputs\n\nBrowse data files",
    "crumbs": [
      "Home",
      "Case Studies",
      "Employee Commits"
    ]
  },
  {
    "objectID": "case-studies/employee_commits/index.html#code",
    "href": "case-studies/employee_commits/index.html#code",
    "title": "Employee Commits Dataset",
    "section": "Code",
    "text": "Code\nCode examples to be added\nBrowse code files",
    "crumbs": [
      "Home",
      "Case Studies",
      "Employee Commits"
    ]
  },
  {
    "objectID": "case-studies/employee_commits/index.html#key-features",
    "href": "case-studies/employee_commits/index.html#key-features",
    "title": "Employee Commits Dataset",
    "section": "Key Features",
    "text": "Key Features\n\nComparison of AI-generated analyses\nMultiple model outputs for same dataset\nVisualization comparisons",
    "crumbs": [
      "Home",
      "Case Studies",
      "Employee Commits"
    ]
  },
  {
    "objectID": "case-studies/employee_commits/index.html#downloads",
    "href": "case-studies/employee_commits/index.html#downloads",
    "title": "Employee Commits Dataset",
    "section": "Downloads",
    "text": "Downloads\n\nemployee_commits_clean.csv\nemployee_commits_raw.csv",
    "crumbs": [
      "Home",
      "Case Studies",
      "Employee Commits"
    ]
  },
  {
    "objectID": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html",
    "href": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#whats-new-in-v3.0---key-improvements",
    "href": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#whats-new-in-v3.0---key-improvements",
    "title": "",
    "section": "What’s New in v3.0 - KEY IMPROVEMENTS",
    "text": "What’s New in v3.0 - KEY IMPROVEMENTS\n\n1. ✅ Rising Productivity in First 2 Years (Ramp-Up)\n\nEarly career (0-2yr): Mean = 9.7 commits/month\nProductivity increases from ~30% to 100% over first 730 days\nPattern: People are learning, ramping up skills\n\n\n\n2. ✅ Smooth Tenure Distribution (No Cliff!)\n\nExponential distribution: Most people have low tenure\nNatural decline: 148 employees &lt;1yr, 112 at 1-2yr, 167 at 2-5yr, 115 at 5+yr\nPattern: Realistic employee turnover and retention\n\n\n\n3. ✅ Marked Decline After 7-8 Years\n\nLate career (7+yr): Mean = 8.7 commits/month (lower than early career!)\nZero commits: 19.8% of employees with 7+ years (vs 5% baseline)\nPattern: Move to management, mentoring, architecture roles"
  },
  {
    "objectID": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#the-x-y-relationship-now-much-clearer",
    "href": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#the-x-y-relationship-now-much-clearer",
    "title": "",
    "section": "The X-Y Relationship: Now MUCH Clearer!",
    "text": "The X-Y Relationship: Now MUCH Clearer!\n\nOverall Statistics (Clean Data, N=542)\nCorrelations: - Pearson (linear): r = 0.019 (still weak in linear sense) - Spearman (rank): r = 0.190 (p &lt; 0.0001, significant!)\nKey Insight: The relationship is non-linear and non-monotonic!\n\n\nThe Career Progression Curve\n\n\n\n\n\n\n\n\n\n\n\nCareer Stage\nTenure\nMean\nMedian\nN\nPattern\n\n\n\n\nEarly (Learning)\n0-1 year\n7.4\n5.5\n148\nLow (ramping up)\n\n\nGrowing (Maturing)\n1-2 years\n13.0\n10.5\n112\nRising (learning curve)\n\n\nPeak (Full productivity)\n2-5 years\n32.0\n24.0\n167\nMaximum (IC contributors)\n\n\nLate (Leadership)\n5+ years\n10.3\n7.0\n115\nDeclining (management)\n\n\n\nPeak is 4.3x higher than early career!\nLate career drops back to early levels!\n\n\nStatistical Significance\nANOVA for Seniority Effect: - F-statistic: 93.66 - p-value: &lt; 0.0001 - Result: Highly significant differences across career stages\nThe evidence is overwhelming that career stage matters!"
  },
  {
    "objectID": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#visualizations-show-clear-patterns",
    "href": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#visualizations-show-clear-patterns",
    "title": "",
    "section": "Visualizations Show Clear Patterns",
    "text": "Visualizations Show Clear Patterns\n\nPanel 1: Tenure Distribution (Top Left)\n\nSmooth exponential decline (no cliff at 2000 days)\nMost employees concentrated in 0-2000 day range\nNatural thinning as tenure increases\n\n\n\nPanel 2: Career Progression Curve (Top Right)\nRED MEDIAN TREND LINE shows: 1. Rise from 0 to ~1000 days (0-3 years) 2. Peak around 1000-2000 days (3-5 years)\n3. Decline after 2500 days (7+ years) 4. Low plateau after 3500+ days (10+ years)\nThis is the inverted-U pattern that students should discover!\n\n\nPanel 3: Productivity by Career Stage (Bottom Left)\nBoxplots dramatically show: - Early: Median ~5, tight distribution - Peak: Median ~24, wide distribution (high variance) - Late: Median ~7, back down to early levels\n\n\nPanel 4: Zero Commits Rate (Bottom Right)\nThe “management transition” effect: - Stable ~5-15% for first 2500 days - Spike to 30-50% after 2500 days (7 years) - Shows clear transition from coding to leadership"
  },
  {
    "objectID": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#why-this-is-the-best-teaching-dataset",
    "href": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#why-this-is-the-best-teaching-dataset",
    "title": "",
    "section": "Why This is the BEST Teaching Dataset",
    "text": "Why This is the BEST Teaching Dataset\n\n1. Realistic Career Arc\nThis pattern matches real-world phenomena: - Academic research output (inverted-U with age) - Sports performance (peak then decline) - Software engineering (senior devs code less) - Management transitions (IC → people leadership)\n\n\n2. Multiple Valid Analysis Approaches\nStudents can explore:\nLinear approaches: - Simple OLS: R² ≈ 0 (fails completely) - With group dummies: R² ≈ 0.25 (much better!)\nNon-linear approaches: - Polynomial regression (quadratic or cubic) - Splines (natural or smoothing splines) - GAM (generalized additive models) - Locally weighted regression (LOESS)\nCount data approaches: - Poisson regression - Negative binomial (handles overdispersion) - Zero-inflated models (for late-career zeros)\nGroup-based approaches: - ANOVA / linear model with tenure groups - Separate models by career stage - Mixed effects with random slopes\n\n\n3. Forces Critical Thinking\nQuestions students must grapple with: - Why does simple linear regression fail? - How do we model non-monotonic relationships? - When to use continuous vs categorical predictors? - How to interpret “no linear relationship” vs “no relationship”?\n\n\n4. Data Quality Challenges\nStill includes 58 problematic cases (8 errors + 50 extreme values): - Students must clean data first - Different cleaning decisions affect results - Teaches importance of documentation"
  },
  {
    "objectID": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#key-statistics-summary",
    "href": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#key-statistics-summary",
    "title": "",
    "section": "Key Statistics Summary",
    "text": "Key Statistics Summary\n\nTenure Distribution (Clean Data)\n\nMean: 1,162 days (3.2 years)\nMedian: 788 days (2.2 years)\nRange: 1 to 5,475 days (0 to 15 years)\nMost employees: 0-2000 days (70%)\n\n\n\nCommits Distribution (Clean Data)\n\nMean: 16.8 commits/month\nMedian: 10.0 commits/month (right-skewed!)\nRange: 0 to 119 commits/month\nZero commits: 50 cases (9.2%)\n\n\n\nBy Department (Clean Data)\n\nIT: Mean = 18.0, Median = 12.0 (n=307)\nAnalytics: Mean = 15.1, Median = 9.0 (n=235)\nDifference: Not statistically significant (p=0.09)\n\n\n\nData Quality Issues\n\n8 errors (must fix): negative values, impossibly high/long\n50 extreme values (analyst judgment): very high commits, zeros with long tenure\n542 clean observations (90.3%)"
  },
  {
    "objectID": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#files-provided-v3.0",
    "href": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#files-provided-v3.0",
    "title": "",
    "section": "Files Provided (v3.0)",
    "text": "Files Provided (v3.0)\n\nMain Data Files\n\nemployee_commits_claude_v3.csv (600 rows)\n\nFull dataset with metadata\nColumns: i, x, y, department, seniority, role, data_issue, issue_category\nUse for: Teaching with answer key\n\nemployee_commits_raw_v3.csv (600 rows)\n\nMinimal dataset (i, x, y, department, role only)\nUse for: Give to students first (discovery learning)\n\nemployee_commits_clean_v3.csv (542 rows)\n\nOnly clean observations\nUse for: Analysis after cleaning or skip cleaning exercise\n\ndata_quality_report_v3.csv (58 rows)\n\nList of all problematic cases\nUse for: Answer key for data cleaning\n\n\n\n\nVisualization Files\n\nemployee_commits_v3_plots.png\n\nShows DGP features: ramp-up, peak, decline, zeros\n4 panels demonstrating career progression\n\nxy_relationship_analysis_v3.png\n\nComprehensive 8-panel analysis\nShows relationship from multiple angles\nIncludes residual diagnostics\n\n\n\n\nCode Files\n\ngenerate_data_v3.py\n\nComplete data generation code\nFully commented and reproducible\nChange seed or parameters as needed\n\nanalyze_xy_relationship_v3.py\n\nRelationship analysis script\nCreates 8-panel visualization\nStatistical tests included"
  },
  {
    "objectID": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#teaching-workflow-recommended",
    "href": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#teaching-workflow-recommended",
    "title": "",
    "section": "Teaching Workflow (Recommended)",
    "text": "Teaching Workflow (Recommended)\n\nStage 1: Discovery (Raw Data)\n\nGive students employee_commits_raw_v3.csv\nAsk: “What is the relationship between x and y?”\nLet them explore, visualize, analyze\nThey should discover the non-linear pattern!\n\n\n\nStage 2: Data Quality\n\nAsk: “Are there any data quality issues?”\nStudents identify errors and extreme values\nMake and document cleaning decisions\nCompare with data_quality_report_v3.csv\n\n\n\nStage 3: Multiple Models\n\nTry various modeling approaches\nCompare: OLS, polynomials, splines, groups, etc.\nEvaluate which model best captures the pattern\nDiscuss trade-offs (interpretability vs fit)\n\n\n\nStage 4: Interpretation\n\nWhat is the relationship between tenure and commits?\nWhy is it non-monotonic?\nWhat does this tell us about career progression?\nHow would you communicate findings to stakeholders?"
  },
  {
    "objectID": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#key-teaching-points",
    "href": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#key-teaching-points",
    "title": "",
    "section": "Key Teaching Points",
    "text": "Key Teaching Points\n\n1. Linear Models Can Miss Important Patterns\n\nr ≈ 0 doesn’t mean “no relationship”\nIt means “no linear relationship”\nAlways visualize first!\n\n\n\n2. Non-Monotonic Relationships Are Common\n\nNot everything increases or decreases monotonically\nCareer arcs, learning curves, life cycles all have peaks\nNeed appropriate modeling approaches\n\n\n\n3. Group-Based Analysis Often Better\n\nCategorical predictors (seniority) explain more than continuous (days)\nDomain knowledge helps create meaningful groups\nSometimes simpler is better (interpretability)\n\n\n\n4. Count Data Has Special Properties\n\nDiscrete, non-negative, right-skewed\nPoisson/NB often better than OLS\nZero-inflation is a real phenomenon\n\n\n\n5. Context Matters\n\nThe inverted-U makes perfect sense for careers\nEarly: learning and ramping up\nPeak: full IC productivity\n\nLate: transitioning to leadership\nDomain knowledge guides modeling choices"
  },
  {
    "objectID": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#comparison-v2.1-vs-v3.0",
    "href": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#comparison-v2.1-vs-v3.0",
    "title": "",
    "section": "Comparison: v2.1 vs v3.0",
    "text": "Comparison: v2.1 vs v3.0\n\n\n\n\n\n\n\n\n\nFeature\nv2.1\nv3.0\nImpact\n\n\n\n\nEarly career\nFlat/low\nRising\nMore realistic learning curve\n\n\nTenure distribution\nHard cutoffs\nSmooth exponential\nNo artificial cliff\n\n\nLate career\nModerate decline\nMarked decline + zeros\nClear management transition\n\n\nX-Y correlation\n-0.005\n0.019\nSlightly positive (better)\n\n\nSpearman correlation\n0.120\n0.190\nStronger non-linear signal\n\n\nSeniority F-stat\n35.5\n93.7\nMuch stronger group effects\n\n\nPedagogical clarity\nGood\nExcellent\nPattern unmistakable\n\n\n\nv3.0 is the recommended version for teaching!"
  },
  {
    "objectID": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#extensions-and-variations",
    "href": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#extensions-and-variations",
    "title": "",
    "section": "Extensions and Variations",
    "text": "Extensions and Variations\n\nEasy Modifications (change in generate_data_v3.py):\n\nDifferent ramp-up speed: Change x/730 factor\nEarlier/later peak: Adjust 2500 day threshold\nSteeper decline: Increase 0.12 decline factor\nMore zeros: Increase 0.07 probability increment\nDifferent roles: Modify base_monthly_rates\nMore departments: Add to dept_split\n\n\n\nAdvanced Extensions:\n\nTime series: Multiple observations per employee\nTeam effects: Add team ID with random effects\nProject complexity: Add covariate affecting commits\nTurnover: Some employees leave (censoring)\nPromotions: Explicit seniority changes over time"
  },
  {
    "objectID": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#technical-details",
    "href": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#technical-details",
    "title": "",
    "section": "Technical Details",
    "text": "Technical Details\n\nDGP Specifications (v3.0)\nTenure Distribution:\ndays_with_company ~ Exponential(mean=1200)\nTruncated: [1, 5475] days\nProductivity Function:\nramp_factor = {\n  0.3 + 0.7*(x/730)           if x &lt; 730\n  1.0                          if 730 ≤ x &lt; 2500\n  max(0.25, 1.0 - 0.12*years)  if x ≥ 2500\n}\n\nlambda = base_rate × seniority_mult × ramp_factor × individual_effect\ny ~ NegativeBinomial(mu=lambda, size=5)\nZero Inflation:\nP(y=0) = {\n  0.05                           if x &lt; 2500\n  min(0.35, 0.05 + 0.07*years)  if x ≥ 2500\n}\n\n\nRandom Seed\nnp.random.seed(20251112) - fully reproducible\n\n\nSoftware Requirements\n\nPython 3.x\nnumpy, pandas, matplotlib, scipy, sklearn"
  },
  {
    "objectID": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#bottom-line",
    "href": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#bottom-line",
    "title": "",
    "section": "Bottom Line",
    "text": "Bottom Line\nv3.0 creates a realistic dataset that:\n✅ Shows clear non-monotonic career progression\n✅ Challenges students’ intuitions about linearity\n✅ Requires sophisticated thinking about modeling\n✅ Reflects real-world career dynamics\n✅ Provides rich opportunities for exploration\n✅ Has no single “right” answer (by design)\n✅ Teaches both technical skills and critical thinking\nPerfect for teaching “What is the relationship between x and y?”"
  },
  {
    "objectID": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#quick-start",
    "href": "case-studies/employee_commits/data/background-v3/COMPLETE_SUMMARY_v3.html#quick-start",
    "title": "",
    "section": "Quick Start",
    "text": "Quick Start\n\nGive students: employee_commits_raw_v3.csv\nAsk: “What is the relationship between tenure (x) and monthly commits (y)?”\nLet them explore and struggle (this is where learning happens!)\nReveal patterns using: employee_commits_v3_plots.png and xy_relationship_analysis_v3.png\nDiscuss: Why did simple approaches fail? What models work better?\n\nWelcome to the wonderful world of non-linear relationships!"
  },
  {
    "objectID": "case-studies/employee_commits/code/index.html",
    "href": "case-studies/employee_commits/code/index.html",
    "title": "Employee Commits - Code",
    "section": "",
    "text": "← Back to Employee Commits Case Study"
  },
  {
    "objectID": "case-studies/employee_commits/code/index.html#code-files",
    "href": "case-studies/employee_commits/code/index.html#code-files",
    "title": "Employee Commits - Code",
    "section": "Code Files",
    "text": "Code Files\nCode examples to be added."
  },
  {
    "objectID": "case-studies/common-support-r/index.html",
    "href": "case-studies/common-support-r/index.html",
    "title": "Common Support Analysis (R)",
    "section": "",
    "text": "Iterative development of R code for common support analysis using Claude AI, demonstrating the prompt-refine workflow for building statistical analysis code.\nTopic: Causal inference and common support in treatment effect estimation",
    "crumbs": [
      "Home",
      "Case Studies",
      "Common Support (R)"
    ]
  },
  {
    "objectID": "case-studies/common-support-r/index.html#overview",
    "href": "case-studies/common-support-r/index.html#overview",
    "title": "Common Support Analysis (R)",
    "section": "",
    "text": "Iterative development of R code for common support analysis using Claude AI, demonstrating the prompt-refine workflow for building statistical analysis code.\nTopic: Causal inference and common support in treatment effect estimation",
    "crumbs": [
      "Home",
      "Case Studies",
      "Common Support (R)"
    ]
  },
  {
    "objectID": "case-studies/common-support-r/index.html#code-files",
    "href": "case-studies/common-support-r/index.html#code-files",
    "title": "Common Support Analysis (R)",
    "section": "Code Files",
    "text": "Code Files\nSequential prompts showing iterative development:\n\n\n\nFile\nDescription\n\n\n\n\nclaude-sonnet-v4-prompt-01.R\nInitial prompt and response\n\n\nclaude-sonnet-v4-prompt-02.R\nFirst refinement\n\n\nclaude-sonnet-v4-prompt-03.R\nAdding features\n\n\nclaude-sonnet-v4-prompt-04.R\nFurther improvements\n\n\nclaude-sonnet-v4-prompt-05.R\nExtended analysis\n\n\nclaude-sonnet-v4-prompt-06.R\nVisualization updates\n\n\nclaude-sonnet-v4-prompt-07.R\nAdditional refinements\n\n\nclaude-sonnet-v4-prompt-08.R\nNear-final version\n\n\nclaude-sonnet-v4-prompt-09.R\nFinal polished version\n\n\n\nBrowse code files",
    "crumbs": [
      "Home",
      "Case Studies",
      "Common Support (R)"
    ]
  },
  {
    "objectID": "case-studies/common-support-r/index.html#data",
    "href": "case-studies/common-support-r/index.html#data",
    "title": "Common Support Analysis (R)",
    "section": "Data",
    "text": "Data\nUses simulated data generated within scripts\nBrowse data files",
    "crumbs": [
      "Home",
      "Case Studies",
      "Common Support (R)"
    ]
  },
  {
    "objectID": "case-studies/common-support-r/index.html#key-features",
    "href": "case-studies/common-support-r/index.html#key-features",
    "title": "Common Support Analysis (R)",
    "section": "Key Features",
    "text": "Key Features\n\nDemonstrates iterative AI-assisted coding workflow\nShows progression from basic to sophisticated analysis\nReal example of prompt engineering for statistical code\nCommon support visualization techniques",
    "crumbs": [
      "Home",
      "Case Studies",
      "Common Support (R)"
    ]
  },
  {
    "objectID": "case-studies/common-support-r/index.html#learning-objectives",
    "href": "case-studies/common-support-r/index.html#learning-objectives",
    "title": "Common Support Analysis (R)",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nHow to iteratively develop code with AI\nPrompt refinement strategies\nBuilding complex statistical analyses step-by-step",
    "crumbs": [
      "Home",
      "Case Studies",
      "Common Support (R)"
    ]
  },
  {
    "objectID": "case-studies/common-support-r/code/index.html",
    "href": "case-studies/common-support-r/code/index.html",
    "title": "Common Support Analysis - Code",
    "section": "",
    "text": "← Back to Common Support Case Study"
  },
  {
    "objectID": "case-studies/common-support-r/code/index.html#code-files",
    "href": "case-studies/common-support-r/code/index.html#code-files",
    "title": "Common Support Analysis - Code",
    "section": "Code Files",
    "text": "Code Files\nSequential prompts showing iterative development with Claude AI:\n\n\n\nFile\nDescription\n\n\n\n\nclaude-sonnet-v4-prompt-01.R\nInitial prompt and response\n\n\nclaude-sonnet-v4-prompt-02.R\nFirst refinement\n\n\nclaude-sonnet-v4-prompt-03.R\nAdding features\n\n\nclaude-sonnet-v4-prompt-04.R\nFurther improvements\n\n\nclaude-sonnet-v4-prompt-05.R\nExtended analysis\n\n\nclaude-sonnet-v4-prompt-06.R\nVisualization updates\n\n\nclaude-sonnet-v4-prompt-07.R\nAdditional refinements\n\n\nclaude-sonnet-v4-prompt-08.R\nNear-final version\n\n\nclaude-sonnet-v4-prompt-09.R\nFinal polished version"
  },
  {
    "objectID": "case-studies/common-support-r/code/index.html#learning-points",
    "href": "case-studies/common-support-r/code/index.html#learning-points",
    "title": "Common Support Analysis - Code",
    "section": "Learning Points",
    "text": "Learning Points\nThis series demonstrates: - How to iteratively develop code with AI assistance - Prompt refinement strategies - Building complex statistical analyses step-by-step"
  },
  {
    "objectID": "case-studies/austria-hotels/data/hotel-data-readme.html",
    "href": "case-studies/austria-hotels/data/hotel-data-readme.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "case-studies/austria-hotels/data/hotel-data-readme.html#info",
    "href": "case-studies/austria-hotels/data/hotel-data-readme.html#info",
    "title": "",
    "section": "Info",
    "text": "Info\nThis dataset contains realistic data on hotels across Austria.\n\nThis dataset was generated programmatically with the generate_austrian_hotels_data.R script to ensure realistic relationships between variables while maintaining privacy.\n\nThe scipt was writen by Claude AI, Sonnet 3.7, 2025-03-15, and reviwed and approved by Gabor 2025-03-17\n\nThe dataset consists of multiple related tables that can be combined.\nThe data patterns are based on typical hotel industry metrics but do not represent actual hotels."
  },
  {
    "objectID": "case-studies/austria-hotels/data/hotel-data-readme.html#dataset-overview",
    "href": "case-studies/austria-hotels/data/hotel-data-readme.html#dataset-overview",
    "title": "",
    "section": "Dataset Overview",
    "text": "Dataset Overview\nThe dataset includes hotels across Austrian cities with data on occupancy, pricing, tourism statistics, and economic indicators.\n\nFiles\nAll files are located in the data/raw/ directory:\n\n\n\n\n\n\n\n\n\nFile\nDescription\nRows\nKey Columns\n\n\n\n\nhotels.csv\nBasic hotel information\n200\nhotel_id (PK)\n\n\ncities.csv\nCity information\n10\ncity (PK)\n\n\nmonthly_occupancy.csv\nMonthly hotel performance metrics\n~3,800\nhotel_id, month, year\n\n\ncity_tourism.csv\nMonthly tourism statistics by city\n240\ncity, month, year\n\n\neconomic_indicators.csv\nMonthly economic indicators\n24\nmonth, year\n\n\nreviews.csv\nHotel guest reviews\n~1,700\nreview_id (PK), hotel_id (FK)\n\n\namenities.csv\nList of possible hotel amenities\n10\namenity_id (PK)\n\n\nhotel_amenities.csv\nHotel-amenity relationships\n~1,000\nhotel_id, amenity_id\n\n\n\n## Schema Details\n### hotels.csv Information about individual hotels.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nhotel_id\ninteger\nPrimary key\n\n\nhotel_name\ncharacter\nHotel name\n\n\ncity\ncharacter\nCity where hotel is located\n\n\nstar_rating\ninteger\nHotel quality rating (3-5 stars)\n\n\nrooms\ninteger\nNumber of rooms in the hotel\n\n\nyear_built\ninteger\nYear the hotel was built\n\n\n\n### cities.csv Information about Austrian cities.\n\n\n\nColumn\nType\nDescription\n\n\n\n\ncity\ncharacter\nCity name (primary key)\n\n\nprovince\ncharacter\nAustrian province\n\n\npopulation\ninteger\nCity population\n\n\ntourism_rank\ninteger\nTourism popularity rank (1 = highest)\n\n\n\n### monthly_occupancy.csv Monthly hotel performance metrics.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nhotel_id\ninteger\nForeign key to hotels.csv\n\n\nmonth\ninteger\nMonth (1-12)\n\n\nyear\ninteger\nYear (2023-2024)\n\n\noccupancy_rate\nnumeric\nPercentage of rooms occupied (0.0-1.0)\n\n\navg_daily_rate\nnumeric\nAverage price per night in EUR\n\n\nrevenue_per_room\nnumeric\nRevenue per available room (RevPAR)\n\n\n\n### city_tourism.csv Monthly tourism statistics for each city.\n\n\n\nColumn\nType\nDescription\n\n\n\n\ncity\ncharacter\nCity name\n\n\nmonth\ninteger\nMonth (1-12)\n\n\nyear\ninteger\nYear (2023-2024)\n\n\ntourist_arrivals\ninteger\nNumber of tourists arriving\n\n\nevent_days\ninteger\nNumber of event days in the month\n\n\navg_stay_length\nnumeric\nAverage length of stay in days\n\n\n\n### economic_indicators.csv Monthly economic indicators for Austria.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nmonth\ninteger\nMonth (1-12)\n\n\nyear\ninteger\nYear (2023-2024)\n\n\ninflation_rate\nnumeric\nMonthly inflation rate (decimal)\n\n\nunemployment\nnumeric\nUnemployment rate (decimal)\n\n\nconsumer_confidence\nnumeric\nConsumer confidence index\n\n\n\n### reviews.csv Hotel guest reviews.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nreview_id\ninteger\nPrimary key\n\n\nhotel_id\ninteger\nForeign key to hotels.csv\n\n\nrating\nnumeric\nRating (1.0-5.0)\n\n\nreview_date\ndate\nDate of the review\n\n\n\n### amenities.csv List of possible hotel amenities.\n\n\n\nColumn\nType\nDescription\n\n\n\n\namenity_id\ninteger\nPrimary key\n\n\namenity_name\ncharacter\nName of the amenity\n\n\n\n### hotel_amenities.csv Many-to-many relationship between hotels and amenities.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nhotel_id\ninteger\nForeign key to hotels.csv\n\n\namenity_id\ninteger\nForeign key to amenities.csv"
  },
  {
    "objectID": "case-studies/austria-hotels/code/index.html",
    "href": "case-studies/austria-hotels/code/index.html",
    "title": "Austrian Hotels - Code",
    "section": "",
    "text": "← Back to Austrian Hotels Case Study"
  },
  {
    "objectID": "case-studies/austria-hotels/code/index.html#code-files",
    "href": "case-studies/austria-hotels/code/index.html#code-files",
    "title": "Austrian Hotels - Code",
    "section": "Code Files",
    "text": "Code Files\nCode examples to be added.\nCheck back later for R and Python examples demonstrating table joins with this dataset."
  },
  {
    "objectID": "case-studies/VWS/data/index.html",
    "href": "case-studies/VWS/data/index.html",
    "title": "World Values Survey - Data Files",
    "section": "",
    "text": "← Back to WVS Case Study"
  },
  {
    "objectID": "case-studies/VWS/data/index.html#data-files",
    "href": "case-studies/VWS/data/index.html#data-files",
    "title": "World Values Survey - Data Files",
    "section": "Data Files",
    "text": "Data Files\n\n\n\nFile\nDescription\n\n\n\n\nWVS_random_subset2000.csv\nRandom subset of 2,000 respondents stratified by country\n\n\nWVS_GDP_merged_data.csv\nCountry-level aggregated data merged with GDP indicators\n\n\nWVS_subset.csv\nSelected variables from full dataset"
  },
  {
    "objectID": "case-studies/VWS/data/index.html#documentation",
    "href": "case-studies/VWS/data/index.html#documentation",
    "title": "World Values Survey - Data Files",
    "section": "Documentation",
    "text": "Documentation\n\n\n\nFile\nDescription\n\n\n\n\ncodebook.pdf\nOfficial WVS codebook with variable definitions\n\n\nREADME_WVS.md\nData processing documentation"
  },
  {
    "objectID": "case-studies/VWS/data/index.html#external-source",
    "href": "case-studies/VWS/data/index.html#external-source",
    "title": "World Values Survey - Data Files",
    "section": "External Source",
    "text": "External Source\nFull dataset available at OSF - Gabors Data Analysis / World Values Survey"
  },
  {
    "objectID": "case-studies/VWS/code/index.html",
    "href": "case-studies/VWS/code/index.html",
    "title": "World Values Survey - Code",
    "section": "",
    "text": "← Back to WVS Case Study"
  },
  {
    "objectID": "case-studies/VWS/code/index.html#code-files",
    "href": "case-studies/VWS/code/index.html#code-files",
    "title": "World Values Survey - Code",
    "section": "Code Files",
    "text": "Code Files\n\n\n\nFile\nDescription\n\n\n\n\ncleaning.R\nR script that processes raw WVS data and creates cleaned datasets"
  },
  {
    "objectID": "case-studies/earnings/data/index.html",
    "href": "case-studies/earnings/data/index.html",
    "title": "US Earnings - Data Files",
    "section": "",
    "text": "← Back to Earnings Case Study"
  },
  {
    "objectID": "case-studies/earnings/data/index.html#data-files",
    "href": "case-studies/earnings/data/index.html#data-files",
    "title": "US Earnings - Data Files",
    "section": "Data Files",
    "text": "Data Files\n\n\n\nFile\nDescription\n\n\n\n\nmorg-2014-emp-state5.csv\nCPS MORG 2014 data filtered to 5 states\n\n\ncpsx.pdf\nCodebook and variable documentation"
  },
  {
    "objectID": "case-studies/earnings/data/index.html#external-source",
    "href": "case-studies/earnings/data/index.html#external-source",
    "title": "US Earnings - Data Files",
    "section": "External Source",
    "text": "External Source\nFull CPS data available at OSF - Gabors Data Analysis"
  },
  {
    "objectID": "navbar/weeks.html",
    "href": "navbar/weeks.html",
    "title": "Weekly Content",
    "section": "",
    "text": "Using AI for code. May not be covered in this class, as it had often been already covered in coding classes.\nContent"
  },
  {
    "objectID": "navbar/weeks.html#week01-llm-review",
    "href": "navbar/weeks.html#week01-llm-review",
    "title": "Weekly Content",
    "section": "Week01: LLM Review",
    "text": "Week01: LLM Review\nWhat are LLMs, how is the magic happening. A non-technical brief intro. How to work with LLMs? Plus ideas on applications. Includes suggested readings, podcasts, and vids to listen to.\nContent\nWhich AI? See my take on current models. As of May 2025."
  },
  {
    "objectID": "navbar/weeks.html#week02-data-and-code-discovery-and-documentation-with-ai",
    "href": "navbar/weeks.html#week02-data-and-code-discovery-and-documentation-with-ai",
    "title": "Weekly Content",
    "section": "Week02: Data and code discovery and documentation with AI",
    "text": "Week02: Data and code discovery and documentation with AI\nLearn how to write a clear and professional code and data documentation. LLMs are great help once you know the basics.\nCase study: World Values Survey\nContent"
  },
  {
    "objectID": "navbar/weeks.html#week-03-writing-reports",
    "href": "navbar/weeks.html#week-03-writing-reports",
    "title": "Weekly Content",
    "section": "Week 03: Writing Reports",
    "text": "Week 03: Writing Reports\nYou have your data and task, and need to write a short report. We compare different options with LLM, from one-shot prompt to iteration.\nCase study: World Values Survey\nContent"
  },
  {
    "objectID": "navbar/weeks.html#week04-agentic-ai-with-claude-code",
    "href": "navbar/weeks.html#week04-agentic-ai-with-claude-code",
    "title": "Weekly Content",
    "section": "Week04: Agentic AI with Claude Code",
    "text": "Week04: Agentic AI with Claude Code\nFrom chat to terminal - introducing Claude Code for data analysis. Students learn to use agentic AI that works directly with files, generates data, and iterates on analysis.\nCase study: Austrian Hotels\nContent"
  },
  {
    "objectID": "navbar/weeks.html#week05-text-as-data-1-intro-lecture",
    "href": "navbar/weeks.html#week05-text-as-data-1-intro-lecture",
    "title": "Weekly Content",
    "section": "Week05: Text as data 1 – intro lecture",
    "text": "Week05: Text as data 1 – intro lecture\nNo course of mine can escape football (soccer). Here we look at post-game interviews to learn basics of text analysis and apply LLMs in what they are best - context dependent learning. Two class series. First is more intro to natural language processing.\nCase study: Football Manager Interviews\nContent"
  },
  {
    "objectID": "navbar/weeks.html#week06-sentiment-analysis-with-ai",
    "href": "navbar/weeks.html#week06-sentiment-analysis-with-ai",
    "title": "Weekly Content",
    "section": "Week06: Sentiment Analysis with AI",
    "text": "Week06: Sentiment Analysis with AI\nSecond class, now we are in action. How does LLM compare to humans?\nCase study: Football Manager Interviews\nContent"
  },
  {
    "objectID": "navbar/weeks.html#week07-tba",
    "href": "navbar/weeks.html#week07-tba",
    "title": "Weekly Content",
    "section": "Week07: TBA",
    "text": "Week07: TBA\nTBA"
  },
  {
    "objectID": "navbar/weeks.html#week08-sql-like-natural-language-query-in-a-data-warehouse",
    "href": "navbar/weeks.html#week08-sql-like-natural-language-query-in-a-data-warehouse",
    "title": "Weekly Content",
    "section": "Week08: SQL like natural language query in a data warehouse",
    "text": "Week08: SQL like natural language query in a data warehouse\nTBA"
  },
  {
    "objectID": "navbar/weeks.html#week09-ai-as-research-companion-control-variables",
    "href": "navbar/weeks.html#week09-ai-as-research-companion-control-variables",
    "title": "Weekly Content",
    "section": "Week09: AI as research companion: Control variables",
    "text": "Week09: AI as research companion: Control variables\nContent"
  },
  {
    "objectID": "navbar/weeks.html#week10-ai-as-research-companion-instrumental-variables",
    "href": "navbar/weeks.html#week10-ai-as-research-companion-instrumental-variables",
    "title": "Weekly Content",
    "section": "Week10: AI as research companion: Instrumental variables",
    "text": "Week10: AI as research companion: Instrumental variables\nContent"
  },
  {
    "objectID": "navbar/weeks.html#week11-ai-as-research-companion-differences-in-differences",
    "href": "navbar/weeks.html#week11-ai-as-research-companion-differences-in-differences",
    "title": "Weekly Content",
    "section": "Week11: AI as research companion: Differences in differences",
    "text": "Week11: AI as research companion: Differences in differences\nTBA"
  },
  {
    "objectID": "navbar/weeks.html#week12-creating-simulations-with-apps-and-dashboards",
    "href": "navbar/weeks.html#week12-creating-simulations-with-apps-and-dashboards",
    "title": "Weekly Content",
    "section": "Week12: Creating simulations with apps and dashboards",
    "text": "Week12: Creating simulations with apps and dashboards\nContent\nTBA"
  },
  {
    "objectID": "navbar/resources.html",
    "href": "navbar/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Glossary of technical terms used in various presentations or in class\nLLM Glossary\nWhich AI – the 2026 January comparison"
  },
  {
    "objectID": "navbar/resources.html#llm-technical-terms",
    "href": "navbar/resources.html#llm-technical-terms",
    "title": "Resources",
    "section": "",
    "text": "Glossary of technical terms used in various presentations or in class\nLLM Glossary\nWhich AI – the 2026 January comparison"
  },
  {
    "objectID": "navbar/resources.html#ai-tools-setup",
    "href": "navbar/resources.html#ai-tools-setup",
    "title": "Resources",
    "section": "AI Tools Setup",
    "text": "AI Tools Setup\nTerminal Basics – essential commands for CLI tools\nInstalling AI CLI Tools – Claude Code, Gemini CLI, Codex CLI, OpenCode\nSetting up Claude Code – detailed Claude Code guide"
  },
  {
    "objectID": "navbar/resources.html#beyond-the-material-learn-more",
    "href": "navbar/resources.html#beyond-the-material-learn-more",
    "title": "Resources",
    "section": "Beyond (the material) — Learn more",
    "text": "Beyond (the material) — Learn more\nI’m collecting videos, papers in the beyond page. Check them out. Regular updates."
  },
  {
    "objectID": "navbar/resources.html#various-data-analysis-related-pages-referenced-in-class",
    "href": "navbar/resources.html#various-data-analysis-related-pages-referenced-in-class",
    "title": "Resources",
    "section": "Various data analysis related pages referenced in class",
    "text": "Various data analysis related pages referenced in class\nDocumentation and readme Good report ideas"
  },
  {
    "objectID": "navbar/resources.html#case-studies",
    "href": "navbar/resources.html#case-studies",
    "title": "Resources",
    "section": "Case Studies",
    "text": "Case Studies\nCourse datasets and code are organized by case study:\n\nWorld Values Survey - Week 2-3 (Documentation, Reports)\nAustrian Hotels - Week 4 (Joining Tables)\nFootball Manager Interviews - Week 5-6 (Text as Data)\nUS Earnings (CPS) - Week 0 (Coding with AI)\nEmployee Commits - Data analysis demos\nCommon Support (R) - Iterative AI coding\n\nBrowse All Case Studies"
  },
  {
    "objectID": "week11/index.html",
    "href": "week11/index.html",
    "title": "Week 11: AI as research companion: Difference in differences",
    "section": "",
    "text": "Week 11: AI as research companion: Difference in differences\n\n\nGetting closer to causality by building Difference in differences models",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 11: AI Research: DiD"
    ]
  },
  {
    "objectID": "week00/index.html",
    "href": "week00/index.html",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "",
    "text": "Week 00: AI-Assisted Coding Prep\n\n\nBoosting coding for data analysis with AI tools",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: AI Coding Prep"
    ]
  },
  {
    "objectID": "week00/index.html#learning-objectives",
    "href": "week00/index.html#learning-objectives",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, students will:\n\nUnderstand how AI transforms coding workflows for data analysis tasks\nLearn effective prompting strategies for data analysis code generation\nExperience key AI coding tools: ChatGPT Canvas, Claude Projects, GitHub Copilot\nPractice AI-assisted coding for common data analysis tasks\nSet up integrated development environments with AI assistance\nFocus on R and Python.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: AI Coding Prep"
    ]
  },
  {
    "objectID": "week00/index.html#preparation-before-class",
    "href": "week00/index.html#preparation-before-class",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Preparation / Before Class",
    "text": "Preparation / Before Class\n\n\n🔧 Tool Setup\n\n\nRequired Access:\n\nChatGPT account (free tier sufficient for testing)\nClaude account (free tier sufficient)\n\nRecommended Setup:\n\nGitHub account for Copilot (free for students)\nVSCode, RStudio, or Jupyter Notebook installed\nYour preferred data analysis language ready (R/Python)\nYou can you a great deal of this for other languages like SQL, Stata or Julia.\n\nOptional but Valuable:\n\nCursor AI editor (free trial available)",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: AI Coding Prep"
    ]
  },
  {
    "objectID": "week00/index.html#class-material",
    "href": "week00/index.html#class-material",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Class Material",
    "text": "Class Material\n\n\n🤖 Why AI + Data Analysis Coding Works (20 min)\n\n\nCovers key set of tasks:\n\nRepetitive patterns: Data analysis has common workflows (load → clean/wrangle → analyze → visualize)\nWell-documented libraries: pandas, scikit-learn, as well as dplyr, ggplot2, are extensively covered in AI training data (had been in use for a while). With updates, it knows more recent libraries from tensorflow to fixest (R)\nClear intentions: “Create a scatterplot with regression line” is specific enough for good code generation. Well established “good practice”\nIterative nature: Data analysis involves lots of tweaking and refinement – chat aspect helpful.\n\nWhat AI Excels At:\n\nWell defined code chunks and setup (libraries, dependencies)\nSyntax, how exactly do certain coding tasks (such as regex, loops)\nStandard statistical procedures\nData manipulation and cleaning\nBasic visualization code\n\nHuman Oversight and Decisions are Needed:\n\nMost code\nResearch design decisions\nStatistical / econometric interpretation\nDomain-specific logic\nFinalize question (vs bland suggestions)\nQuality control and validation\n\n\n\n\n\n📝 Effective Prompting for Data Analysis (25 min)\n\n\nModel Recommendations:\n\nChatGPT 4o / Claude Sonnet 4. works about equally fine.\n\n\nPrompting Best Practices\nSome ideas to help get the code do what you want, or at least close enough.\nBe specific\n\nBe specific about libraries: “Using pandas and seaborn…” vs. “Using R and ggplot2…”\nInclude data structure: “DataFrame with columns: date, price, volume”\nSpecify output format: “Save as PNG for publication” or “Return as tidy data table” or “give back markedown (latex) text with equations”\n\nDefine language, preferences\n\nI use R and tidyverse, so unless specified, use that\nI use Python, and when possible prefer polars to pandas\nAsk for comments: “Include detailed comments explaining each step”\n\nFor frequent tasks, AI will know which library to use in a language\n\nFilter on size&lt;5 – will do it in Python Pandas as default.\n\nScatterplot of sales and employment – Will do it seaborn / matplotlib or ggplot in R.\n\nAdvice include data structure\n\nIf possible upload the data\nif not, upload a small bit of data, like 1/1000 random sample\nStart with creating a data dictionary (see also class 3)\n\n\n\nExample prompts\nYou can experiment with a vague prompt or being specific.\nExample Prompt 1: broad\n\"Here is some sales data, summarize regional variation.\"\nvs\nExample Prompt 2: with details\n\"Create Python (R) code using pandas (dplyr) to:\n\n1) Load CSV with columns id, date, sales, region\n2) Filter for 2023 data  \n3) Group by region and calculate mean sales\n4) Create a bar chart with plotnine / ggplot2\"\nWhat are the pros and cons of each?\n\n\nSome tasks where AI works well\n1. Data Cleaning Pipeline 2. Exploratory Data Analysis 3. Statistical Modeling 4. Machine Learning Pipeline\n\n\n\n\n\n📝 Creating Graphs exercise (40 min)\n\n\nA full practice exercise\nCreated one example on gender wage gap.\nIt follows the case study on the in Chapter 09\nWe start with actual earnings data from the US, called CPS MORG.\nWe end up with:\n\n\n\nNice heatmap\n\n\nOther ideas\n\nInstructors may have their datasets and exercises. We have plenty here: Case studies, each comes with data",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: AI Coding Prep"
    ]
  },
  {
    "objectID": "week00/index.html#setting-up-tool",
    "href": "week00/index.html#setting-up-tool",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Setting up tool",
    "text": "Setting up tool\n\n\n🛠️ AI Coding Tools Showcase (30 min)\n\n\nChatGPT Tools:\n\nCanvas: Collaborative coding environment for iterative development\nAdvanced Data Analysis: Upload datasets, generate analysis with code execution\nGPT-4.1: Optimized specifically for coding tasks\n\nClaude Tools:\n\nProjects: Upload full datasets and documentation for context-aware coding\nArtifacts: Code generation with real-time preview and editing\nClaude 4 Sonnet: Strong reasoning for complex analytical workflows\n\nSpecialized Coding Tools:\n\nGitHub Copilot: Inline code completion integrated into your existing editor\nCursor AI: AI-first code editor with context-aware suggestions\nReplit: Browser-based coding with AI assistance\n\nHands-on Demo:\n\nUpload sample dataset to ChatGPT Advanced Data Analysis\nCreate Claude Project with course data\nCompare code generation approaches\n\n\n\n\n\n⚡ GitHub Copilot Integration (20 min)\n\n\nSetup in Different Environments:\nVSCode:\n\nInstall GitHub Copilot extension\nAuthenticate with GitHub account\nUse Ctrl+Space for suggestions, Tab to accept\n\nJupyter Notebook:\n\nInstall via VS Code Jupyter extension or JupyterLab extension\nInline suggestions while typing\nCopilot Chat for longer explanations\n\nRStudio:\n\nEnable GitHub Copilot in Global Options &gt; Code &gt; Completion\nWorks with R scripts and R Markdown\nSuggests tidyverse and base R patterns\n\nWorkflow Best Practices:\n\nWrite descriptive comments before code blocks\nUse meaningful variable names to guide suggestions\nAccept suggestions, then modify as needed\nUse Copilot Chat for explanations and debugging",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: AI Coding Prep"
    ]
  },
  {
    "objectID": "week00/index.html#discussion-questions",
    "href": "week00/index.html#discussion-questions",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Discussion Questions",
    "text": "Discussion Questions\nReflection:\n\nWhich AI tool felt most natural for your coding style?\nWhere did AI suggestions surprise you (positively or negatively)?\nHow might this change your typical data analysis workflow?\nWhat validation steps would you add when using AI-generated code?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: AI Coding Prep"
    ]
  },
  {
    "objectID": "week00/index.html#background-tools-and-resources",
    "href": "week00/index.html#background-tools-and-resources",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Background, Tools and Resources",
    "text": "Background, Tools and Resources\nGetting Started:\n\nGitHub Student Pack – Free Copilot access\nCursor AI – AI-first code editor\nOpenAI Codex Cookbook – Advanced prompting examples\n\nKey Insight: AI coding assistance is most powerful when you understand the underlying concepts. Use AI to accelerate implementation, not replace understanding.\nNext Week: Week 1 - LLM Review where we’ll explore broader AI concepts for data analysis.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: AI Coding Prep"
    ]
  },
  {
    "objectID": "week00/index.html#some-personal-comments-on-ai-and-this-class",
    "href": "week00/index.html#some-personal-comments-on-ai-and-this-class",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Some personal comments on AI and this class",
    "text": "Some personal comments on AI and this class\n\nWhile this class is called week00, it was created last. You may have guessed righ.\nAI (Claude 4.0) created a great deal of this class following a detailed prompt. I also asked whether to keep it as week00 or change all numbering. Great answers.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: AI Coding Prep"
    ]
  },
  {
    "objectID": "week00/assets/variable-dictionary-chatgpt.html",
    "href": "week00/assets/variable-dictionary-chatgpt.html",
    "title": "CPS MORG 2014 Variable Dictionary by ChatGPT 4o",
    "section": "",
    "text": "my single prompt\nThis is my data and the codebook. Create a variable dictionary. Use the pdf i shared earlier. Output as markdown. For each variable: varname, labels, type, coverage (% missing), mean and mode. Round up numbers. Look at cps and provide short labels. Get me an .md I can download.\n\n\nVariable dictionary\n\n\n\nvarname\ntype\nmissing (%)\nmean\nmode\n\n\n\n\n…1\nint64\n0%\n197435.3\n83378\n\n\nhhid\nint64\n0%\n404953427059356.6\n250109964300640\n\n\nintmonth\nobject\n0%\nNA\nDecember\n\n\nstfips\nobject\n0%\nNA\nFL\n\n\nweight\nfloat64\n0%\n2550.8\n291.4682\n\n\nearnwke\nfloat64\n0%\n933.7\n2884.61\n\n\nuhours\nint64\n0%\n39.4\n40\n\n\ngrade92\nint64\n0%\n41.0\n39\n\n\nrace\nint64\n0%\n1.5\n1\n\n\nethnic\nfloat64\n89%\n3.8\n1.0\n\n\nage\nint64\n0%\n41.0\n44\n\n\nsex\nint64\n0%\n1.5\n2\n\n\nmarital\nint64\n0%\n3.6\n1\n\n\nownchild\nint64\n0%\n0.6\n0\n\n\nchldpres\nint64\n0%\n1.6\n0\n\n\nprcitshp\nobject\n0%\nNA\nNative, Born In US\n\n\nstate\nint64\n0%\n5.0\n5\n\n\nind02\nobject\n0%\nNA\nElementary and secondary schools (6111)\n\n\nocc2012\nint64\n0%\n3917.9\n430\n\n\nclass\nobject\n0%\nNA\nPrivate, For Profit\n\n\nunionmme\nobject\n0%\nNA\nNo\n\n\nunioncov\nobject\n7%\nNA\nNo\n\n\nlfsr94\nobject\n0%\nNA\nEmployed-At Work"
  },
  {
    "objectID": "week00/assets/creating-graphs.html",
    "href": "week00/assets/creating-graphs.html",
    "title": "Creating Graphs",
    "section": "",
    "text": "Is there a difference between how much men and women make? How does it vary by age?\nThe topic is related to the case study on gender gap in earnings.\n\n\n\nAs a case study, consider the earnings case study from the Data Analysis textbook\n\nHave a look at the info readme\nDownload morg-2014-emp.csv from earnings data\nDownload cpsx.pdf, which is an old school codebook\nRead in morg-2014-emp.csv\n\nOne state\nLet us start filtering on the largest state\n1. Create an ordered frequency table of the state, and filter on the largest one\nI used RStudio with GitHub Copilot to write all three code snippets. It automatically guessed the third bit.\n```{r}\n\n# setup\nlibrary(tidyverse)\n\n# ordered freq table of state\nmorg %&gt;%\n  group_by(state) %&gt;%\n  summarise(n = n()) %&gt;%\n  arrange(desc(n)) %&gt;%\n  mutate(state = fct_reorder(state, n)) %&gt;%\n  ggplot(aes(x = state, y = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Number of Employees by State in Morg\",\n       x = \"State\",\n       y = \"Number of Employees\") +\n  theme_minimal()\n\n# find the largest state in terms of number of obs\n\nstate_max=morg %&gt;%\n  group_by(state) %&gt;%\n  summarise(n = n()) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice(1) %&gt;%\n  pull(state)\n\n\n# filter morg for the largest state\nmorg_largest_state &lt;- morg %&gt;%\n  filter(state == state_max)\n\n```\nHold on, what does max state=5 mean? Ask ChatGPT.\n\nSave this file as morg-2014-emp-state5.csv. You can also find it in the US Earnings case study\nGet to know the file\n\nUpload the file and ask for a variable dictionary. After 2 iterations, I used this.\n\nCreate a variable dictionary. Use the pdf i shared earlier. Output as markdown. For each variable: varname, labels, type, coverage (% missing), mean and mode. Round up numbers. Look at cps and provide short labels. Get me an .md I can download.\n\n\nChatGPT created a table, yes.\nBut look at what Claude did, amazing.\n\n\n\n\nFirst try\nIs there a gender gap in earnings? Show me a graph.\nChatGPT will run Data Analysis – create a short Python script and display a graph.\n```{python}\nimport matplotlib.pyplot as plt\n\n# Check for gender and earnings columns\nif \"sex\" in df.columns and \"earnwke\" in df.columns:\n    # Drop rows with missing or zero earnings\n    df_filtered = df[(df[\"earnwke\"].notna()) & (df[\"earnwke\"] &gt; 0)]\n\n    # Group by sex and calculate mean earnings\n    gender_earnings = df_filtered.groupby(\"sex\")[\"earnwke\"].mean().reset_index()\n    gender_earnings[\"earnwke\"] = gender_earnings[\"earnwke\"].round(2)\n\n    # Create a bar plot\n    plt.figure(figsize=(6, 4))\n    plt.bar(gender_earnings[\"sex\"], gender_earnings[\"earnwke\"])\n    plt.title(\"Average Weekly Earnings by Gender\")\n    plt.xlabel(\"Sex (1 = Male, 2 = Female)\")\n    plt.ylabel(\"Mean Weekly Earnings\")\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.tight_layout()\n    plt.show()\nelse:\n    raise ValueError(\"Required columns 'sex' and 'earnwke' not found in dataset.\")\n\n```\nHere is the chart:\n Followed by this text “Here is the graph showing the gender gap in weekly earnings. Males (1) earn more on average than females (2) in this sample. Let me know if you want to adjust for hours worked or other covariates.”\nYes, adjust by age.\n\n\n\nThe graph I was created\n\n\nNow what? By now, you have some idea, what you’d like. For me, it was:\nMaybe binscatter by year, start 21. fit loess for males and females\nTry it!\nFor me, it turns out that it needed some adjustments after several iterations to get this. Add CI. Add Footnote. Increase FN font size. Add N of obs. Change color scheme.\n```{python}\n\nimport matplotlib.cm as cm\nfrom matplotlib.colors import to_hex\n\n# Get viridis colors\nviridis = cm.get_cmap(\"viridis\", 2)\ncolor_male = to_hex(viridis(0.1))\ncolor_female = to_hex(viridis(0.8))\n\n# Count observations\nn_obs = len(reg_data)\n\n# Updated footnote\nfootnote = (\n    f\"Notes: Residual hourly wages from a linear regression controlling for occupation (C(occ2012))\\n\"\n    f\"and education (C(grade92)), using 2014 CPS MORG data from California (N = {n_obs}). Residuals smoothed by\\n\"\n    f\"LOESS (span = 0.3), with 95% CI from 100 bootstrap replications. Sample restricted to individuals aged\\n\"\n    f\"21–80 with positive usual weekly hours and earnings.\"\n)\n\n# Plot\nplt.figure(figsize=(9, 6))\n\nplt.plot(x_m, y_m, label=\"Men\", color=color_male, linewidth=2)\nplt.fill_between(x_m, l_m, u_m, color=color_male, alpha=0.2)\n\nplt.plot(x_f, y_f, label=\"Women\", color=color_female, linewidth=2)\nplt.fill_between(x_f, l_f, u_f, color=color_female, alpha=0.2)\n\nplt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=0.8)\nplt.title(\"Adjusted Gender Wage Gap Across Age\", fontsize=15)\nplt.xlabel(\"Age\", fontsize=12)\nplt.ylabel(\"Residual Hourly Wage (USD)\", fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend(title=\"Gender\", labels=[\"Men\", \"Women\"], fontsize=10, title_fontsize=11)\n\n# Updated footnote with larger font\nplt.figtext(0.01, -0.12, footnote, fontsize=10, ha=\"left\", va=\"top\")\n\nplt.tight_layout(rect=[0, 0.12, 1, 1])\nplt.savefig(\"/mnt/data/gender_wage_gap_viridis_residuals.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n```\nIt took some iterations, starting with:\n\n\n\nFourth iteration\n\n\nTo get to something I liked. It made errors, forgot labels, and made coding errors. Check the process.\n\n\n\nFourth iteration\n\n\n```{python}\nimport statsmodels.api as sm\nfrom sklearn.utils import resample\n\n# Restrict and prepare data for regression\nreg_data = df[\n    ['age', 'earnwke', 'uhours', 'sex', 'grade92', 'occ2012']\n].dropna()\nreg_data = reg_data[\n    (reg_data['age'] &gt;= 21) & (reg_data['age'] &lt;= 62) &\n    (reg_data['uhours'] &gt; 0) & (reg_data['earnwke'] &gt; 0)\n].copy()\n\n# Compute hourly wage\nreg_data['hourly'] = reg_data['earnwke'] / reg_data['uhours']\n\n# Residualize hourly wage on occ and grade\nX = pd.get_dummies(reg_data[['grade92', 'occ2012']].astype(\"category\"), drop_first=True)\nX = sm.add_constant(X)\nmodel = sm.OLS(reg_data['hourly'], X).fit()\nreg_data['resid'] = model.resid\nreg_data['gender'] = reg_data['sex'].map({1: 'Male', 2: 'Female'})\n\n# Define bootstrap function\ndef bootstrap_loess(data, gender, x_col, y_col, span=0.3, n_boot=100):\n    from statsmodels.nonparametric.smoothers_lowess import lowess\n\n    subset = data[data['gender'] == gender]\n    x_vals = np.linspace(21, 80, 60)\n    y_boot = np.zeros((n_boot, len(x_vals)))\n\n    for i in range(n_boot):\n        boot_sample = resample(subset)\n        smoothed = lowess(boot_sample[y_col], boot_sample[x_col], frac=span, return_sorted=True)\n        y_interp = np.interp(x_vals, smoothed[:, 0], smoothed[:, 1])\n        y_boot[i, :] = y_interp\n\n    y_mean = y_boot.mean(axis=0)\n    y_low = np.percentile(y_boot, 2.5, axis=0)\n    y_high = np.percentile(y_boot, 97.5, axis=0)\n    return x_vals, y_mean, y_low, y_high\n\n# Run bootstrap LOESS for both genders\nx_m, y_m, l_m, u_m = bootstrap_loess(reg_data, 'Male', 'age', 'resid')\nx_f, y_f, l_f, u_f = bootstrap_loess(reg_data, 'Female', 'age', 'resid')\n\n# Colors and footnote\nimport matplotlib.cm as cm\nfrom matplotlib.colors import to_hex\n\nviridis = cm.get_cmap(\"viridis\", 2)\ncolor_male = to_hex(viridis(0.1))\ncolor_female = to_hex(viridis(0.8))\n\nn_obs = len(reg_data)\nfootnote = (\n    f\"Notes: Residual hourly wages from a linear regression controlling for occupation (C(occ2012))\\n\"\n    f\"and education (C(grade92)), using 2014 CPS MORG data from California (N = {n_obs}). Residuals smoothed by\\n\"\n    f\"LOESS (span = 0.3), with 95% CI from 100 bootstrap replications. Sample restricted to individuals aged\\n\"\n    f\"21–80 with positive usual weekly hours and earnings.\"\n)\n\n# Plot\nplt.figure(figsize=(9, 6))\nplt.plot(x_m, y_m, label=\"Men\", color=color_male, linewidth=2)\nplt.fill_between(x_m, l_m, u_m, color=color_male, alpha=0.2)\nplt.plot(x_f, y_f, label=\"Women\", color=color_female, linewidth=2)\nplt.fill_between(x_f, l_f, u_f, color=color_female, alpha=0.2)\n\nplt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=0.8)\nplt.title(\"Adjusted Gender Wage Gap Across Age\", fontsize=15)\nplt.xlabel(\"Age\", fontsize=12)\nplt.ylabel(\"Residual Hourly Wage (USD)\", fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend(title=\"Gender\", labels=[\"Men\", \"Women\"], fontsize=10, title_fontsize=11)\nplt.figtext(0.01, -0.12, footnote, fontsize=10, ha=\"left\", va=\"top\")\nplt.tight_layout(rect=[0, 0.12, 1, 1])\nplt.savefig(\"/mnt/data/gender_wage_gap_viridis_residuals.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n```\nFinal thoughts \n\nWhile AI is great, it needed some iterations to get what I want, not just something.\nOh, and you can go further. Think about this graph. Is the econometrics of it okay?\n\n\n\n\n Ok now show a heatmap of hourly wages by occupation and education level. Keep viridis. \nNot great.\nCreate a few distinct categories of occupation.\nThen create bins for education.\nAlmost.\nSwap colors\nNow we are talking:\n\n\n\nFourth iteration\n\n\n\n\n\nMany other options. Two extremes.\n\nDescribe age. In a graph. (Hint: think about binsize)\nCreate an interesting and well-done scatterplot (turn o3 on or use Claude Sonnet)"
  },
  {
    "objectID": "week00/assets/creating-graphs.html#an-example",
    "href": "week00/assets/creating-graphs.html#an-example",
    "title": "Creating Graphs",
    "section": "",
    "text": "Is there a difference between how much men and women make? How does it vary by age?\nThe topic is related to the case study on gender gap in earnings.\n\n\n\nAs a case study, consider the earnings case study from the Data Analysis textbook\n\nHave a look at the info readme\nDownload morg-2014-emp.csv from earnings data\nDownload cpsx.pdf, which is an old school codebook\nRead in morg-2014-emp.csv\n\nOne state\nLet us start filtering on the largest state\n1. Create an ordered frequency table of the state, and filter on the largest one\nI used RStudio with GitHub Copilot to write all three code snippets. It automatically guessed the third bit.\n```{r}\n\n# setup\nlibrary(tidyverse)\n\n# ordered freq table of state\nmorg %&gt;%\n  group_by(state) %&gt;%\n  summarise(n = n()) %&gt;%\n  arrange(desc(n)) %&gt;%\n  mutate(state = fct_reorder(state, n)) %&gt;%\n  ggplot(aes(x = state, y = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Number of Employees by State in Morg\",\n       x = \"State\",\n       y = \"Number of Employees\") +\n  theme_minimal()\n\n# find the largest state in terms of number of obs\n\nstate_max=morg %&gt;%\n  group_by(state) %&gt;%\n  summarise(n = n()) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice(1) %&gt;%\n  pull(state)\n\n\n# filter morg for the largest state\nmorg_largest_state &lt;- morg %&gt;%\n  filter(state == state_max)\n\n```\nHold on, what does max state=5 mean? Ask ChatGPT.\n\nSave this file as morg-2014-emp-state5.csv. You can also find it in the US Earnings case study\nGet to know the file\n\nUpload the file and ask for a variable dictionary. After 2 iterations, I used this.\n\nCreate a variable dictionary. Use the pdf i shared earlier. Output as markdown. For each variable: varname, labels, type, coverage (% missing), mean and mode. Round up numbers. Look at cps and provide short labels. Get me an .md I can download.\n\n\nChatGPT created a table, yes.\nBut look at what Claude did, amazing.\n\n\n\n\nFirst try\nIs there a gender gap in earnings? Show me a graph.\nChatGPT will run Data Analysis – create a short Python script and display a graph.\n```{python}\nimport matplotlib.pyplot as plt\n\n# Check for gender and earnings columns\nif \"sex\" in df.columns and \"earnwke\" in df.columns:\n    # Drop rows with missing or zero earnings\n    df_filtered = df[(df[\"earnwke\"].notna()) & (df[\"earnwke\"] &gt; 0)]\n\n    # Group by sex and calculate mean earnings\n    gender_earnings = df_filtered.groupby(\"sex\")[\"earnwke\"].mean().reset_index()\n    gender_earnings[\"earnwke\"] = gender_earnings[\"earnwke\"].round(2)\n\n    # Create a bar plot\n    plt.figure(figsize=(6, 4))\n    plt.bar(gender_earnings[\"sex\"], gender_earnings[\"earnwke\"])\n    plt.title(\"Average Weekly Earnings by Gender\")\n    plt.xlabel(\"Sex (1 = Male, 2 = Female)\")\n    plt.ylabel(\"Mean Weekly Earnings\")\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.tight_layout()\n    plt.show()\nelse:\n    raise ValueError(\"Required columns 'sex' and 'earnwke' not found in dataset.\")\n\n```\nHere is the chart:\n Followed by this text “Here is the graph showing the gender gap in weekly earnings. Males (1) earn more on average than females (2) in this sample. Let me know if you want to adjust for hours worked or other covariates.”\nYes, adjust by age.\n\n\n\nThe graph I was created\n\n\nNow what? By now, you have some idea, what you’d like. For me, it was:\nMaybe binscatter by year, start 21. fit loess for males and females\nTry it!\nFor me, it turns out that it needed some adjustments after several iterations to get this. Add CI. Add Footnote. Increase FN font size. Add N of obs. Change color scheme.\n```{python}\n\nimport matplotlib.cm as cm\nfrom matplotlib.colors import to_hex\n\n# Get viridis colors\nviridis = cm.get_cmap(\"viridis\", 2)\ncolor_male = to_hex(viridis(0.1))\ncolor_female = to_hex(viridis(0.8))\n\n# Count observations\nn_obs = len(reg_data)\n\n# Updated footnote\nfootnote = (\n    f\"Notes: Residual hourly wages from a linear regression controlling for occupation (C(occ2012))\\n\"\n    f\"and education (C(grade92)), using 2014 CPS MORG data from California (N = {n_obs}). Residuals smoothed by\\n\"\n    f\"LOESS (span = 0.3), with 95% CI from 100 bootstrap replications. Sample restricted to individuals aged\\n\"\n    f\"21–80 with positive usual weekly hours and earnings.\"\n)\n\n# Plot\nplt.figure(figsize=(9, 6))\n\nplt.plot(x_m, y_m, label=\"Men\", color=color_male, linewidth=2)\nplt.fill_between(x_m, l_m, u_m, color=color_male, alpha=0.2)\n\nplt.plot(x_f, y_f, label=\"Women\", color=color_female, linewidth=2)\nplt.fill_between(x_f, l_f, u_f, color=color_female, alpha=0.2)\n\nplt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=0.8)\nplt.title(\"Adjusted Gender Wage Gap Across Age\", fontsize=15)\nplt.xlabel(\"Age\", fontsize=12)\nplt.ylabel(\"Residual Hourly Wage (USD)\", fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend(title=\"Gender\", labels=[\"Men\", \"Women\"], fontsize=10, title_fontsize=11)\n\n# Updated footnote with larger font\nplt.figtext(0.01, -0.12, footnote, fontsize=10, ha=\"left\", va=\"top\")\n\nplt.tight_layout(rect=[0, 0.12, 1, 1])\nplt.savefig(\"/mnt/data/gender_wage_gap_viridis_residuals.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n```\nIt took some iterations, starting with:\n\n\n\nFourth iteration\n\n\nTo get to something I liked. It made errors, forgot labels, and made coding errors. Check the process.\n\n\n\nFourth iteration\n\n\n```{python}\nimport statsmodels.api as sm\nfrom sklearn.utils import resample\n\n# Restrict and prepare data for regression\nreg_data = df[\n    ['age', 'earnwke', 'uhours', 'sex', 'grade92', 'occ2012']\n].dropna()\nreg_data = reg_data[\n    (reg_data['age'] &gt;= 21) & (reg_data['age'] &lt;= 62) &\n    (reg_data['uhours'] &gt; 0) & (reg_data['earnwke'] &gt; 0)\n].copy()\n\n# Compute hourly wage\nreg_data['hourly'] = reg_data['earnwke'] / reg_data['uhours']\n\n# Residualize hourly wage on occ and grade\nX = pd.get_dummies(reg_data[['grade92', 'occ2012']].astype(\"category\"), drop_first=True)\nX = sm.add_constant(X)\nmodel = sm.OLS(reg_data['hourly'], X).fit()\nreg_data['resid'] = model.resid\nreg_data['gender'] = reg_data['sex'].map({1: 'Male', 2: 'Female'})\n\n# Define bootstrap function\ndef bootstrap_loess(data, gender, x_col, y_col, span=0.3, n_boot=100):\n    from statsmodels.nonparametric.smoothers_lowess import lowess\n\n    subset = data[data['gender'] == gender]\n    x_vals = np.linspace(21, 80, 60)\n    y_boot = np.zeros((n_boot, len(x_vals)))\n\n    for i in range(n_boot):\n        boot_sample = resample(subset)\n        smoothed = lowess(boot_sample[y_col], boot_sample[x_col], frac=span, return_sorted=True)\n        y_interp = np.interp(x_vals, smoothed[:, 0], smoothed[:, 1])\n        y_boot[i, :] = y_interp\n\n    y_mean = y_boot.mean(axis=0)\n    y_low = np.percentile(y_boot, 2.5, axis=0)\n    y_high = np.percentile(y_boot, 97.5, axis=0)\n    return x_vals, y_mean, y_low, y_high\n\n# Run bootstrap LOESS for both genders\nx_m, y_m, l_m, u_m = bootstrap_loess(reg_data, 'Male', 'age', 'resid')\nx_f, y_f, l_f, u_f = bootstrap_loess(reg_data, 'Female', 'age', 'resid')\n\n# Colors and footnote\nimport matplotlib.cm as cm\nfrom matplotlib.colors import to_hex\n\nviridis = cm.get_cmap(\"viridis\", 2)\ncolor_male = to_hex(viridis(0.1))\ncolor_female = to_hex(viridis(0.8))\n\nn_obs = len(reg_data)\nfootnote = (\n    f\"Notes: Residual hourly wages from a linear regression controlling for occupation (C(occ2012))\\n\"\n    f\"and education (C(grade92)), using 2014 CPS MORG data from California (N = {n_obs}). Residuals smoothed by\\n\"\n    f\"LOESS (span = 0.3), with 95% CI from 100 bootstrap replications. Sample restricted to individuals aged\\n\"\n    f\"21–80 with positive usual weekly hours and earnings.\"\n)\n\n# Plot\nplt.figure(figsize=(9, 6))\nplt.plot(x_m, y_m, label=\"Men\", color=color_male, linewidth=2)\nplt.fill_between(x_m, l_m, u_m, color=color_male, alpha=0.2)\nplt.plot(x_f, y_f, label=\"Women\", color=color_female, linewidth=2)\nplt.fill_between(x_f, l_f, u_f, color=color_female, alpha=0.2)\n\nplt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=0.8)\nplt.title(\"Adjusted Gender Wage Gap Across Age\", fontsize=15)\nplt.xlabel(\"Age\", fontsize=12)\nplt.ylabel(\"Residual Hourly Wage (USD)\", fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend(title=\"Gender\", labels=[\"Men\", \"Women\"], fontsize=10, title_fontsize=11)\nplt.figtext(0.01, -0.12, footnote, fontsize=10, ha=\"left\", va=\"top\")\nplt.tight_layout(rect=[0, 0.12, 1, 1])\nplt.savefig(\"/mnt/data/gender_wage_gap_viridis_residuals.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n```\nFinal thoughts \n\nWhile AI is great, it needed some iterations to get what I want, not just something.\nOh, and you can go further. Think about this graph. Is the econometrics of it okay?\n\n\n\n\n Ok now show a heatmap of hourly wages by occupation and education level. Keep viridis. \nNot great.\nCreate a few distinct categories of occupation.\nThen create bins for education.\nAlmost.\nSwap colors\nNow we are talking:\n\n\n\nFourth iteration\n\n\n\n\n\nMany other options. Two extremes.\n\nDescribe age. In a graph. (Hint: think about binsize)\nCreate an interesting and well-done scatterplot (turn o3 on or use Claude Sonnet)"
  },
  {
    "objectID": "week00/assets/creating-graphs.html#now-with-claude",
    "href": "week00/assets/creating-graphs.html#now-with-claude",
    "title": "Creating Graphs",
    "section": "Now with Claude",
    "text": "Now with Claude\nShow me an interesting scatterplot. Make it well done.\nThe Education-Earnings Gradient"
  },
  {
    "objectID": "week00/assets/creating-graphs.html#key-findings",
    "href": "week00/assets/creating-graphs.html#key-findings",
    "title": "Creating Graphs",
    "section": "Key Findings",
    "text": "Key Findings\nThis visualization reveals several important patterns in the 2014 labor market:\n\nClear Education Gradient: Higher education levels consistently associate with higher earnings\nHours Variation: More educated workers tend to work slightly longer hours\nWithin-Group Variation: Substantial earnings variation exists within each education category\nAge Effects: Older workers (larger circles) often earn more within education levels\n\n\n\n\n\n\n\nNote\n\n\n\nThe college earnings premium shown here (47%) aligns with established labor economics research on returns to education.\n\n\nNot perfect. But amazing."
  },
  {
    "objectID": "week00/assets/creating-graphs.html#discussion-points",
    "href": "week00/assets/creating-graphs.html#discussion-points",
    "title": "Creating Graphs",
    "section": "Discussion points",
    "text": "Discussion points\nThink about this for each iteration.\n\nIs the code correct\nWhat do we like and dislike in this graph? Look carefully at all aspects. How would you change it?\nGo through the process and improve each graph to presentation quality.\n\n\nBonus\nYou made it till the end. Your bonus track is California Love"
  },
  {
    "objectID": "week06/index.html",
    "href": "week06/index.html",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "",
    "text": "Week 06: Sentiment Analysis with AI\n\n\nUsing API to AI to create data from text",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Sentiment Analysis"
    ]
  },
  {
    "objectID": "week06/index.html#overview",
    "href": "week06/index.html#overview",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Overview",
    "text": "Overview\nContinue using text for research with AI. Still on sentiment analysis but now we focus on scaling text analysis with APIs: from 20 texts to hundreds and more.\n\nLearning Outcomes\nBy the end of the session, students will:\n\nGain hands-on experience with sentiment analysis.\nHave experience integrating NLP in research\nThink about what is ground truth\nGet a taste of building an analytical pipeline with API usage",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Sentiment Analysis"
    ]
  },
  {
    "objectID": "week06/index.html#preparation-before-class",
    "href": "week06/index.html#preparation-before-class",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Preparation / Before Class",
    "text": "Preparation / Before Class\n\n\n🔧 API Setup\n\n\nAPI Access Setup:\n\nHow to get AI API keys (OpenAI ChatGPT and Anthropic Claude)\nBudget: ~$5 minimum for course exercises\n\nAPI Learning Resources:\n\nIntroduction to APIs - fundamental concepts\nAdvanced API knowledge - how APIs work under the hood\n\n\n\n\n\n📊 Preparation\n\n\nMaterials from Moodle/Course Repository:\n\nCombined interview dataset (text_id level)\nGame information with results (win/draw/loss encoding)\nCombined student ratings from Week 5 assignments (aggregated, anonymized)\nDomain lexicon ratings\n\nData Structure:\n\ntexts: Individual interview transcripts with metadata\ngames: Match results and context information (soccer has 3 outcomes: win, draw, loss – impcation for result encoding)\nratings: Human ratings, AI ratings, lexicon scores by text_id\n\nCode Examples Available:\n\nR implementation requires API key setup in R\nPython implementation\n\ntext and prompts\n\nsentiment guidelines\n\nOther\n\nMore advanced knowledge on APIs – how APIs work",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Sentiment Analysis"
    ]
  },
  {
    "objectID": "week06/index.html#review",
    "href": "week06/index.html#review",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Review",
    "text": "Review\n\n\n💬 Assignment 5 Discussion (20 min)\n\n\nReview of transformation options\n\nlexicon based counting numbers –&gt; generate a transparent script\nMachine-learning classifiers, n-grams etc.\nLLM (transformer-based) one-shot: treating the LLM like a giant classifier: you hand it raw text and ask classes of sentiment. (Deep contextual understanding—word embeddings, attention across the whole sentence etc decide the sentiment.) No separate sentiment lexicon; it’s all encoded in the model weights.\nYou can also take a pretrained LLM and continue training it on thousands of labeled review. See our example guidelines HERE\n\nSharing Experiences:\n\nHuman vs. AI rating differences: Where did you disagree most?\nRating challenges: What aspects of manager interviews were hardest to classify?\nConsistency issues: Did you rate similar texts consistently? Did AI? Was there a consistent gap?\nDomain knowledge impact: How much did football expertise affect your ratings?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Sentiment Analysis"
    ]
  },
  {
    "objectID": "week06/index.html#class-material",
    "href": "week06/index.html#class-material",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Class Material",
    "text": "Class Material\n\n\n🔍 API Walkthrough Sessions (30 min)\n\n\nIntroduction\n\nHow to use APIs\n\nBeginner-Friendly Examples:\n\nSimple walkthrough with GDP data – uses World Bank and FRED APIs\nBit harder walkthrough with football data – uses FBREF soccer data. Guess the club for example.\n\nMore advanced stuff\n\nMore advanced knowledge on APIs – how APIs work\n\n\n\n\n\n📈 Data Analysis Workshop (30 min)\n\n\nData Integration Task:\n\nTake the aggregated file and ask AI for a readme. Discuss what is in the data\nCompare human, domain lexicon and AI rating. For human and AI take the average.\nThink of an interesting comparison using AI rating\nCompare results by human and lexicon rating\n\nDiscussion 2\n\nWhat is ground truth\n\nHow to integrate AI into research\n\ncombine data with text\nthink RQ and how you’d use AI\n\n\n\n\n\n🎯 Advanced Applications (20 min)\n\n\nAdditional tasks if time permits\nPredict gender and result\nNote: Men’s teams have male managers, women’s teams have female managers in this dataset\n\nShow AI all texts and ask to predict the gender of speaker\nShow AI all texts and ask to predict the result (manager’s team won, drew, lost)\n\nDiscussion:\nWhat linguistic cues might reveal gender? Are these reliable?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Sentiment Analysis"
    ]
  },
  {
    "objectID": "week06/index.html#end-of-week-discussion-points",
    "href": "week06/index.html#end-of-week-discussion-points",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "End of Week Discussion points",
    "text": "End of Week Discussion points\n\nGround Truth Problem: In sentiment analysis, what constitutes the “correct” answer? How do we validate when humans disagree?\nAPI Integration: What are the benefits and costs (risks) of using API to scale text analysis?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Sentiment Analysis"
    ]
  },
  {
    "objectID": "week06/index.html#assignment",
    "href": "week06/index.html#assignment",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Assignment",
    "text": "Assignment\n\n\n\n\n\n\nNoteAssignment 6: Gender Classification Pipeline\n\n\n\nDue: Optional extension exercise\nCreate a similar pipeline for predicting manager gender from interview text, including AI explanations of classification decisions.\nFull Assignment Details",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Sentiment Analysis"
    ]
  },
  {
    "objectID": "week06/index.html#some-personal-comments-on-ai-and-this-class",
    "href": "week06/index.html#some-personal-comments-on-ai-and-this-class",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Some personal comments on AI and this class",
    "text": "Some personal comments on AI and this class\n\nAI helped writing the Python code and translating to R. But it needed a great deal of debugging: working with tests and building stable pipelines are hard.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Sentiment Analysis"
    ]
  },
  {
    "objectID": "week06/assets/walkthrough-fbref.html",
    "href": "week06/assets/walkthrough-fbref.html",
    "title": "Walkthrough: Using soccerdata to Fetch Arsenal’s 2023–24 Match Stats",
    "section": "",
    "text": "Below is a simple, step-by-step recipe for pulling Arsenal’s match-by-match team statistics for the 2023–24 Premier League season from FBref, using the soccerdata Python client library. You never write raw HTTP requests—soccerdata handles those for you."
  },
  {
    "objectID": "week06/assets/walkthrough-fbref.html#r-equivalent",
    "href": "week06/assets/walkthrough-fbref.html#r-equivalent",
    "title": "Walkthrough: Using soccerdata to Fetch Arsenal’s 2023–24 Match Stats",
    "section": "R equivalent",
    "text": "R equivalent\n# 1. Install and load the package\ninstall.packages(\"worldfootballR\")  # different package, same idea\nlibrary(worldfootballR)\n\n# 2. Get Arsenal match results from the Premier League 2023/24\narsenal_matches &lt;- fb_team_match_results(\n  team_url = \"https://fbref.com/en/squads/18bb7c10/Arsenal-Stats\")\n\n# 3. Inspect the data\nhead(arsenal_matches)"
  },
  {
    "objectID": "week06/assets/walkthrough-wb-fred.html",
    "href": "week06/assets/walkthrough-wb-fred.html",
    "title": "Getting GDP data",
    "section": "",
    "text": "We’ll show two options: World Bank and FRED"
  },
  {
    "objectID": "week06/assets/walkthrough-wb-fred.html#world-bank-gdp-per-capita-via-wb-api",
    "href": "week06/assets/walkthrough-wb-fred.html#world-bank-gdp-per-capita-via-wb-api",
    "title": "Getting GDP data",
    "section": "World Bank GDP per capita via WB API",
    "text": "World Bank GDP per capita via WB API\nAPI-based, no key needed, works in both R and Python.\n\nSame data source (World Bank).\nOfficial packages in Python, R\nNo messy HTML scraping.\nClear structure: select indicator → countries → year → get table.\n\n\n📈 GDP per capita (constant 2015 US$), code: NY.GDP.PCAP.KD\n\n\n\nPython (using wbdata or pandas-datareader):\nimport wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# Set countries and indicator\ncountries = ['USA', 'HUN', 'DEU']\nindicator = {'NY.GDP.PCAP.KD': 'GDP_per_capita'}\n\n# Get data\ndf = wbdata.get_dataframe(indicator, country=countries, data_date=datetime(2021, 1, 1))\nprint(df.head())\n\n\n\nR (using wbstats):\ninstall.packages(\"wbstats\")\nlibrary(wbstats)\n\n# Set indicator and countries\ngdp_data &lt;- wb(indicator = \"NY.GDP.PCAP.KD\", country = c(\"US\", \"HU\", \"DE\"),\n               startdate = 2021, enddate = 2021)\n\nhead(gdp_data)"
  },
  {
    "objectID": "week06/assets/walkthrough-wb-fred.html#fred",
    "href": "week06/assets/walkthrough-wb-fred.html#fred",
    "title": "Getting GDP data",
    "section": "FRED",
    "text": "FRED\nFRED (Federal Reserve Economic Data) provides economic time series (e.g. GDP, inflation, interest rates).\n\nYou need an API key, but the process is straightforward\nworks in both R and Python with identical logic in both languages: set key → request series → get dataframe.\n\n\n\n🔑 Get API Key\n\nSign up at https://fred.stlouisfed.org/\nGo to your account settings to get your API key.\nCopy your API key, you’ll need it with code\n\n\n\n\n✅ Example: U.S. GDP per capita (A939RC0Q052SBEA)\nDetails at FRED site\n\n\n\nPython (using fredapi)\nfrom fredapi import Fred\n\nfred = Fred(api_key='your_api_key_here')\n\n# GDP per capita\ngdp_pc = fred.get_series('A939RC0Q052SBEA')\nprint(gdp_pc.tail())\n\n\n\nR (using fredr)\ninstall.packages(\"fredr\")\nlibrary(fredr)\n\nfredr_set_key(\"your_api_key_here\")\n\n# GDP per capita\ngdp_pc &lt;- fredr(series_id = \"A939RC0Q052SBEA\")\nhead(gdp_pc)"
  },
  {
    "objectID": "da-knowledge/terminal-basics.html",
    "href": "da-knowledge/terminal-basics.html",
    "title": "Terminal Basics",
    "section": "",
    "text": "A terminal (also called command line, shell, or console) is a text-based interface to your computer. Instead of clicking icons, you type commands.\nWhy does this matter for Claude Code? Claude Code runs in the terminal. It reads your files, runs code, and shows results - all through text commands.\nDon’t worry: You don’t need to become a terminal expert. Just learn the basics below.",
    "crumbs": [
      "Home",
      "Resources",
      "Terminal Basics"
    ]
  },
  {
    "objectID": "da-knowledge/terminal-basics.html#what-is-a-terminal",
    "href": "da-knowledge/terminal-basics.html#what-is-a-terminal",
    "title": "Terminal Basics",
    "section": "",
    "text": "A terminal (also called command line, shell, or console) is a text-based interface to your computer. Instead of clicking icons, you type commands.\nWhy does this matter for Claude Code? Claude Code runs in the terminal. It reads your files, runs code, and shows results - all through text commands.\nDon’t worry: You don’t need to become a terminal expert. Just learn the basics below.",
    "crumbs": [
      "Home",
      "Resources",
      "Terminal Basics"
    ]
  },
  {
    "objectID": "da-knowledge/terminal-basics.html#opening-the-terminal",
    "href": "da-knowledge/terminal-basics.html#opening-the-terminal",
    "title": "Terminal Basics",
    "section": "Opening the Terminal",
    "text": "Opening the Terminal\n\nWindowsMacLinux\n\n\nOption 1: Command Prompt\n\nPress Win key\nType cmd\nClick “Command Prompt”\n\nOption 2: PowerShell (recommended)\n\nPress Win key\nType powershell\nClick “Windows PowerShell”\n\nOption 3: Windows Terminal (best)\nIf you have Windows 11 or installed Windows Terminal:\n\nPress Win key\nType terminal\nClick “Terminal”\n\n\n\nOption 1: Spotlight\n\nPress Cmd + Space\nType terminal\nPress Enter\n\nOption 2: Finder\n\nOpen Finder\nGo to Applications → Utilities\nDouble-click “Terminal”\n\nTip: Keep Terminal in your Dock for quick access.\n\n\nMost distributions:\n\nPress Ctrl + Alt + T\n\nOr: Search for “Terminal” in your applications menu.",
    "crumbs": [
      "Home",
      "Resources",
      "Terminal Basics"
    ]
  },
  {
    "objectID": "da-knowledge/terminal-basics.html#essential-commands",
    "href": "da-knowledge/terminal-basics.html#essential-commands",
    "title": "Terminal Basics",
    "section": "Essential Commands",
    "text": "Essential Commands\nThese are the only commands you need to know to use Claude Code.\n\nWhere Am I?\nShows your current folder (directory).\n\nWindowsMac / Linux\n\n\ncd\nor\npwd\n(in PowerShell)\n\n\npwd\n\n\n\n\n\nList Files\nSee what’s in the current folder.\n\nWindowsMac / Linux\n\n\ndir\nor\nls\n(in PowerShell)\n\n\nls\nTo see more details:\nls -la\n\n\n\n\n\nChange Folder\nMove to a different folder.\n\nWindowsMac / Linux\n\n\ncd Documents\nGo up one level:\ncd ..\nGo to a specific path:\ncd C:\\Users\\YourName\\Documents\\project\n\n\ncd Documents\nGo up one level:\ncd ..\nGo to a specific path:\ncd /Users/YourName/Documents/project\nGo to home folder:\ncd ~\n\n\n\n\n\nClear the Screen\nClean up the terminal when it gets cluttered.\n\nWindowsMac / Linux\n\n\ncls\n\n\nclear",
    "crumbs": [
      "Home",
      "Resources",
      "Terminal Basics"
    ]
  },
  {
    "objectID": "da-knowledge/terminal-basics.html#paths-finding-your-files",
    "href": "da-knowledge/terminal-basics.html#paths-finding-your-files",
    "title": "Terminal Basics",
    "section": "Paths: Finding Your Files",
    "text": "Paths: Finding Your Files\nA path is the address of a file or folder on your computer.\n\nExamples\n\nWindowsMacLinux\n\n\nC:\\Users\\Anna\\Documents\\data-project\\hotels.csv\nParts: - C: - the drive - \\Users\\Anna\\Documents\\data-project\\ - folders - hotels.csv - the file\n\n\n/Users/Anna/Documents/data-project/hotels.csv\nParts: - /Users/Anna/Documents/data-project/ - folders - hotels.csv - the file\n\n\n/home/anna/Documents/data-project/hotels.csv\nParts: - /home/anna/Documents/data-project/ - folders - hotels.csv - the file\n\n\n\n\n\nTips for Paths\n\nSpaces in names: Use quotes: cd \"My Documents\"\nTab completion: Type part of a name and press Tab to autocomplete\nDrag and drop: On Mac/Windows, you can drag a folder into the terminal to paste its path",
    "crumbs": [
      "Home",
      "Resources",
      "Terminal Basics"
    ]
  },
  {
    "objectID": "da-knowledge/terminal-basics.html#starting-claude-code",
    "href": "da-knowledge/terminal-basics.html#starting-claude-code",
    "title": "Terminal Basics",
    "section": "Starting Claude Code",
    "text": "Starting Claude Code\nOnce you’re in the folder with your data:\nclaude\nThat’s it! Claude Code starts and you can begin typing your requests.\n\nExample Workflow\n\nWindows (PowerShell)MacLinux\n\n\ncd C:\\Users\\Anna\\Documents\\austrian-hotels\nclaude\n\n\ncd ~/Documents/austrian-hotels\nclaude\n\n\ncd ~/Documents/austrian-hotels\nclaude",
    "crumbs": [
      "Home",
      "Resources",
      "Terminal Basics"
    ]
  },
  {
    "objectID": "da-knowledge/terminal-basics.html#common-issues",
    "href": "da-knowledge/terminal-basics.html#common-issues",
    "title": "Terminal Basics",
    "section": "Common Issues",
    "text": "Common Issues\n\n“Command not found” for claude\nClaude Code isn’t installed, or not in your PATH.\nFix: Follow the Claude Code setup guide\n\n\n“No such file or directory”\nYou typed a path that doesn’t exist.\nFix: 1. Use ls (or dir) to see what’s actually in your folder 2. Check spelling 3. Use Tab completion\n\n\n“Permission denied”\nYou don’t have access to that folder/file.\nFix: - On Mac/Linux: Try adding sudo before the command (you’ll need your password) - On Windows: Run terminal as Administrator (right-click → Run as administrator)",
    "crumbs": [
      "Home",
      "Resources",
      "Terminal Basics"
    ]
  },
  {
    "objectID": "da-knowledge/terminal-basics.html#quick-reference-card",
    "href": "da-knowledge/terminal-basics.html#quick-reference-card",
    "title": "Terminal Basics",
    "section": "Quick Reference Card",
    "text": "Quick Reference Card\n\n\n\nTask\nWindows\nMac / Linux\n\n\n\n\nOpen terminal\nWin → type cmd\nCmd+Space → type terminal\n\n\nWhere am I?\ncd\npwd\n\n\nList files\ndir or ls\nls\n\n\nChange folder\ncd foldername\ncd foldername\n\n\nGo up\ncd ..\ncd ..\n\n\nClear screen\ncls\nclear\n\n\nStart Claude\nclaude\nclaude\n\n\nExit Claude\ntype exit or /exit\ntype exit or /exit",
    "crumbs": [
      "Home",
      "Resources",
      "Terminal Basics"
    ]
  },
  {
    "objectID": "da-knowledge/terminal-basics.html#youre-ready",
    "href": "da-knowledge/terminal-basics.html#youre-ready",
    "title": "Terminal Basics",
    "section": "You’re Ready!",
    "text": "You’re Ready!\nThat’s all you need. The terminal might feel unfamiliar at first, but after using it a few times with Claude Code, it becomes natural.\nNext step: Set up Claude Code",
    "crumbs": [
      "Home",
      "Resources",
      "Terminal Basics"
    ]
  },
  {
    "objectID": "da-knowledge/setting-up-claude-code.html",
    "href": "da-knowledge/setting-up-claude-code.html",
    "title": "Setting Up Claude Code",
    "section": "",
    "text": "New to the terminal? Check out Terminal Basics first.\nWant alternatives? See Installing AI CLI Tools for Gemini CLI, Codex CLI, and more.",
    "crumbs": [
      "Home",
      "Resources",
      "Setting Up Claude Code"
    ]
  },
  {
    "objectID": "da-knowledge/setting-up-claude-code.html#what-is-claude-code",
    "href": "da-knowledge/setting-up-claude-code.html#what-is-claude-code",
    "title": "Setting Up Claude Code",
    "section": "What is Claude Code?",
    "text": "What is Claude Code?\nClaude Code is a command-line tool made by Anthropic that lets you work with Claude AI directly in your terminal (the text-based interface on your computer). Instead of chatting in a browser, you type commands and Claude helps you with coding, file editing, data analysis, and more — right where your code lives.\nWhy use it?\n\nWorks directly with your files and folders\nCan read, edit, and create code for you\nRuns commands and sees the output\nGreat for data analysis workflows\nNo copy-pasting between browser and code editor",
    "crumbs": [
      "Home",
      "Resources",
      "Setting Up Claude Code"
    ]
  },
  {
    "objectID": "da-knowledge/setting-up-claude-code.html#installation-instructions",
    "href": "da-knowledge/setting-up-claude-code.html#installation-instructions",
    "title": "Setting Up Claude Code",
    "section": "Installation Instructions",
    "text": "Installation Instructions\nChoose your operating system:\n\nWindows\nMac\nLinux",
    "crumbs": [
      "Home",
      "Resources",
      "Setting Up Claude Code"
    ]
  },
  {
    "objectID": "da-knowledge/setting-up-claude-code.html#windows",
    "href": "da-knowledge/setting-up-claude-code.html#windows",
    "title": "Setting Up Claude Code",
    "section": "Windows",
    "text": "Windows\n\nStep 1: Install Node.js\nClaude Code needs Node.js (a program that runs JavaScript) to work.\n\nGo to nodejs.org\nDownload the LTS version (the button on the left)\nRun the installer\nClick Next through all the steps (default settings are fine)\nCheck the box for “Automatically install necessary tools” if asked\nClick Install, then Finish\n\n\n\nStep 2: Open Terminal\n\nPress the Windows key on your keyboard\nType cmd\nClick on Command Prompt\n\nA black window with white text will open. This is your terminal.\n\n\nStep 3: Install Claude Code\nIn the terminal window, type this exactly and press Enter:\nnpm install -g @anthropic-ai/claude-code\nWait for it to finish (may take 1-2 minutes). You’ll see some text scrolling.\n\n\nStep 4: Start Claude Code\nIn the same terminal, type:\nclaude\nThe first time, it will ask you to log in with your Anthropic account (the same one you use for claude.ai). Follow the instructions on screen.\n\n\nYou’re done!\nType your questions or requests and press Enter. Type exit to quit.",
    "crumbs": [
      "Home",
      "Resources",
      "Setting Up Claude Code"
    ]
  },
  {
    "objectID": "da-knowledge/setting-up-claude-code.html#mac",
    "href": "da-knowledge/setting-up-claude-code.html#mac",
    "title": "Setting Up Claude Code",
    "section": "Mac",
    "text": "Mac\n\nStep 1: Install Node.js\n\nGo to nodejs.org\nDownload the LTS version (the button on the left)\nOpen the downloaded .pkg file\nFollow the installer steps (click Continue, then Install)\nEnter your Mac password when asked\n\n\n\nStep 2: Open Terminal\n\nPress Command + Space to open Spotlight\nType Terminal\nPress Enter\n\nA window with text will open. This is your terminal.\n\n\nStep 3: Install Claude Code\nIn the terminal window, type this exactly and press Enter:\nnpm install -g @anthropic-ai/claude-code\nWait for it to finish (may take 1-2 minutes).\nIf you get a permission error, type this instead:\nsudo npm install -g @anthropic-ai/claude-code\nEnter your Mac password when asked (you won’t see the characters as you type — that’s normal).\n\n\nStep 4: Start Claude Code\nIn the same terminal, type:\nclaude\nThe first time, it will ask you to log in with your Anthropic account. Follow the instructions on screen.\n\n\nYou’re done!\nType your questions or requests and press Enter. Type exit to quit.",
    "crumbs": [
      "Home",
      "Resources",
      "Setting Up Claude Code"
    ]
  },
  {
    "objectID": "da-knowledge/setting-up-claude-code.html#linux",
    "href": "da-knowledge/setting-up-claude-code.html#linux",
    "title": "Setting Up Claude Code",
    "section": "Linux",
    "text": "Linux\n\nStep 1: Install Node.js\nOpen your terminal and run these commands:\nUbuntu/Debian:\nsudo apt update\nsudo apt install nodejs npm\nFedora:\nsudo dnf install nodejs npm\nArch:\nsudo pacman -S nodejs npm\n\n\nStep 2: Install Claude Code\nnpm install -g @anthropic-ai/claude-code\nIf you get a permission error:\nsudo npm install -g @anthropic-ai/claude-code\n\n\nStep 3: Start Claude Code\nclaude\nLog in with your Anthropic account when prompted.\n\n\nYou’re done!",
    "crumbs": [
      "Home",
      "Resources",
      "Setting Up Claude Code"
    ]
  },
  {
    "objectID": "da-knowledge/setting-up-claude-code.html#quick-tips",
    "href": "da-knowledge/setting-up-claude-code.html#quick-tips",
    "title": "Setting Up Claude Code",
    "section": "Quick Tips",
    "text": "Quick Tips\n\n\n\nCommand\nWhat it does\n\n\n\n\nclaude\nStart Claude Code\n\n\nexit\nQuit Claude Code\n\n\nclaude --help\nSee all options\n\n\n/help\nGet help while inside Claude",
    "crumbs": [
      "Home",
      "Resources",
      "Setting Up Claude Code"
    ]
  },
  {
    "objectID": "da-knowledge/setting-up-claude-code.html#requirements",
    "href": "da-knowledge/setting-up-claude-code.html#requirements",
    "title": "Setting Up Claude Code",
    "section": "Requirements",
    "text": "Requirements\n\nAnthropic account — Sign up at claude.ai if you don’t have one\nClaude Pro or API access — Claude Code requires a paid subscription or API credits\nInternet connection — Claude runs in the cloud",
    "crumbs": [
      "Home",
      "Resources",
      "Setting Up Claude Code"
    ]
  },
  {
    "objectID": "da-knowledge/setting-up-claude-code.html#troubleshooting",
    "href": "da-knowledge/setting-up-claude-code.html#troubleshooting",
    "title": "Setting Up Claude Code",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n“npm not found” — Node.js didn’t install correctly. Restart your terminal and try again, or reinstall Node.js.\n“Permission denied” — On Mac/Linux, add sudo before the command. On Windows, right-click Command Prompt and select “Run as administrator”.\n“Authentication failed” — Make sure you’re using the same account you use for claude.ai and that you have an active subscription.",
    "crumbs": [
      "Home",
      "Resources",
      "Setting Up Claude Code"
    ]
  },
  {
    "objectID": "da-knowledge/setting-up-claude-code.html#learn-more",
    "href": "da-knowledge/setting-up-claude-code.html#learn-more",
    "title": "Setting Up Claude Code",
    "section": "Learn More",
    "text": "Learn More\n\nClaude Code Documentation\nAnthropic Console — for API keys and usage",
    "crumbs": [
      "Home",
      "Resources",
      "Setting Up Claude Code"
    ]
  },
  {
    "objectID": "da-knowledge/joining-tables.html",
    "href": "da-knowledge/joining-tables.html",
    "title": "Joining Data Tables",
    "section": "",
    "text": "Most real-world data analysis involves combining information from multiple tables. Customer data lives in one table, their orders in another. Hotels are in one file, their monthly occupancy in another.\nJoining tables is how we bring this information together. It’s a fundamental skill - and one where mistakes are easy to make and hard to spot.\nThis page covers the essentials. If you’ve done this before, skim for a refresher. If it’s new, read carefully - we’ll use these concepts throughout the course.",
    "crumbs": [
      "Home",
      "Resources",
      "Joining Data Tables"
    ]
  },
  {
    "objectID": "da-knowledge/joining-tables.html#why-this-matters",
    "href": "da-knowledge/joining-tables.html#why-this-matters",
    "title": "Joining Data Tables",
    "section": "",
    "text": "Most real-world data analysis involves combining information from multiple tables. Customer data lives in one table, their orders in another. Hotels are in one file, their monthly occupancy in another.\nJoining tables is how we bring this information together. It’s a fundamental skill - and one where mistakes are easy to make and hard to spot.\nThis page covers the essentials. If you’ve done this before, skim for a refresher. If it’s new, read carefully - we’ll use these concepts throughout the course.",
    "crumbs": [
      "Home",
      "Resources",
      "Joining Data Tables"
    ]
  },
  {
    "objectID": "da-knowledge/joining-tables.html#key-vocabulary",
    "href": "da-knowledge/joining-tables.html#key-vocabulary",
    "title": "Joining Data Tables",
    "section": "Key Vocabulary",
    "text": "Key Vocabulary\n\nKeys\n\nPrimary Key: A column (or columns) that uniquely identifies each row. Example: hotel_id in a hotels table - each hotel has exactly one ID.\nForeign Key: A column that references a primary key in another table. Example: hotel_id in a reviews table - it points to which hotel the review is about.\nJoin Key: The column(s) you use to match rows between tables. Often a foreign key joined to a primary key.\nComposite Key: When you need multiple columns together to uniquely identify a row. Example: hotel_id + month + year for monthly occupancy data.\n\n\n\nRelationship Types\n\nOne-to-One: Each row in Table A matches exactly one row in Table B. Example: hotels and their addresses.\nOne-to-Many: Each row in Table A matches multiple rows in Table B. Example: one hotel has many reviews.\nMany-to-Many: Multiple rows match in both directions. Example: hotels and amenities (each hotel has many amenities; each amenity exists in many hotels). Usually handled with a linking table.",
    "crumbs": [
      "Home",
      "Resources",
      "Joining Data Tables"
    ]
  },
  {
    "objectID": "da-knowledge/joining-tables.html#join-types",
    "href": "da-knowledge/joining-tables.html#join-types",
    "title": "Joining Data Tables",
    "section": "Join Types",
    "text": "Join Types\nThere are four main types of joins. The difference is what happens when rows don’t match.\n\nInner Join\nReturns only rows that match in both tables.\nTable A: Hotels          Table B: Ratings\nhotel_id | name          hotel_id | stars\n---------|-----          ---------|------\n1        | Sacher        1        | 5\n2        | Imperial      2        | 5\n3        | Fuschl        5        | 4\n\nInner Join Result:\nhotel_id | name     | stars\n---------|----------|------\n1        | Sacher   | 5\n2        | Imperial | 5\nHotel 3 disappears (no rating). Hotel 5’s rating disappears (no hotel info).\nUse when: You only want complete records with data from both tables.\n\n\nLeft Join\nReturns all rows from the left table, plus matching data from the right. Non-matches get NULL.\nLeft Join Result (Hotels LEFT JOIN Ratings):\nhotel_id | name     | stars\n---------|----------|------\n1        | Sacher   | 5\n2        | Imperial | 5\n3        | Fuschl   | NULL\nHotel 3 is kept, but has no star rating.\nUse when: You want to keep all records from your main table, adding information where available.\n\n\nRight Join\nReturns all rows from the right table, plus matching data from the left. The mirror of left join.\nUse when: Same as left join, just with tables in different order. Most people just use left join and swap table order.\n\n\nFull (Outer) Join\nReturns all rows from both tables. Non-matches get NULL on the missing side.\nFull Join Result:\nhotel_id | name     | stars\n---------|----------|------\n1        | Sacher   | 5\n2        | Imperial | 5\n3        | Fuschl   | NULL\n5        | NULL     | 4\nUse when: You need everything and want to see what’s missing on each side.",
    "crumbs": [
      "Home",
      "Resources",
      "Joining Data Tables"
    ]
  },
  {
    "objectID": "da-knowledge/joining-tables.html#one-to-many-joins",
    "href": "da-knowledge/joining-tables.html#one-to-many-joins",
    "title": "Joining Data Tables",
    "section": "One-to-Many Joins",
    "text": "One-to-Many Joins\nWhen one row matches multiple rows, the result table has more rows than you started with.\nHotels (3 rows)              Reviews (6 rows)\nhotel_id | name              review_id | hotel_id | rating\n---------|-----              ----------|----------|-------\n1        | Sacher            101       | 1        | 4.8\n2        | Imperial          102       | 1        | 4.7\n3        | Fuschl            103       | 1        | 4.9\n                             104       | 2        | 4.6\n                             105       | 2        | 4.8\n                             106       | 3        | 4.5\n\nLeft Join Result (6 rows):\nhotel_id | name     | review_id | rating\n---------|----------|-----------|-------\n1        | Sacher   | 101       | 4.8\n1        | Sacher   | 102       | 4.7\n1        | Sacher   | 103       | 4.9\n2        | Imperial | 104       | 4.6\n2        | Imperial | 105       | 4.8\n3        | Fuschl   | 106       | 4.5\nWatch out: Hotel information is now duplicated. If you calculate mean(stars) without thinking, you’ll weight hotels with more reviews more heavily.",
    "crumbs": [
      "Home",
      "Resources",
      "Joining Data Tables"
    ]
  },
  {
    "objectID": "da-knowledge/joining-tables.html#composite-key-joins",
    "href": "da-knowledge/joining-tables.html#composite-key-joins",
    "title": "Joining Data Tables",
    "section": "Composite Key Joins",
    "text": "Composite Key Joins\nSometimes you need multiple columns to make a match.\nHotels                      Monthly Occupancy\nhotel_id | name             hotel_id | month | year | occupancy\n---------|-----             ---------|-------|------|----------\n1        | Sacher           1        | 1     | 2024 | 0.85\n2        | Imperial         1        | 2     | 2024 | 0.78\n                            2        | 1     | 2024 | 0.82\nTo join these properly, you’d join on hotel_id alone - but the result will have multiple rows per hotel (one per month).\nIf you had a separate table with city-level monthly data:\nCity Tourism\ncity   | month | year | tourists\n-------|-------|------|----------\nVienna | 1     | 2024 | 500000\nVienna | 2     | 2024 | 450000\nYou’d need to join on city + month + year together.",
    "crumbs": [
      "Home",
      "Resources",
      "Joining Data Tables"
    ]
  },
  {
    "objectID": "da-knowledge/joining-tables.html#common-mistakes",
    "href": "da-knowledge/joining-tables.html#common-mistakes",
    "title": "Joining Data Tables",
    "section": "Common Mistakes",
    "text": "Common Mistakes\n\n1. Unexpected Row Counts\nProblem: Your joined table has way more (or fewer) rows than expected.\nCause: Usually a one-to-many relationship you didn’t anticipate, or join keys that don’t match properly.\nFix: Always check row counts before and after joining:\nprint(f\"Hotels: {len(hotels)}, Occupancy: {len(occupancy)}\")\nresult = hotels.merge(occupancy, on='hotel_id')\nprint(f\"Joined: {len(result)}\")\n\n\n2. Losing Data Silently\nProblem: Rows disappear and you don’t notice.\nCause: Inner join drops non-matching rows. If your keys have typos or different formats, matches fail silently.\nFix: Use left join and check for NULLs:\nresult = hotels.merge(ratings, on='hotel_id', how='left')\nprint(f\"Missing ratings: {result['stars'].isna().sum()}\")\n\n\n3. Duplicate Column Names\nProblem: Both tables have a column called name or date.\nCause: Join keeps both, often with suffixes like name_x and name_y.\nFix: Rename columns before joining, or select only the columns you need.\n\n\n4. Wrong Join Key Type\nProblem: Join returns no matches even though the data looks right.\nCause: One table has hotel_id as integer, another as string. 1 != \"1\".\nFix: Check and convert types:\nhotels['hotel_id'] = hotels['hotel_id'].astype(int)\nratings['hotel_id'] = ratings['hotel_id'].astype(int)",
    "crumbs": [
      "Home",
      "Resources",
      "Joining Data Tables"
    ]
  },
  {
    "objectID": "da-knowledge/joining-tables.html#quick-reference",
    "href": "da-knowledge/joining-tables.html#quick-reference",
    "title": "Joining Data Tables",
    "section": "Quick Reference",
    "text": "Quick Reference\n\n\n\nJoin Type\nKeeps from Left\nKeeps from Right\nUse Case\n\n\n\n\nInner\nOnly matches\nOnly matches\nComplete records only\n\n\nLeft\nAll\nOnly matches\nKeep all from main table\n\n\nRight\nOnly matches\nAll\n(Same as left, reversed)\n\n\nFull\nAll\nAll\nSee everything, find gaps",
    "crumbs": [
      "Home",
      "Resources",
      "Joining Data Tables"
    ]
  },
  {
    "objectID": "da-knowledge/joining-tables.html#code-examples",
    "href": "da-knowledge/joining-tables.html#code-examples",
    "title": "Joining Data Tables",
    "section": "Code Examples",
    "text": "Code Examples\n\nPython (pandas)R (dplyr)SQLStata\n\n\nimport pandas as pd\n\n# Inner join\nresult = hotels.merge(ratings, on='hotel_id', how='inner')\n\n# Left join\nresult = hotels.merge(ratings, on='hotel_id', how='left')\n\n# Join on multiple columns\nresult = occupancy.merge(tourism, on=['city', 'month', 'year'])\n\n# Check for issues\nprint(f\"Rows before: {len(hotels)}, after: {len(result)}\")\nprint(f\"NULLs created: {result.isna().any(axis=1).sum()}\")\n\n\nlibrary(dplyr)\n\n# Inner join\nresult &lt;- inner_join(hotels, ratings, by = \"hotel_id\")\n\n# Left join\nresult &lt;- left_join(hotels, ratings, by = \"hotel_id\")\n\n# Join on multiple columns\nresult &lt;- left_join(occupancy, tourism, by = c(\"city\", \"month\", \"year\"))\n\n# Check for issues\ncat(\"Rows before:\", nrow(hotels), \"after:\", nrow(result), \"\\n\")\ncat(\"NAs created:\", sum(!complete.cases(result)), \"\\n\")\n\n\n-- Inner join\nSELECT * FROM hotels h\nINNER JOIN ratings r ON h.hotel_id = r.hotel_id;\n\n-- Left join\nSELECT * FROM hotels h\nLEFT JOIN ratings r ON h.hotel_id = r.hotel_id;\n\n-- Join on multiple columns\nSELECT * FROM occupancy o\nLEFT JOIN tourism t\n  ON o.city = t.city\n  AND o.month = t.month\n  AND o.year = t.year;\n\n\n* Load datasets\nuse hotels, clear\ncount  // Check rows before\n\n* Inner join (1:1)\nmerge 1:1 hotel_id using ratings\nkeep if _merge == 3  // Keep only matched\ndrop _merge\n\n* Left join (1:1) - keep all from master\nuse hotels, clear\nmerge 1:1 hotel_id using ratings, keep(master match)\ndrop _merge\n\n* Left join (1:m) - one hotel, many reviews\nuse hotels, clear\nmerge 1:m hotel_id using reviews, keep(master match)\ntab _merge  // Check match results\ndrop _merge\n\n* Join on multiple columns\nuse occupancy, clear\nmerge m:1 city month year using tourism, keep(master match)\ndrop _merge\n\n* Check for issues\ncount  // Rows after\nmisstab  // Check missing values (requires misstab package)\nStata notes:\n\nmerge 1:1 for one-to-one, merge 1:m for one-to-many, merge m:1 for many-to-one\n_merge variable shows: 1 = master only, 2 = using only, 3 = matched\nkeep(master match) is like a left join\nkeep(match) is like an inner join",
    "crumbs": [
      "Home",
      "Resources",
      "Joining Data Tables"
    ]
  },
  {
    "objectID": "da-knowledge/joining-tables.html#further-reading",
    "href": "da-knowledge/joining-tables.html#further-reading",
    "title": "Joining Data Tables",
    "section": "Further Reading",
    "text": "Further Reading\n\nTidy Data (Hadley Wickham) - The principles behind good data structure\npandas merge documentation\ndplyr join documentation",
    "crumbs": [
      "Home",
      "Resources",
      "Joining Data Tables"
    ]
  },
  {
    "objectID": "da-knowledge/beyond.html",
    "href": "da-knowledge/beyond.html",
    "title": "Beyond: suggested readings and resources to learn more",
    "section": "",
    "text": "Learning more\n\n\nBeyond: Suggested readings and resources to learn more\nGábor’s collection of recommended readings, listening. Wide variety from practical to business and nerdy stuff.",
    "crumbs": [
      "Home",
      "Resources",
      "Learn More"
    ]
  },
  {
    "objectID": "da-knowledge/beyond.html#basics",
    "href": "da-knowledge/beyond.html#basics",
    "title": "Beyond: suggested readings and resources to learn more",
    "section": "Basics",
    "text": "Basics\n\nCore readings\n\nEthan Mollick “Co-Intelligence: Living and Working with AI” Penguin Random House 2024\nAnton Korinek “Generative AI for Economic Research: Use Cases and Implications for Economists,” Journal of Economic Literature 61(4) December 2024 Update 1–74\n\n\n\nImportant reviews\n\nReview of LLMs by Simon Willison\nMachines of Loving Grace Dario Amodei\n\n\n\nPrompting and context management\n\nAI Frontiers in Plain English: Prompt Engineering guide from Google with LM Notebook Part 1. LLM output configurations + others\n\n\n\nUnderstanding LLMs\n\nGlossary of LLM terms Glossary of LLM Terms\nFinancial Times: How AI Large Language Models Work\nThe Economist: How Large Language Models Work\nThinking like AI\nWhat’s an LLM context window and why is it getting larger? IBM research on context window\n\n\n\nAI and business / management\n\nStrategy in business Build a winning AI strategy, HBR 2023\nInterview with Rafella Sadun on Reskilling workforce with AI from MIT Sloan Review",
    "crumbs": [
      "Home",
      "Resources",
      "Learn More"
    ]
  },
  {
    "objectID": "da-knowledge/beyond.html#new-methods-and-tools",
    "href": "da-knowledge/beyond.html#new-methods-and-tools",
    "title": "Beyond: suggested readings and resources to learn more",
    "section": "New methods and tools",
    "text": "New methods and tools\n\nBest video series on introduction to neural networks and LLMs. [3blue1brown youtube playlist] (https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) 7 times 20-30mins videos, from zero to conversational understanding\nVery usefil video on skills and Claude’s skill creator.\nAgents – Google’s seminar on agents at work – requires registration",
    "crumbs": [
      "Home",
      "Resources",
      "Learn More"
    ]
  },
  {
    "objectID": "da-knowledge/beyond.html#claude-code-and-clis",
    "href": "da-knowledge/beyond.html#claude-code-and-clis",
    "title": "Beyond: suggested readings and resources to learn more",
    "section": "Claude Code and CLIs",
    "text": "Claude Code and CLIs\n\nIntro videos and use\n\nGetting started with Claude Code 2.0 – great 30 mins intro by Anthropic’s dev\nSankalp on Claude Code – list – great collection of links and ideas",
    "crumbs": [
      "Home",
      "Resources",
      "Learn More"
    ]
  },
  {
    "objectID": "da-knowledge/beyond.html#additional-content",
    "href": "da-knowledge/beyond.html#additional-content",
    "title": "Beyond: suggested readings and resources to learn more",
    "section": "Additional content",
    "text": "Additional content\n\nConsequence of AI\nHow NLP was killed by Transformers/ LLMs in Quant magazine 2025 April\nAmazing compilation of videos by leading AI Lab founders and leaders on the future of AI by Nature, published on 14 November 2025. Mustafa Suleyman on fears is one of my favourite short vids here.\n\n\nBlogs, newsletters\n\nBlog post by Posit Text Summarization, Translation, and Classification using LLMs: mall does it all\nSimon Willis blog post LLM and coding\nEthan Mollick substack: One useful thing\nAlpha Signal newsletter\nHow Andrej Karpathy is adopting AI assisted coding.\nBlog by Anthropic Claude explains – cool collection on ideas for coding with Claude\n\n\n\nVideo Resources on AI\n\nAndrej Karpathy Introduction to Large Language Models – 1hs overview, a great start\nAndrej Karpathy Deep Dive into LLMs like ChatGPT – 3hs comprehensive updated version of the Intro video\nAndrej Karpathy: “Let’s build GPT: from scratch, in code, spelled out”\nInterview with a great Sendhil Mullainathan on direction AI",
    "crumbs": [
      "Home",
      "Resources",
      "Learn More"
    ]
  },
  {
    "objectID": "da-knowledge/beyond.html#deeper-stuff-on-ai",
    "href": "da-knowledge/beyond.html#deeper-stuff-on-ai",
    "title": "Beyond: suggested readings and resources to learn more",
    "section": "Deeper stuff on AI",
    "text": "Deeper stuff on AI\n\nArtificial intelligence learns to reason in Science",
    "crumbs": [
      "Home",
      "Resources",
      "Learn More"
    ]
  },
  {
    "objectID": "week01/index.html",
    "href": "week01/index.html",
    "title": "Week 01: LLM Review",
    "section": "",
    "text": "Week 01: LLM Review\n\n\nIntroduction to Large Language Models and their applications in data analysis",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#learning-objectives",
    "href": "week01/index.html#learning-objectives",
    "title": "Week 01: LLM Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, students will:\n\nUnderstand core concepts of large language models (LLMs)\nDistinguish between “Cyborg” and “Centaur” approaches to AI collaboration\nExperience the “jagged frontier” of LLM capabilities through hands-on practice\nCritically assess capabilities and limitations of AI tools in academic contexts",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#preparation-before-class",
    "href": "week01/index.html#preparation-before-class",
    "title": "Week 01: LLM Review",
    "section": "Preparation / Before Class",
    "text": "Preparation / Before Class\n\n\n📚 No Required Reading\n\n\nThis is Week 1 - come ready to explore and discuss!\nReflection Preparation:\n\nThink about your current experience with AI tools (if any)\nConsider examples where you’ve encountered AI in your work/studies\nIdentify one data analysis task you find time-consuming or repetitive\n\nOptional Background:\n\nEthan Mollick: “Co-Intelligence: Living and Working with AI” (Chapters 1-2)",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#class-material",
    "href": "week01/index.html#class-material",
    "title": "Week 01: LLM Review",
    "section": "Class Material",
    "text": "Class Material\n\n\n📊 Core LLM Concepts (60 min)\n\n\nSlideshow: LLM Concepts and Applications\nKey Topics Covered:\n\nWhat are LLMs? Statistical models predicting next tokens from massive training data\nThe Transformer Revolution: How 2017’s “Attention is All You Need” changed everything\nContext Windows: why this matters for data analysis\nTraining Process: Resources and human feedback\nCollaborative Frameworks: How to integrate human and AI work\n\n\n\n\n\n🔧 Hands-on: Data search (15 min)\n\n\nExample\nTry this in a chat\nGet me an income dataset by planning regions (county) in Connecticut for 2023. Present the results as a table I can copy and edit..\n\nNow try find the actual data and compare.\nTry multiple times in different models\nTry modifying the prompt to be more specific or different",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#discussion-questions",
    "href": "week01/index.html#discussion-questions",
    "title": "Week 01: LLM Review",
    "section": "Discussion Questions",
    "text": "Discussion Questions\nEnd of Week Reflection:\n\nPersonal AI Experience: How have you already incorporated AI into your routine? Which model feels most natural to you?\nError Management: How do you currently deal with AI hallucinations or imperfect answers? What strategies emerged during the FT graph exercise?\nThe Jagged Frontier: What tasks do you expect AI to excel at? Where do you think it will struggle? Did the visualization exercise match your expectations?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#assignment",
    "href": "week01/index.html#assignment",
    "title": "Week 01: LLM Review",
    "section": "Assignment",
    "text": "Assignment\n\n\n\n\n\n\nNoteAssignment 1: Reproduce the FT Graph\n\n\n\nDue: Sunday 23.55 before Week 2 (on moodle)\nFull Assignment Details",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#background-tools-and-resources",
    "href": "week01/index.html#background-tools-and-resources",
    "title": "Week 01: LLM Review",
    "section": "Background, Tools and Resources",
    "text": "Background, Tools and Resources\nAI Model Selection Guide\nGlossary of key terms\n\nAcademic Integrity and AI use\nCourse Philosophy:\n\nAI as Assistant: Use AI to enhance your capabilities, not replace your thinking\nMaintain Authority: You remain responsible for all outputs and interpretations\nVerify Everything: Always validate AI suggestions, especially statistical claims\nDocument Usage: Keep track of how AI helped – to learn (and for transparency)\n\nRed Lines:\n\nNever submit unverified AI output as your own work\nAlways understand the analysis you’re presenting\nAvoid over-reliance on AI for critical thinking or interpretation (you be you)\n\nThe Goal:\nBecome a more capable data analyst who can leverage AI tools effectively while maintaining scientific rigor.\nNext Week:\nWeek 2 - Data Discovery and Documentation where we’ll use AI to understand and document complex datasets.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#read-more",
    "href": "week01/index.html#read-more",
    "title": "Week 01: LLM Review",
    "section": "Read more",
    "text": "Read more\n\nEthan Mollick update on Jagged Frontier",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#some-personal-comments-on-ai-and-this-class",
    "href": "week01/index.html#some-personal-comments-on-ai-and-this-class",
    "title": "Week 01: LLM Review",
    "section": "Some personal comments on AI and this class",
    "text": "Some personal comments on AI and this class\n\nI needed to rewrite, edit the slideshow frequently. Bloody hell, this course material is tricky. (While Gosset’s t-test has been around since 1908…)\nIt was an AI (Claude Sonnet 4.0) that suggest to include the last bit on Academic Integrity and red lines. Hahh.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "assignments/assignment_05.html",
    "href": "assignments/assignment_05.html",
    "title": "Student-Specific Sentiment Analysis",
    "section": "",
    "text": "Task\nStudent-Specific Sentiment Analysis of a series of texts – Compare manual ratings with AI-generated ratings.\nSteps:\n\nDownload the text file of interviews from the Football Manager Interviews case study. Randomly generate a series of 25 numbers between 1 and 121 (without replacement). Those will be your interviews to rate. Filter those interviews and save the file as lastname_firstname_sentiment_rated.xlsx.\nGo through your interviews and manually rate managers’ sentiment (25 total).\nInput full manager interviews (without your rating) into the AI of choice and obtain and compare AI-generated sentiment ratings to initial manual assessments.\nFor both exercises, incorporate the guidelines HERE\nCompare your manual and AI generated ratings, and write one paragraph summarizing similarities and deviation.\n\n\n\nSubmit:\n\nA file with your manual ratings and AI generated ratings: lastname_firstname_sentiment_rated.xlsx.\nA text file with the one paragraph summarizing similarities and deviation: lastname_firstname_sentiment_eval.txt/"
  },
  {
    "objectID": "assignments/assignment_06.html",
    "href": "assignments/assignment_06.html",
    "title": "Gender guessing Analysis Pipeline",
    "section": "",
    "text": "Task\nCreate a similar pipeline we used in class but now with the aim to decide if the coach is a man or a women. In this example dataset, men teams have men coach and women teams have women coach.\nPick a few texts and do it yourself\nGet AI explain it’s decisions\n\n\nSubmit:\n\nA file with AI generated ratings: lastname_firstname_geneder_text.xlsx.\nA short para on why this could be easy or hard lastname_firstname_geneder_text.txt."
  },
  {
    "objectID": "assignments/assignment_08.html",
    "href": "assignments/assignment_08.html",
    "title": "Design an IV",
    "section": "",
    "text": "Task: Design an IV\nThink about an identification problem and interate to find a good IV. Explain why it may work and why could it fail.\nDiscuss what data would be needed and where it would come."
  },
  {
    "objectID": "week03/index.html",
    "href": "week03/index.html",
    "title": "Week 03: Reporting your data analysis",
    "section": "",
    "text": "Week 03: Reporting your data analysis\n\n\nCreating inputs and organizing a short data analytics report with AI",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 3: Reporting"
    ]
  },
  {
    "objectID": "week03/index.html#objectives",
    "href": "week03/index.html#objectives",
    "title": "Week 03: Reporting your data analysis",
    "section": "Objectives",
    "text": "Objectives\n\nSummary:\nHow to organize a short data analytics report? The job includes choosing and creating relevant plots, running regression. The class will exlopre how we can use AI to assist in these tasks, and get the distinction between using AI as input vs. output in report writing.\n\n\nDetails\n\nUnderstand how to connect an empirical question to data\nCreate relevant visualizations and tables using AI.\nLearn to critically assess reports with the help of AI tools. We’ll especially focus on half-truths: convincing, looks okay, is okay-ish, but not 100% true",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 3: Reporting"
    ]
  },
  {
    "objectID": "week03/index.html#preparation-before-class",
    "href": "week03/index.html#preparation-before-class",
    "title": "Week 03: Reporting your data analysis",
    "section": "Preparation BEFORE class",
    "text": "Preparation BEFORE class\n\nBackground reading: Békés-Kézdi (2021) Chapters 3-4, 7-10. Pay special attention to Chapter 4’s discussion of good vs. bad graphs and Chapter 10 on interpreting regression coefficients precisely.\nDownload the WVS_GDP_merged_data.csv from the World Values Survey case study. This is an aggregated, cleaned subset of the 7th Wave of WVS dataset merged with GDP data from World Bank\n\n\nThe data\n\nThis is aggregated data: country level\nYear: Wave 7 of the WVS – survey was conducted at different years.\nCombined with World Bank data: at year when survey was conducted\nGDP: level USD, level USD PPP, level USD PPP per capita.\npopulation",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 3: Reporting"
    ]
  },
  {
    "objectID": "week03/index.html#class-plan",
    "href": "week03/index.html#class-plan",
    "title": "Week 03: Reporting your data analysis",
    "section": "Class plan",
    "text": "Class plan\n\n\n🤝 Review on readme docs (20 min)\n\n\n\nReview Readme assignment\n\nQ+A\nCommon AI errors in data documentation?\nVerify, verify, verify (AI is good but rarely perfect)\n\n\n\nHow does a good report look like?\n\nHow to write a good short report: structure\ngood graphs and tables\n\nMake sure precise language. Recap on causal language.\n\n\n\n\nProblems with AI generated reports.\nUse AI as input (like advanced google search) not as output writer, because\n\nCreates “average” / generic / bland / repetitive text\nNot your style and not your exact plan\nConvincing but would have errors especially with info outside of context window\nnot precise enough, especially using causal language inappropriately.\nToo broad (like adds further research)\n\n\n\n\n\n\n🤝 No AI vs AI prompting strategies to create a report (50 min)\n\n\n\nNO AI\nForm 2-3 member groups freely\n\nEach group: Choose one these pre-defined research questions:\n\n\nIs there a relationship between income level and trust?\nIs there a relationship between income level and happiness?\nIs there a relationship between income level and gender attitudes?\n\n\nChoose the relevant variables to answer your question (you can use AI to understand variables like in week2 (Use your Week 2 AI skills to understand complex variable definitions.)\nDesign a plan for a report on the topic: list of exhibits (graphs, tables). Do not write code (yet)\nDiscuss plans\n\n\n\nAI 1\nTry get a report with a single prompt. * Hint 1: translate your plan into a prompt using ideas from the intro. * Hint 2: Look for impressive-looking but problematic results.\n\n\nAI 2\n\nShowcase an iterative process where key exhibits are created\nHint 3: get AI to create precisely the exhibits you designed, not what it thinks you need.\n\n\n\nAI 3\n\ncreate a system prompt where summarize preferences, and add it to the context window. Repeat the one shot version.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 3: Reporting"
    ]
  },
  {
    "objectID": "week03/index.html#end-of-week-discussion-points",
    "href": "week03/index.html#end-of-week-discussion-points",
    "title": "Week 03: Reporting your data analysis",
    "section": "End of Week Discussion points",
    "text": "End of Week Discussion points\n\nCompare single and multi-step approach generating reports?\nHow good is AI in creating good enough vs exactly as planned graphs?\nWhat is happiness? :-)",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 3: Reporting"
    ]
  },
  {
    "objectID": "week03/index.html#assignment",
    "href": "week03/index.html#assignment",
    "title": "Week 03: Reporting your data analysis",
    "section": "Assignment",
    "text": "Assignment\n\n\n\n\n\n\nNote\n\n\n\nAssignment 3: Creating a Report\nCritical Note: Reports will be shared for peer review in Week 4 - upload to Moodle student folder.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 3: Reporting"
    ]
  },
  {
    "objectID": "week03/index.html#some-personal-comments-on-ai-and-this-class",
    "href": "week03/index.html#some-personal-comments-on-ai-and-this-class",
    "title": "Week 03: Reporting your data analysis",
    "section": "Some personal comments on AI and this class",
    "text": "Some personal comments on AI and this class\n\nThe AI (Claude 4.0) suggested to add “Pay special attention to Chapter 4’s discussion of good vs. bad graphs and Chapter 10 on interpreting regression coefficients precisely.” Well. We indeed have bits on good graphs in Chapter4 but not really on bad ones. It is Chapter 07 that builds coefficient estimation, albeit we have some stuff in Chapter 10 for multiple coefficients. I asked AI about errors. It noted that it predicted them based on knowing about the core summary of our book + contents of”typical econometrics textbook”. Instructive re half-truth danger.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 3: Reporting"
    ]
  },
  {
    "objectID": "week03/assets/trust_income_report.html",
    "href": "week03/assets/trust_income_report.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "week03/assets/trust_income_report.html#introduction",
    "href": "week03/assets/trust_income_report.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nWe examine whether national income levels are associated with interpersonal trust. Trust is central to economic performance and social cohesion, and understanding its determinants can inform development policy."
  },
  {
    "objectID": "week03/assets/trust_income_report.html#data-measures",
    "href": "week03/assets/trust_income_report.html#data-measures",
    "title": "",
    "section": "Data & Measures",
    "text": "Data & Measures\n\nIncome: GDP per capita, PPP (constant USD, log‑scaled).\nBaseline trust: share answering “Most people can be trusted” (Q57).\nAlternative trust index: reverse‑coded average of trust in neighbours, people known personally, first‑time met, other religion, other nationality (Q59–Q63)."
  },
  {
    "objectID": "week03/assets/trust_income_report.html#descriptive-pattern",
    "href": "week03/assets/trust_income_report.html#descriptive-pattern",
    "title": "",
    "section": "Descriptive Pattern",
    "text": "Descriptive Pattern\n\n\n\nBaseline trust vs income"
  },
  {
    "objectID": "week03/assets/trust_income_report.html#regression-results",
    "href": "week03/assets/trust_income_report.html#regression-results",
    "title": "",
    "section": "Regression Results",
    "text": "Regression Results\n\nBaseline trust\nβ₁ = 0.104 (SE 0.018, p = 0.000)\n\n\nAlternative trust\nβ₁ = 0.137 (SE 0.030, p = 0.000)"
  },
  {
    "objectID": "week03/assets/trust_income_report.html#discussion",
    "href": "week03/assets/trust_income_report.html#discussion",
    "title": "",
    "section": "Discussion",
    "text": "Discussion\nThe positive coefficient indicates that richer countries tend to report higher levels of interpersonal trust. A one‑log (≈ 2.7×) increase in GDP per capita is associated with an average increase of 0.104 points in the baseline trust measure (on a 0‑1 scale). Results hold for the broader five‑item trust index."
  },
  {
    "objectID": "week03/assets/trust_income_report.html#conclusion",
    "href": "week03/assets/trust_income_report.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nHigher income levels correlate with greater trust. While causality cannot be established here, the pattern supports theories linking economic prosperity to social capital."
  }
]