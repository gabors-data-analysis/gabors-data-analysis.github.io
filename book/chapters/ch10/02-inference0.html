<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Chapter 10: Multiple Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
.display.math{display: block; text-align: center; margin: 0.5rem auto;}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="02-inference0_files/libs/clipboard/clipboard.min.js"></script>
<script src="02-inference0_files/libs/quarto-html/tabby.min.js"></script>
<script src="02-inference0_files/libs/quarto-html/popper.min.js"></script>
<script src="02-inference0_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="02-inference0_files/libs/quarto-html/anchor.min.js"></script>
<link href="02-inference0_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="02-inference0_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="02-inference0_files/libs/quarto-html/quarto-html-435ba6093e6f0817aa7405139811e7b1.min.css" rel="stylesheet" append-hash="true" data-mode="light">
<link href="02-inference0_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">


<link rel="stylesheet" href="../../_assets/styles.css">
</head>

<body>


<header id="title-block-header">
<h1 class="title">Chapter 10: Multiple Linear Regression</h1>
<p class="subtitle">Sections 10.4-10.7 - Terminology, Inference &amp; Extensions</p>

</header>

<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sections-10.4-10.7-terminology-inference-extensions" id="toc-sections-10.4-10.7-terminology-inference-extensions"><span class="header-section-number">1</span> Sections 10.4-10.7: Terminology, Inference &amp; Extensions</a>
  <ul>
  <li><a href="#sec-10-4" id="toc-sec-10-4"><span class="header-section-number">1.1</span> 10.4 Multiple linear regression terminology</a></li>
  <li><a href="#sec-10-5" id="toc-sec-10-5"><span class="header-section-number">1.2</span> 10.5 Standard errors and confidence intervals in multiple linear regression</a></li>
  <li><a href="#sec-10-6" id="toc-sec-10-6"><span class="header-section-number">1.3</span> 10.6 Hypothesis testing in multiple linear regression</a></li>
  <li><a href="#case-study-a2-understanding-the-gender-difference-in-earnings" id="toc-case-study-a2-understanding-the-gender-difference-in-earnings"><span class="header-section-number">1.4</span> Case Study A2: Understanding the Gender Difference in Earnings</a></li>
  <li><a href="#sec-10-7" id="toc-sec-10-7"><span class="header-section-number">1.5</span> 10.7 Multiple linear regression with three or more explanatory variables</a></li>
  <li><a href="#sec-10-8" id="toc-sec-10-8"><span class="header-section-number">1.6</span> 10.8 Nonlinear patterns and multiple linear regression</a></li>
  <li><a href="#case-study-a3-understanding-the-gender-difference-in-earnings" id="toc-case-study-a3-understanding-the-gender-difference-in-earnings"><span class="header-section-number">1.7</span> Case Study A3: Understanding the Gender Difference in Earnings</a></li>
  </ul></li>
  </ul>
</nav>
<!-- 
File: 02-terminology-inference.qmd
Version: 1.0
Created: 2025-10-20
Iterations: 1
Model: Claude Sonnet 4.5 (2025-10-20)
Notes: Sections 10.4-10.7 from original LaTeX
       Verbatim textbook content with interactive enhancements
       All math syntax correct from start
       Figure paths project-relative
-->
<p><link href="https://fonts.googleapis.com/css2?family=Anton&amp;family=Oswald:wght@300;400;500;600;700&amp;family=Roboto+Condensed:wght@300;400;700&amp;family=Lora:wght@400;600&amp;display=swap" rel="stylesheet"></p>
<h1 class="chapter-title" data-number="1" id="sections-10.4-10.7-terminology-inference-extensions"><span class="header-section-number">1</span> Sections 10.4-10.7: Terminology, Inference &amp; Extensions</h1>
<hr>
<h2 data-number="1.1" id="sec-10-4" class="anchored"><span class="header-section-number">1.1</span> 10.4 Multiple linear regression terminology</h2>
<p>Multiple regression with two explanatory variables (<span class="math inline"><em>x</em><sub>1</sub></span> and <span class="math inline"><em>x</em><sub>2</sub></span>) allows for assessing the differences in expected <span class="math inline"><em>y</em></span> across observations that differ in <span class="math inline"><em>x</em><sub>1</sub></span> but are similar in terms of <span class="math inline"><em>x</em><sub>2</sub></span>. This difference is called <strong>conditional</strong> on that other explanatory variable <span class="math inline"><em>x</em><sub>2</sub></span>: difference in <span class="math inline"><em>y</em></span> by <span class="math inline"><em>x</em><sub>1</sub></span>, conditional on <span class="math inline"><em>x</em><sub>2</sub></span>. It is also called the controlled difference: difference in <span class="math inline"><em>y</em></span> by <span class="math inline"><em>x</em><sub>1</sub></span>, controlling for <span class="math inline"><em>x</em><sub>2</sub></span>. We often say that we condition on <span class="math inline"><em>x</em><sub>2</sub></span>, or control for <span class="math inline"><em>x</em><sub>2</sub></span>, when we include it in a multiple regression that focuses on average differences in <span class="math inline"><em>y</em></span> by <span class="math inline"><em>x</em><sub>1</sub></span>. When we focus on <span class="math inline"><em>x</em><sub>1</sub></span> in the multiple regression, the other right-hand-side variable, <span class="math inline"><em>x</em><sub>2</sub></span>, is called a <strong>covariate</strong>. In some cases, it is also called a <strong>confounder</strong>: if omitting <span class="math inline"><em>x</em><sub>2</sub></span> makes the slope on <span class="math inline"><em>x</em><sub>1</sub></span> different, it is said to confound the association of <span class="math inline"><em>y</em></span> and <span class="math inline"><em>x</em><sub>1</sub></span> (we’ll discuss confounders in Chapter 19, Section 19.3).</p>
<div class="review-box">
<div class="review-box-title">
Multiple Linear Regression Terminology
</div>
<ul>
<li>In a regression <span class="math inline"><em>y</em><sup><em>E</em></sup> = <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub><em>x</em><sub>1</sub> + <em>β</em><sub>2</sub><em>x</em><sub>2</sub></span> that focuses on <span class="math inline"><em>β</em><sub>1</sub></span>,</li>
<li>If we estimate a multiple regression <span class="math inline"><em>y</em><sup><em>E</em></sup> = <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub><em>x</em><sub>1</sub> + <em>β</em><sub>2</sub><em>x</em><sub>2</sub></span>, and we are interested in <span class="math inline"><em>β</em><sub>1</sub></span>, <span class="math inline"><em>x</em><sub>2</sub></span> is a covariate, and we say that we condition on <span class="math inline"><em>x</em><sub>2</sub></span> or control for <span class="math inline"><em>x</em><sub>2</sub></span>.</li>
<li>If, instead, we estimate <span class="math inline"><em>y</em><sup><em>E</em></sup> = <em>α</em> + <em>β</em><em>x</em><sub>1</sub></span> we say <span class="math inline"><em>x</em><sub>2</sub></span> is an omitted variable or a confounder.</li>
</ul>
</div>
<div class="ai-task">
<p><span class="ai-task-icon">🤖 AI PRACTICE TASK #5</span></p>
<p><strong>Prompt:</strong> “Explain the difference between ‘conditioning on’ a variable and ‘confounding’ in regression analysis. Give a concrete example where omitting a confounder would lead to wrong conclusions.”</p>
<p><a href="https://chatgpt.com/?q=Explain the difference between 'conditioning on' a variable and 'confounding' in regression analysis. Give a concrete example where omitting a confounder would lead to wrong conclusions." class="ai-task-copy" target="_blank">📋 COPY &amp; OPEN IN CHATGPT</a></p>
</div>
<hr>
<h2 data-number="1.2" id="sec-10-5" class="anchored"><span class="header-section-number">1.2</span> 10.5 Standard errors and confidence intervals in multiple linear regression</h2>
<p>The concept of statistical inference and the interpretation of confidence intervals in multiple regressions is similar to that in simple regressions. For example, the 95% confidence interval of slope of <span class="math inline"><em>x</em><sub>1</sub></span> in a multiple linear regression that conditions on another explanatory variable <span class="math inline"><em>x</em><sub>2</sub></span> (CI of <span class="math inline"><em>β</em><sub>1</sub></span> in <span class="math inline"><em>y</em><sup><em>E</em></sup> = <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub><em>x</em><sub>1</sub> + <em>β</em><sub>2</sub><em>x</em><sub>2</sub></span>) shows where we can expect the coefficient in the population, or general pattern, represented by the data.</p>
<p>Similarly to the coefficients in the simple regression, the 95% CI of a slope in a multiple regression is the coefficient value estimated from the data plus-or-minus two standard errors. Again similarly to the simple regression case, we can get the standard error either by bootstrap or using an appropriate formula. And, as usual, the simple SE formula is not a good approximation in general: it assumes homoskedasticity (same fit of the regression over the range of the explanatory variables). There is a robust SE formula for multiple regression, too, that works in general, both under homoskedasticity and heteroskedasticity. Thus, just as with simple regressions we advise you to make the software calculate <strong>robust SE</strong> as default.</p>
<p>While not correct in general, the simple formula is good to examine because it shows what makes the SE larger in a simpler more intuitive way than the robust formula. The simple SE formula for the slope <span class="math inline"><em>β̂</em><sub>1</sub></span> is</p>
<p><span class="math display">$$
SE(\hat{\beta}_{1}) = \frac{Std[e]}{\sqrt{n}Std(x_1) \sqrt{1-R_{1}^2}}
$$</span></p>
<p>Similarly to the simple SE formula for the simple linear regression in Chapter 9, Section 9.3, this formula has <span class="math inline">$\sqrt{n}$</span> in its denominator. But, similarly again to the simple linear regression, the correct number to divide with would be slightly different: the degrees of freedom instead of the number of the observations (see Chapter 9, Section 9.3.3). Here that would be <span class="math inline">$\sqrt{n-k-1}$</span> where <span class="math inline"><em>k</em></span> is the number of right-hand-side variables in the regression. Similarly to the simple regression, this makes little practical difference in most cases. However, in contrast with the simple regression case, it may make a difference not only when we have too few observations, but also when we have many right-hand-side variables relative to the number of observations. We’ll ignore that issue for most of this textbook, but it will come back sometimes, as, for example, in Chapter 21, Section 21.3.</p>
<p>This formula is very similar to what we have for simple regressions in other details, too, except for that new <span class="math inline">$\sqrt{1-R_{1}^2}$</span> term in the denominator. <span class="math inline"><em>R</em><sub>1</sub><sup>2</sup></span> is the R-squared of the regression of <span class="math inline"><em>x</em><sub>1</sub></span> on <span class="math inline"><em>x</em><sub>2</sub></span>. Recall that the R-squared of a simple regression is the square of the correlation between the two variables in the regression. Thus, <span class="math inline"><em>R</em><sub>1</sub><sup>2</sup></span> is the correlation between <span class="math inline"><em>x</em><sub>1</sub></span> and <span class="math inline"><em>x</em><sub>2</sub></span>. The stronger this correlation, the larger <span class="math inline"><em>R</em><sub>1</sub><sup>2</sup></span>, the smaller <span class="math inline">$\sqrt{1-R_{1}^2}$</span>, but then the larger <span class="math inline">$1/\sqrt{1-R_{1}^2}$</span> (<span class="math inline">$\sqrt{1-R_{1}^2}$</span> is in the denominator). So, the stronger the correlation between <span class="math inline"><em>x</em><sub>1</sub></span> and <span class="math inline"><em>x</em><sub>2</sub></span>, the larger the SE of <span class="math inline"><em>β̂</em><sub>1</sub></span>. Note the symmetry: the same would apply to the SE of <span class="math inline"><em>β̂</em><sub>2</sub></span>. As for the familiar terms in the formula: the SE is smaller, the smaller the standard deviation of the residuals (the better the fit of the regression), the larger the sample, and the larger the standard deviation of <span class="math inline"><em>x</em><sub>1</sub></span>.</p>
<p>At the polar case of a correlation of one (or negative one) that corresponds to <span class="math inline"><em>R</em><sub>1</sub><sup>2</sup> = 1</span>, the SE of the two coefficients does not exist. A correlation of one means that <span class="math inline"><em>x</em><sub>1</sub></span> and <span class="math inline"><em>x</em><sub>2</sub></span> are linear functions of each other. It is not only the SE formulae that cannot be computed in this case; the regression coefficients cannot be computed either. In this case the explanatory variables are said to be <strong>perfectly collinear</strong>.</p>
<p>Strong but imperfect correlation between explanatory variables is called <strong>multicollinearity</strong>. It allows for calculating the slope coefficients and their standard errors, but the standard errors may be large. Intuitively, this is because we would like to compare observations that are different in one of the variables but similar in the other. But strong correlation between the two implies that there are not many observations that are the same in one variable but different in the other variable. Therefore, there are just not enough valid observations for comparing average <span class="math inline"><em>y</em></span> across them. Indeed, the problem of multicollinearity is very similar to the problem of having too few observations in general. We can see it in the formula as well: the role of <span class="math inline">(1 − <em>R</em><sup>2</sup>)</span> and <span class="math inline"><em>n</em></span> are the same.</p>
<p>Consider our example of estimating how sales of the main product of our company tend to change when our price changes but the prices of competitors do not. In that example our own price and the competitors’ prices tended to move together. That’s multicollinearity. One consequence of this is that omitting the change in the competitors’ price would lead to omitted variable bias; thus we need to include that in our regression. But here we see that multicollinearity has another consequence. Including both price variables in the regression makes the SE of the coefficient of our own price larger, and its confidence interval wider, too. Intuitively, that’s because there are few months when our price changes but the competitors’ prices don’t change, and it is changes in sales in those months that contain the valuable information for estimating the coefficient on our own price. Months when our own and competitors’ prices change the same way don’t help. So the reason why we want competitors’ price in our regression (strong co-movement) is exactly the reason for having imprecise estimates with wide confidence intervals.</p>
<p>That’s true in general, too. Unfortunately, there is not much we can do about multicollinearity in the data we have, just as there is not much we can do about having too few observations. More data helps both, of course, but that is not much help when we have to work with the data that’s available. Alternatively, we may decide to change the specification of the regression and drop one of the strongly correlated explanatory variables. However, that results in a different regression. Whether we want a different regression or not needs to be evaluated keeping the substantive question of the analysis in mind.</p>
<div class="review-box">
<div class="review-box-title">
Inference in Multiple Regression
</div>
<ul>
<li><p>In the linear regression <span class="math inline"><em>y</em><sup><em>E</em></sup> = <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub><em>x</em><sub>1</sub> + <em>β</em><sub>2</sub><em>x</em><sub>2</sub></span>, the 95% Confidence Interval (CI) of <span class="math inline"><em>β̂</em><sub>1</sub></span> gives the range in which we can expect, with 95% confidence, the difference in <span class="math inline"><em>y</em></span> to fall in the general pattern, or population, that our data represents, when comparing observations with the same <span class="math inline"><em>x</em><sub>2</sub></span> but differing in <span class="math inline"><em>x</em><sub>1</sub></span> by one unit.</p></li>
<li><p>The simple SE formula for the slope in multiple regression with two explanatory variables: in the linear regression <span class="math inline"><em>y</em><sup><em>E</em></sup> = <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub><em>x</em><sub>1</sub> + <em>β</em><sub>2</sub><em>x</em><sub>2</sub></span>, the simple SE formula of <span class="math inline"><em>β̂</em><sub>1</sub></span> is</p></li>
</ul>
<p><span class="math display">$$
SE(\hat{\beta}_{1})=\frac{Std[e]}{\sqrt{n}Std(x_1) \sqrt{1-R_{1}^2}}
$$</span></p>
<p>where <span class="math inline"><em>e</em></span> is the residual <span class="math inline"><em>e</em> = <em>y</em> − <em>β̂</em><sub>0</sub> + <em>β̂</em><sub>1</sub><em>x</em><sub>1</sub> + <em>β̂</em><sub>2</sub><em>x</em><sub>2</sub></span> and <span class="math inline"><em>R</em><sub>1</sub><sup>2</sup></span> is the R-squared in the simple linear regression of <span class="math inline"><em>x</em><sub>1</sub></span> on <span class="math inline"><em>x</em><sub>2</sub></span>.</p>
<ul>
<li>This SE is smaller:
<ul>
<li>the smaller the standard deviation of the residual (the better the fit of the regression)</li>
<li>the larger the sample</li>
<li>the larger the variance of <span class="math inline"><em>x</em><sub>1</sub></span></li>
<li>the smaller the correlation between <span class="math inline"><em>x</em><sub>1</sub></span> and <span class="math inline"><em>x</em><sub>2</sub></span> (the smaller <span class="math inline"><em>R</em><sub>1</sub><sup>2</sup></span>).</li>
</ul></li>
</ul>
</div>
<div class="ai-task">
<p><span class="ai-task-icon">🤖 AI PRACTICE TASK #6</span></p>
<p><strong>Prompt:</strong> “I have two potential explanatory variables that are highly correlated (r = 0.95). Should I include both in my regression? Explain the trade-off between omitted variable bias and multicollinearity.”</p>
<button class="ai-task-copy" onclick="navigator.clipboard.writeText('I have two potential explanatory variables that are highly correlated (r = 0.95). Should I include both in my regression? Explain the trade-off between omitted variable bias and multicollinearity.'); alert('Copied!')">
📋 COPY &amp; OPEN IN CLAUDE
</button>
</div>
<hr>
<h2 data-number="1.3" id="sec-10-6" class="anchored"><span class="header-section-number">1.3</span> 10.6 Hypothesis testing in multiple linear regression</h2>
<p>Testing hypotheses about coefficients in a multiple regression is also very similar to that in a simple regression. The standard errors are estimated in a different way but with the appropriate SE, all works just the same. For example, testing whether <span class="math inline"><em>H</em><sub>0</sub> : <em>β</em><sub>1</sub> = 0</span> against <span class="math inline"><em>H</em><sub><em>A</em></sub> : <em>β</em><sub>1</sub> ≠ 0</span>, we need the p-value or the t-statistic. Standard regression output produced by most statistical software shows those statistics. If our level of significance is 0.05, we reject <span class="math inline"><em>H</em><sub>0</sub></span> if the p-value is less than 0.05, or – which is the same information in a different form – the t-statistic is less than -2 or greater than +2.</p>
<p>Besides testing a hypothesis that involves a single coefficient, we sometimes test a hypothesis that involves more coefficients. As we explained in Chapter 9, Section 9.4, these come in two forms: a single null hypothesis about two or more coefficients (e.g., if they are equal), or a list of null hypotheses (e.g., that several slope coefficients are zero). The latter is called testing joint hypotheses.</p>
<p>Testing joint hypotheses are based on a test statistic called the F-statistic, and the related test is called the <strong>F-test</strong>. The underlying logic of hypothesis testing is the same here: reject the null if the test statistic is larger than a critical value, which shows that the estimated coefficients are too far from what’s in the null. The technical details are different. But the meaning of the p-value is the same as always. Thus, we advise getting the p-value when testing a joint hypothesis.</p>
<p>In fact, the test that asks whether all slope coefficients are zero in the regression has its own name: the <strong>global F-test</strong>, or simply “the” F-test. Its results are often shown by statistical software by default. More frequently, we use joint testing of joint hypotheses to decide whether a subset of the coefficients (such as all geographical variables) are all zero.</p>
<p>Similarly to testing hypotheses about single coefficients, the F-test needs appropriate standard error estimates. In cross sectional data, those appropriate estimates are usually the robust SE estimates.</p>
<div>
<blockquote>
<p><strong>Hypothesis Testing in Multiple Regression</strong></p>
<p><strong>Single Coefficient Tests (t-tests):</strong> - Test <span class="math inline"><em>H</em><sub>0</sub> : <em>β</em><sub><em>j</em></sub> = 0</span> vs.&nbsp;<span class="math inline"><em>H</em><sub><em>A</em></sub> : <em>β</em><sub><em>j</em></sub> ≠ 0</span> - Use t-statistic or p-value from regression output - Reject <span class="math inline"><em>H</em><sub>0</sub></span> if p-value &lt; 0.05 (or if |t| &gt; 2 as rough guide)</p>
<p><strong>Joint Hypothesis Tests (F-tests):</strong> - Test whether multiple coefficients are zero simultaneously - Example: <span class="math inline"><em>H</em><sub>0</sub> : <em>β</em><sub>1</sub> = <em>β</em><sub>2</sub> = <em>β</em><sub>3</sub> = 0</span> - Use F-statistic and its p-value - <strong>Global F-test</strong>: Tests whether ALL slope coefficients are zero</p>
<p><strong>Critical:</strong> Always use robust standard errors for valid inference!</p>
</blockquote>
</div>
<div class="ai-task">
<p><span class="ai-task-icon">🤖 AI PRACTICE TASK #7</span></p>
<p><strong>Prompt:</strong> “What’s the difference between testing whether β₁ = 0 and testing whether β₁ = β₂ = β₃ = 0? Why can’t I just do three separate t-tests for the second question?”</p>
<button class="ai-task-copy" onclick="navigator.clipboard.writeText('What is the difference between testing whether β₁ = 0 and testing whether β₁ = β₂ = β₃ = 0? Why cannot I just do three separate t-tests for the second question?'); alert('Copied!')">
📋 COPY &amp; OPEN IN CLAUDE
</button>
</div>
<hr>
<h2 class="case-study anchored" data-number="1.4" id="case-study-a2-understanding-the-gender-difference-in-earnings"><span class="header-section-number">1.4</span> Case Study A2: Understanding the Gender Difference in Earnings</h2>
<div class="case-study-title">
CASE STUDY A2: STATISTICAL INFERENCE
</div>
<p><strong>Statistical inference</strong></p>
<p>Let’s revisit the results in Table 10.1, taking statistical inference into account. The data represents employees with a graduate degree in the U.S.A. in 2014. According to the estimate in column (1), women in this sample earn 19.5 percent less than men, on average. The appropriately estimated (robust) standard error is 0.008, implying a 95% CI of approximately [-0.21,-0.18]. We can be 95% confident that women earned 18 to 21 percent less, on average, than men among employees with graduate degrees in the U.S.A. in 2014.</p>
<p>Column (2) suggests that when we compare employees of the same age, women in this sample earn approximately 18.5 percent less than men, on average. The 95% CI is approximately [-0.20,-0.17]. It turns out that the estimated -0.195 in column (1) is within this CI, and the two CIs overlap. Thus it is very possible that there is no difference between these two coefficients in the population. We uncovered a difference in the data between the unconditional gender wage gap and the gender gap conditional on age. However, that difference is small. Moreover, it may not exist in the population. These two facts tend to go together: small differences are harder to pin down in the population, or general pattern, represented by the data. Often, that’s all right. Small differences are rarely very important. When they are, we need more precise estimates, which may come with larger sample size.</p>
<div>
<blockquote>
<p><strong>Understanding Confidence Intervals</strong></p>
<p>The 95% CI of [-0.20, -0.17] for the conditional gender gap means: - We are 95% confident the true population difference is between -20% and -17% - This interval does NOT include zero, so we can be confident there is a real gender gap - The unconditional estimate of -19.5% falls within this interval - While we see a difference in point estimates (19.5% vs 18.5%), the CIs overlap, suggesting the difference may not be statistically significant</p>
</blockquote>
</div>
<div class="ai-task">
<p><span class="ai-task-icon">🤖 AI PRACTICE TASK #8</span></p>
<p><strong>Prompt:</strong> “If two confidence intervals overlap, does that mean the coefficients are not significantly different? Explain why overlapping CIs are related to but not the same as a formal test of coefficient equality.”</p>
<button class="ai-task-copy" onclick="navigator.clipboard.writeText('If two confidence intervals overlap, does that mean the coefficients are not significantly different? Explain why overlapping CIs are related to but not the same as a formal test of coefficient equality.'); alert('Copied!')">
📋 COPY &amp; OPEN IN CLAUDE
</button>
</div>
<hr>
<h2 data-number="1.5" id="sec-10-7" class="anchored"><span class="header-section-number">1.5</span> 10.7 Multiple linear regression with three or more explanatory variables</h2>
<p>We spent a lot of time on multiple regression with two right-hand-side variables. That’s because that regression shows all the important differences between simple regression and multiple regression in intuitive ways. In practice, however, we rarely estimate regressions with exactly two right-hand-side variables. The number of right-hand-side variables in a multiple regression varies from case to case, but it’s typically more than two. In this section we describe multiple regressions with three or more right-hand-side variables. Their general form is</p>
<p><span class="math display"><em>y</em><sup><em>E</em></sup> = <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub><em>x</em><sub>1</sub> + <em>β</em><sub>2</sub><em>x</em><sub>2</sub> + <em>β</em><sub>3</sub><em>x</em><sub>3</sub> + ...</span></p>
<p>All of the results, language, and interpretations discussed so far carry forward to multiple linear regressions with three or more explanatory variables. Interpreting the slope of <span class="math inline"><em>x</em><sub>1</sub></span>: on average, <span class="math inline"><em>y</em></span> is <span class="math inline"><em>β</em><sub>1</sub></span> units larger in the data for observations with one unit larger <span class="math inline"><em>x</em><sub>1</sub></span> but with the same value for all other <span class="math inline"><em>x</em></span> variables. The interpretation of the other slope coefficients is analogous. The language of multiple regression is the same, including the concepts of conditioning, controlling, omitted, or confounder variables.</p>
<p>The standard error of coefficients may be estimated by bootstrap or a formula. As always, the appropriate formula is the robust SE formula. But the simple formula contains the things that make even the robust SE larger or smaller. For any slope coefficient <span class="math inline"><em>β̂</em><sub><em>k</em></sub></span> the simple SE formula is</p>
<p><span class="math display">$$
SE(\hat{\beta}_{k})=\frac{Std[e]}{\sqrt{n}Std[x_k]\sqrt{1-R_{k}^2}}
$$</span></p>
<p>Almost all is the same as with two right-hand-side variables. In particular, The SE is smaller, the smaller the standard deviation of the residuals (the better the fit of the regression), the larger the sample, and the larger the standard deviation of <span class="math inline"><em>x</em><sub><em>k</em></sub></span>. The new-looking thing is <span class="math inline"><em>R</em><sub><em>k</em></sub><sup>2</sup></span>. But that’s simply the generalization of <span class="math inline"><em>R</em><sub>1</sub><sup>2</sup></span> in the previous formula. It is the R-squared of the regression of <span class="math inline"><em>x</em><sub><em>k</em></sub></span> on all other <span class="math inline"><em>x</em></span> variables. The smaller that R-squared, the smaller the SE.</p>
<div class="review-box">
<div class="review-box-title">
Multiple Linear Regression with Three or More Variables
</div>
<p><strong>Equation:</strong> <span class="math inline"><em>y</em><sup><em>E</em></sup> = <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub><em>x</em><sub>1</sub> + <em>β</em><sub>2</sub><em>x</em><sub>2</sub> + <em>β</em><sub>3</sub><em>x</em><sub>3</sub> + ...</span></p>
<p><strong>Interpretation of <span class="math inline"><em>β</em><sub><em>k</em></sub></span> (slope of <span class="math inline"><em>x</em><sub><em>k</em></sub></span>):</strong> - On average, <span class="math inline"><em>y</em></span> is <span class="math inline"><em>β</em><sub><em>k</em></sub></span> units larger in the data for observations with one unit larger <span class="math inline"><em>x</em><sub><em>k</em></sub></span> but with the same value for all other x variables.</p>
<p><strong>Standard Error:</strong> <span class="math display">$$
SE(\hat{\beta}_{k})=\frac{Std[e]}{\sqrt{n}Std[x_k]\sqrt{1-R_{k}^2}}
$$</span></p>
<p>where <span class="math inline"><em>e</em></span> is the regression residual and <span class="math inline"><em>R</em><sub><em>k</em></sub><sup>2</sup></span> is the R-squared of the regression of <span class="math inline"><em>x</em><sub><em>k</em></sub></span> on all other <span class="math inline"><em>x</em></span> variables.</p>
</div>
<div class="ai-task">
<p><span class="ai-task-icon">🤖 AI PRACTICE TASK #9</span></p>
<p><strong>Prompt:</strong> “I want to add a 5th explanatory variable to my regression. What should I consider before doing so? Discuss degrees of freedom, multicollinearity, and interpretation.”</p>
<button class="ai-task-copy" onclick="navigator.clipboard.writeText('I want to add a 5th explanatory variable to my regression. What should I consider before doing so? Discuss degrees of freedom, multicollinearity, and interpretation.'); alert('Copied!')">
📋 COPY &amp; OPEN IN CLAUDE
</button>
</div>
<hr>
<h2 data-number="1.6" id="sec-10-8" class="anchored"><span class="header-section-number">1.6</span> 10.8 Nonlinear patterns and multiple linear regression</h2>
<p>In Chapter 8 we introduced piecewise linear splines, quadratics, and other polynomials to approximate a nonlinear <span class="math inline"><em>y</em><sup><em>E</em></sup> = <em>f</em>(<em>x</em>)</span> regression.</p>
<p>From a substantive point of view, piecewise linear splines and polynomials of a single explanatory variable are not multiple regressions. They do not uncover differences with respect to one right-hand-side variable conditional on one or more other right-hand-side variables. Their slope coefficients cannot be interpreted as the coefficients of multiple regressions: it does not make sense to compare observations that have the same <span class="math inline"><em>x</em></span> but a different <span class="math inline"><em>x</em><sup>2</sup></span>.</p>
<p>But such regressions are multiple linear regressions from a technical point of view. This means that the way their coefficients are calculated is the exact same way the coefficients of multiple linear regressions are calculated. Their standard errors are calculated the same way, too and so are their confidence intervals, test statistics, and p-values.</p>
<p>Testing hypotheses can be especially useful here, as it can help choose the functional form. With a piecewise linear spline, we can test whether the slopes are the same in adjacent line segments. If we can’t reject the null that they are the same, we may as well join them instead of having separate line segments. Testing hypotheses helps in choosing a polynomial, too. Here an additional complication is that the coefficients don’t have an easy interpretation in themselves. However, testing if all nonlinear coefficients are zero may help decide whether to include them at all.</p>
<p>However, testing hypotheses to decide whether to include a higher-order polynomial has its issues. Recall that a multiple linear regression requires that the right-hand-side variables are not perfectly collinear. In other words, they cannot be linear functions of each other. With a polynomial on the right-hand side, those variables are exact functions of each other: <span class="math inline"><em>x</em><sup>2</sup></span> is the square of <span class="math inline"><em>x</em></span>. But they are not a linear function of each other, so, technically, they are not perfectly collinear. That’s why we can include both <span class="math inline"><em>x</em></span> and <span class="math inline"><em>x</em><sup>2</sup></span> and, if needed, its higher order terms, in a linear regression. While they are not perfectly collinear, explanatory variables in a polynomial are often highly correlated. That multicollinearity results in high standard errors, wide confidence intervals, and high p-values. As with all kinds of multicollinearity, there isn’t anything we can do about that once we have settled on a functional form.</p>
<p>Importantly, when thinking about functional form, we should always keep in mind the substantive focus of our analysis. As we emphasized in Chapter 8, Section 8.3, we should go back to that original focus when deciding whether we want to include a piecewise linear spline or a polynomial to approximate a nonlinear pattern. There we said that we want our regression to have a good approximation to a nonlinear pattern in <span class="math inline"><em>x</em></span> if our goal is prediction or analyzing residuals. We may not want that if all we care about is the average association between <span class="math inline"><em>x</em></span> and <span class="math inline"><em>y</em></span>, except if that nonlinearity messes up the average association. This last point is a bit subtle, but usually means that we may want to transform variables to relative changes or take logs if the distribution of <span class="math inline"><em>x</em></span> or <span class="math inline"><em>y</em></span> is very skewed.</p>
<p>Here we have multiple <span class="math inline"><em>x</em></span> variables. Should we care about whether each is related to average <span class="math inline"><em>y</em></span> in a nonlinear fashion? The answer is the same as earlier: yes, if we want to do prediction or analyze residuals; no, if we care about average associations (except we may want to have transformed variables here, too). In addition, when we focus on a single average association (with, say, <span class="math inline"><em>x</em><sub>1</sub></span>) and all the other variables (<span class="math inline"><em>x</em><sub>2</sub></span>, <span class="math inline"><em>x</em><sub>3</sub></span>, …) are covariates to condition on, the only thing that matters is the coefficient on <span class="math inline"><em>x</em><sub>1</sub></span>. Even if nonlinearities matter for <span class="math inline"><em>x</em><sub>2</sub></span> and <span class="math inline"><em>x</em><sub>3</sub></span> themselves, they only matter for us if they make a difference in the estimated coefficient on <span class="math inline"><em>x</em><sub>1</sub></span>. Sometimes they do; very often they don’t.</p>
<div>
<blockquote>
<p><strong>When to Care About Nonlinear Patterns</strong></p>
<p><strong>Care about functional form when:</strong> - Your goal is <strong>prediction</strong> → Need accurate <span class="math inline"><em>ŷ</em></span> - You’re analyzing <strong>residuals</strong> → Need good overall fit - The distribution of <span class="math inline"><em>x</em></span> or <span class="math inline"><em>y</em></span> is highly skewed → Consider transformations (logs, etc.)</p>
<p><strong>Don’t worry as much when:</strong> - You care only about <strong>average associations</strong> → Linear may be “good enough” - Nonlinearities in covariates don’t affect your coefficient of interest</p>
<p><strong>Rule of thumb:</strong> If including nonlinear terms doesn’t meaningfully change your main coefficient, the linear specification may be adequate for your purpose.</p>
</blockquote>
</div>
<hr>
<h2 class="case-study anchored" data-number="1.7" id="case-study-a3-understanding-the-gender-difference-in-earnings"><span class="header-section-number">1.7</span> Case Study A3: Understanding the Gender Difference in Earnings</h2>
<div class="case-study-title">
CASE STUDY A3: NONLINEAR PATTERNS
</div>
<p><strong>Nonlinear patterns and multiple linear regression</strong></p>
<p>This step in our case study illustrates the point we made in the previous section. The regressions in Table 10.1 enter age in linear ways. Using part of the same data, in Chapter 9, Section 9.2 we found that log earnings and age follow a nonlinear pattern. In particular, there we found that average log earnings are a positive and steep function of age for younger people, but the pattern becomes gradually flatter for the middle-aged and may become completely flat, or even negative, among older employees.</p>
<p>Should we worry about the non-linear age-earnings pattern when our question is the average earnings difference between men and women? We investigated the gender gap conditional on age. Table 10.2 shows the results for multiple ways of doing it. Column (1) shows the regression with the unconditional difference that we showed in Table 10.1, for reference. Column (2) enters age in linear form. Column (3) enters it as quadratic. Column (4) enters it as a fourth-order polynomial.</p>
<div class="code-block">
<div class="code-header">
📊 REPLICATE TABLE 10.2
</div>
<div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># R Code to replicate Table 10.2</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(estimatr)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(modelsummary)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"cps_earnings_grad.csv"</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate four models</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lm_robust</span>(lnearnings <span class="sc">~</span> female, <span class="at">data =</span> data)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">lm_robust</span>(lnearnings <span class="sc">~</span> female <span class="sc">+</span> age, <span class="at">data =</span> data)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">lm_robust</span>(lnearnings <span class="sc">~</span> female <span class="sc">+</span> age <span class="sc">+</span> <span class="fu">I</span>(age<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> data)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>model4 <span class="ot">&lt;-</span> <span class="fu">lm_robust</span>(lnearnings <span class="sc">~</span> female <span class="sc">+</span> age <span class="sc">+</span> <span class="fu">I</span>(age<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">I</span>(age<span class="sc">^</span><span class="dv">3</span>) <span class="sc">+</span> <span class="fu">I</span>(age<span class="sc">^</span><span class="dv">4</span>), </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> data)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Display results</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">modelsummary</span>(<span class="fu">list</span>(model1, model2, model3, model4),</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>             <span class="at">stars =</span> <span class="fu">c</span>(<span class="st">'***'</span> <span class="ot">=</span> <span class="fl">0.01</span>, <span class="st">'**'</span> <span class="ot">=</span> <span class="fl">0.05</span>, <span class="st">'*'</span> <span class="ot">=</span> <span class="fl">0.1</span>),</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>             <span class="at">gof_omit =</span> <span class="st">"IC|Log|F|RMSE"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="https://github.com/gabors-data-analysis/da_case_studies/blob/master/ch10-gender-earnings-understand/ch10-gender-earnings-multireg.ipynb" class="code-link" target="_blank">🚀 OPEN IN CODESPACE</a></p>
</div>
<p><strong>Table 10.2: Gender differences in earnings – log earnings and age, various functional forms</strong></p>
<table class="caption-top">
<thead>
<tr class="header">
<th>Variable</th>
<th>(1) ln w</th>
<th>(2) ln w</th>
<th>(3) ln w</th>
<th>(4) ln w</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>female</strong></td>
<td>-0.195***</td>
<td>-0.185***</td>
<td>-0.180***</td>
<td>-0.180***</td>
</tr>
<tr class="even">
<td></td>
<td>(0.008)</td>
<td>(0.008)</td>
<td>(0.008)</td>
<td>(0.008)</td>
</tr>
<tr class="odd">
<td><strong>age</strong></td>
<td></td>
<td>0.007***</td>
<td>-0.024**</td>
<td>-0.168***</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>(0.000)</td>
<td>(0.010)</td>
<td>(0.051)</td>
</tr>
<tr class="odd">
<td><strong>age²</strong></td>
<td></td>
<td></td>
<td>0.0003***</td>
<td>0.008***</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td>(0.0001)</td>
<td>(0.003)</td>
</tr>
<tr class="odd">
<td><strong>age³</strong></td>
<td></td>
<td></td>
<td></td>
<td>-0.0001***</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td>(0.00003)</td>
</tr>
<tr class="odd">
<td><strong>age⁴</strong></td>
<td></td>
<td></td>
<td></td>
<td>0.000001**</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td>(0.0000003)</td>
</tr>
<tr class="odd">
<td><strong>Constant</strong></td>
<td>3.514***</td>
<td>3.198***</td>
<td>3.716***</td>
<td>6.924***</td>
</tr>
<tr class="even">
<td></td>
<td>(0.006)</td>
<td>(0.018)</td>
<td>(0.224)</td>
<td>(1.088)</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Observations</strong></td>
<td>18,241</td>
<td>18,241</td>
<td>18,241</td>
<td>18,241</td>
</tr>
<tr class="odd">
<td><strong>R-squared</strong></td>
<td>0.028</td>
<td>0.046</td>
<td>0.048</td>
<td>0.050</td>
</tr>
</tbody>
</table>
<p><em>Note: All employees with a graduate degree. Robust standard error estimates in parentheses. *** p&lt;0.01, ** p&lt;0.05, * p&lt;0.1</em><br>
<em>Source: cps-earnings dataset. 2014, U.S.A.</em></p>
<p>The unconditional difference is -19.5%; the conditional difference is -18% according to column (2), and the same -18% according to columns (3) and (4). The various estimates of the conditional difference are the same up to two digits of rounding, and all of them are within each others’ confidence intervals. Thus, apparently, the functional form for age does not really matter if we are interested in the average gender gap.</p>
<p>At the same time, all coefficient estimates of the high order polynomials are statistically significant, meaning that the nonlinear pattern is very likely true in the population and not just a chance event in the particular dataset. The R-squared of the more complicated regressions are larger. These indicate that the complicated polynomial specifications are better at capturing the patterns. That would certainly matter if our goal was to predict earnings. But it does not matter for uncovering the gender difference in average earnings.</p>
<div class="buy-book-banner">
    <div class="buy-book-content">
        <div class="buy-book-image">
            <a href="https://www.cambridge.org/highereducation/books/data-analysis-for-business-economics-and-policy/D67A1B0B56176D6D6A92E27F3F82AA20#overview" target="_blank">
                <img src="../../assets/cover-full_hd.png" alt="Data Analysis textbook cover">
            </a>
        </div>
        <div class="buy-book-text">
            <h3 class="anchored">📖 Support the Textbook</h3>
            <p>This interactive HTML edition is a free companion to the full textbook. For the complete experience, please consider purchasing the book.</p>
            <a href="https://www.cambridge.org/highereducation/books/data-analysis-for-business-economics-and-policy/D67A1B0B56176D6D6A92E27F3F82AA20#overview" class="btn-buy-book" target="_blank">
                Buy from Cambridge University Press →
            </a>
            <p class="buy-book-links">
                Also available on: 
                <a href="https://www.amazon.com/dp/1108483018" target="_blank">Amazon US</a> · 
                <a href="https://www.amazon.co.uk/dp/1108483018" target="_blank">Amazon UK</a> · 
                <a href="https://www.amazon.de/dp/1108483018" target="_blank">Amazon DE</a>
            </p>
        </div>
    </div>
</div>
<div>
<blockquote>
<p><strong>Key Insight: Purpose Determines Functional Form</strong></p>
<p><strong>What we learned:</strong> - Adding nonlinear age terms (quadratic, quartic) increases R² from 0.046 to 0.050 - All polynomial coefficients are statistically significant - BUT the gender coefficient barely changes: -0.185 → -0.180 → -0.180</p>
<p><strong>Conclusion:</strong> If your question is “what is the gender gap controlling for age?”, the linear specification is adequate. The nonlinear terms matter for prediction but not for estimating the gender gap.</p>
<p><strong>General principle:</strong> Match your functional form to your analytical purpose!</p>
</blockquote>
</div>
<div class="ai-task">
<p><span class="ai-task-icon">🤖 AI PRACTICE TASK #10</span></p>
<p><strong>Prompt:</strong> “I’m estimating the effect of education on wages, controlling for age. Should I include age as a polynomial? Walk me through how to decide, considering my research question.”</p>
<button class="ai-task-copy" onclick="navigator.clipboard.writeText('I am estimating the effect of education on wages, controlling for age. Should I include age as a polynomial? Walk me through how to decide, considering my research question.'); alert('Copied!')">
📋 COPY &amp; OPEN IN CLAUDE
</button>
</div>
<hr>
<div>
<blockquote>
<p><strong>🔗 Explore Further</strong></p>
<p><strong>Interactive Dashboard:</strong> Visualize confidence intervals and hypothesis tests<br>
<a href="https://dashboards.gabors-data-analysis.com/ch10">Open Dashboard</a></p>
<p><strong>Run the Analysis:</strong> Replicate all regressions yourself<br>
<a href="https://github.com/gabors-data-analysis/da_case_studies/blob/master/ch10-gender-earnings-understand/ch10-gender-earnings-multireg.ipynb">Open in GitHub Codespace</a></p>
<p><strong>Download the Data:</strong> cps-earnings dataset<br>
<a href="https://osf.io/4vt9a/">Get Data</a></p>
</blockquote>
</div>
<hr>
<div class="page-nav">
<div class="page-nav-prev">
<pre><code>&lt;a href="01-foundation.html"&gt;← Previous: Page 1 - Foundation&lt;/a&gt;</code></pre>
</div>
<div class="page-nav-next">
<pre><code>&lt;a href="03-categorical.html"&gt;Next: Page 3 - Categorical Variables →&lt;/a&gt;</code></pre>
</div>
</div>

<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
    tabsets.forEach(function(tabset) {
      const tabby = new Tabby('#' + tabset.id);
    });
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>




</body></html>